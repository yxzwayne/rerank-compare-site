[
    {
        "query": "What is the primary function of the attention mechanism in transformer models?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 4,
                "score": 0.9817357,
                "text": "al 2014 Sukhbaatar et al 2015 which assign continuous valued weights to each input component Transformer models Vaswani et al 2017 that are used in LLMs have soft attention as their core component Our method can be viewed as a type of hard attention mechanism as it removes attention away from irrelevant parts of the input The advantage of our method is that it operates in natural language and can leverage the full reasoning power of the LLM to make attention decisions that require deeper understanding while also making it potentially controllable and interpretable Reasoning in LLMs There are a number of other approaches that utilize the power of generating natural language that the LLM has learned in order to perform reasoning For example chain of thought reasoning\n"
            },
            {
                "rank": 1,
                "index": 0,
                "score": 0.94197524,
                "text": "skew its reasoning We refer to this procedure as System 2 Attention S2A because we can consider the underlying transformer and its attention mechanism as automatic operations analogous to system 1 reasoning in humans Kahneman 2011 System 2 allocating effortful mental activity takes over in humans when we need to pay deliberate attention to a task especially in situations where System 1 is likely to make errors Sloman 1996 This subsystem is hence similar to the goal of our S2A approach as our aim is to alleviate the aforementioned failures of transformer soft attention with extra deliberate effort from the reasoning engine LLM We describe the class of System 2 Attention mechanisms provide further motivation and detail several specific implementations in Section 2 In Section 3 we show\n"
            },
            {
                "rank": 2,
                "index": 7,
                "score": 0.8358858,
                "text": "dependencies To address this shortcoming Vaswani et al 2017 introduced the transformer architecture a seminal work that transformed RE and NLP in general The transformer is similar in structure to other sequence transduction models in that it consists of two modules an encoder module and a decoder module The encoder takes an input sequence x and produces a dense representation zthat is fed to the decoder The decoder then uses zto produce an output sequence y However the transformer was unique in its use of stacked multi head attention functions in the encoder and decoder modules In the original transformer architecture the encoder and decoder modules consisted of six identical and serially connected layers Each layer contained a multi head attention function as well as a feedforward network\n"
            },
            {
                "rank": 3,
                "index": 1,
                "score": 0.77218723,
                "text": "is little theory to help us better understand the nature and computational capabilities of attention To add ress this gap in Section 2 we first seek to identify and classify the most fundamental buil ding block of all attention mechanisms within the deep learning framework In particular w e identify three key attentional mechanisms we call activation attention output gating an d synaptic gating In Section 3 we show how output gating and synaptic gating are used in all t he current attention based architectures including transformers In Section 4 we ex plore the functional capacity of output gating and synaptic gating In Section 5 we provide a brief overview of the notion of capacity and the technique of multiplexing which is a for m of activation attention\n"
            },
            {
                "rank": 4,
                "index": 6,
                "score": 0.7690802,
                "text": "generate comprehensive summaries Shaham et al 2022 otherwise models often miss important information Note that typical transformer models apply full attention to capture token dependencies pair wise It leads to a quadratic time and space complexity w r t input length However such a complexity is prohibitive for long sequences In particular it incurs massive memory consumption during the back propagation For example a transformer model with 250M parameters consumes over 80G GPU memory when sequence length is 8k Zuo et al 2022 To address this scalability issue various approaches have been proposed to reduce the complexity One approach is sparse attention which restricts each token to attend a subset of tokens based on predefined sparsity patterns Beltagy et al 2020 Zaheer et al 2020 Ainslie et al\n"
            },
            {
                "rank": 5,
                "index": 3,
                "score": 0.76100606,
                "text": "help Transformer models address their runtime and memory challenges for long sequences Many approximate attention methods have aimed to reduce the compute and memory requirements of attention These methods range from sparse approximation 51 74 to low rank approximation 12 50 84 and their combinations 3 9 92 Although these methods reduce the compute requirements to linear or near linear in sequence length many of them do not display wall clock speedup against standard attention and have not gained wide adoption One main reason is that they focus on FLOP reduction which may not correlate with wall clock speed and tend to ignore overheads from memory access IO In this paper we argue that a missing principle is making attention algorithms IO aware that is carefully accounting for\n"
            },
            {
                "rank": 6,
                "index": 5,
                "score": 0.74242175,
                "text": "result applies to a generalized model of transformers with few restrictions on the component functions It uses the fact that only log bits of information are needed per position 7 3 2 Threshold circuits TC0 Merrill et al 2022 prove an upper bound analogous to that of Hao et al 2022 but for averagehard attention transformers They show that a transformer with activations in Fcan be simulated in TC0 The key reason why soft attention extends the power to TC0is because it enables counting cf 7 2 3 and counting can be used to solve problems like M AJORITY that are outside AC0 Furthermore Merrill and Sabharwal 2023b show that softmax attention log precision transformers are in L uniform TC0 and Strobl 2023 shows that average hard attention\n"
            },
            {
                "rank": 7,
                "index": 2,
                "score": 0.5970794,
                "text": "type of sequence model the Transformer Vaswani et al 2017 and its core attention layer Bahdanau Cho and Bengio 2015 The efficacy of self attention is attributed to its ability to route information densely within a context window allowing it to model complex data However this property brings fundamental drawbacks an inability to model anything outside of a finite window and quadratic scaling with respect to the window length An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks Tay Dehghani Bahri et al 2022 but often at the expense of the very properties that makes it effective As of yet none of these variants have been shown to be empirically effective at scale across domains Recently structured state space sequence\n"
            },
            {
                "rank": 8,
                "index": 9,
                "score": 0.30167055,
                "text": "seem complex and sometimes obscure the underlying neural architecture 16 12 21 4 13 24 it can be checked that in all cases these are built out of the output and synaptic gating operations described in the previous section For conciseness here we demonstrate this in detail only for THE QUARKS OF ATTENTION 11 the transformer architectures 28 27 see also for an MLP alternative to transformers These architectures consist of stacks of similar encoder an d decoder modules with attention mechanisms in each module The details of an encoder module a re shown in Figure 5 As the Figure shows a shared and typically linear network is first a pplied to each of ninput vectors At the bottom of the architecture these input vectors could represent for\n"
            },
            {
                "rank": 9,
                "index": 8,
                "score": 0.00081355654,
                "text": "Khabsa Madian Fang H and Ma H 2020 Linformer self attention with linear complexity Preprint at arXiv https arxiv org abs 2006 04768 39 Zaheer M Guruganesh G Dubey K A Ainslie J Alberti C Ontanon S Pham P Ravula A Wang Q Yang L et al 2020 Big bird trans formers for longer sequences Adv Neural Inf Process Syst 33 1728317297 40 Katharopoulos A Vyas A Pappas N and Fleuret F 2020 Transformers are RNNs fast autoregressive transformers with linearattention In International Conference on Machine Learning PMLR pp 51565165 41 Choromanski K Likhosherstov V Dohan D Song X Gane A Sarlos T Hawkins P Davis J Mohiuddin A Kaiser L et al 2020 Rethinking attention with performers Preprint at arXiv https arxiv org abs 2009 14794 42\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": 0.5252984762191772,
                "text": "skew its reasoning We refer to this procedure as System 2 Attention S2A because we can consider the underlying transformer and its attention mechanism as automatic operations analogous to system 1 reasoning in humans Kahneman 2011 System 2 allocating effortful mental activity takes over in humans when we need to pay deliberate attention to a task especially in situations where System 1 is likely to make errors Sloman 1996 This subsystem is hence similar to the goal of our S2A approach as our aim is to alleviate the aforementioned failures of transformer soft attention with extra deliberate effort from the reasoning engine LLM We describe the class of System 2 Attention mechanisms provide further motivation and detail several specific implementations in Section 2 In Section 3 we show\n"
            },
            {
                "rank": 1,
                "score": -0.32937127351760864,
                "text": "al 2014 Sukhbaatar et al 2015 which assign continuous valued weights to each input component Transformer models Vaswani et al 2017 that are used in LLMs have soft attention as their core component Our method can be viewed as a type of hard attention mechanism as it removes attention away from irrelevant parts of the input The advantage of our method is that it operates in natural language and can leverage the full reasoning power of the LLM to make attention decisions that require deeper understanding while also making it potentially controllable and interpretable Reasoning in LLMs There are a number of other approaches that utilize the power of generating natural language that the LLM has learned in order to perform reasoning For example chain of thought reasoning\n"
            },
            {
                "rank": 2,
                "score": -0.3924310803413391,
                "text": "generate comprehensive summaries Shaham et al 2022 otherwise models often miss important information Note that typical transformer models apply full attention to capture token dependencies pair wise It leads to a quadratic time and space complexity w r t input length However such a complexity is prohibitive for long sequences In particular it incurs massive memory consumption during the back propagation For example a transformer model with 250M parameters consumes over 80G GPU memory when sequence length is 8k Zuo et al 2022 To address this scalability issue various approaches have been proposed to reduce the complexity One approach is sparse attention which restricts each token to attend a subset of tokens based on predefined sparsity patterns Beltagy et al 2020 Zaheer et al 2020 Ainslie et al\n"
            },
            {
                "rank": 3,
                "score": -1.1043870449066162,
                "text": "dependencies To address this shortcoming Vaswani et al 2017 introduced the transformer architecture a seminal work that transformed RE and NLP in general The transformer is similar in structure to other sequence transduction models in that it consists of two modules an encoder module and a decoder module The encoder takes an input sequence x and produces a dense representation zthat is fed to the decoder The decoder then uses zto produce an output sequence y However the transformer was unique in its use of stacked multi head attention functions in the encoder and decoder modules In the original transformer architecture the encoder and decoder modules consisted of six identical and serially connected layers Each layer contained a multi head attention function as well as a feedforward network\n"
            },
            {
                "rank": 4,
                "score": -1.1530033349990845,
                "text": "is little theory to help us better understand the nature and computational capabilities of attention To add ress this gap in Section 2 we first seek to identify and classify the most fundamental buil ding block of all attention mechanisms within the deep learning framework In particular w e identify three key attentional mechanisms we call activation attention output gating an d synaptic gating In Section 3 we show how output gating and synaptic gating are used in all t he current attention based architectures including transformers In Section 4 we ex plore the functional capacity of output gating and synaptic gating In Section 5 we provide a brief overview of the notion of capacity and the technique of multiplexing which is a for m of activation attention\n"
            },
            {
                "rank": 5,
                "score": -1.3478516340255737,
                "text": "help Transformer models address their runtime and memory challenges for long sequences Many approximate attention methods have aimed to reduce the compute and memory requirements of attention These methods range from sparse approximation 51 74 to low rank approximation 12 50 84 and their combinations 3 9 92 Although these methods reduce the compute requirements to linear or near linear in sequence length many of them do not display wall clock speedup against standard attention and have not gained wide adoption One main reason is that they focus on FLOP reduction which may not correlate with wall clock speed and tend to ignore overheads from memory access IO In this paper we argue that a missing principle is making attention algorithms IO aware that is carefully accounting for\n"
            },
            {
                "rank": 6,
                "score": -1.3855183124542236,
                "text": "result applies to a generalized model of transformers with few restrictions on the component functions It uses the fact that only log bits of information are needed per position 7 3 2 Threshold circuits TC0 Merrill et al 2022 prove an upper bound analogous to that of Hao et al 2022 but for averagehard attention transformers They show that a transformer with activations in Fcan be simulated in TC0 The key reason why soft attention extends the power to TC0is because it enables counting cf 7 2 3 and counting can be used to solve problems like M AJORITY that are outside AC0 Furthermore Merrill and Sabharwal 2023b show that softmax attention log precision transformers are in L uniform TC0 and Strobl 2023 shows that average hard attention\n"
            },
            {
                "rank": 7,
                "score": -1.3903430700302124,
                "text": "seem complex and sometimes obscure the underlying neural architecture 16 12 21 4 13 24 it can be checked that in all cases these are built out of the output and synaptic gating operations described in the previous section For conciseness here we demonstrate this in detail only for THE QUARKS OF ATTENTION 11 the transformer architectures 28 27 see also for an MLP alternative to transformers These architectures consist of stacks of similar encoder an d decoder modules with attention mechanisms in each module The details of an encoder module a re shown in Figure 5 As the Figure shows a shared and typically linear network is first a pplied to each of ninput vectors At the bottom of the architecture these input vectors could represent for\n"
            },
            {
                "rank": 8,
                "score": -2.5856499671936035,
                "text": "Khabsa Madian Fang H and Ma H 2020 Linformer self attention with linear complexity Preprint at arXiv https arxiv org abs 2006 04768 39 Zaheer M Guruganesh G Dubey K A Ainslie J Alberti C Ontanon S Pham P Ravula A Wang Q Yang L et al 2020 Big bird trans formers for longer sequences Adv Neural Inf Process Syst 33 1728317297 40 Katharopoulos A Vyas A Pappas N and Fleuret F 2020 Transformers are RNNs fast autoregressive transformers with linearattention In International Conference on Machine Learning PMLR pp 51565165 41 Choromanski K Likhosherstov V Dohan D Song X Gane A Sarlos T Hawkins P Davis J Mohiuddin A Kaiser L et al 2020 Rethinking attention with performers Preprint at arXiv https arxiv org abs 2009 14794 42\n"
            },
            {
                "rank": 9,
                "score": -2.618905782699585,
                "text": "type of sequence model the Transformer Vaswani et al 2017 and its core attention layer Bahdanau Cho and Bengio 2015 The efficacy of self attention is attributed to its ability to route information densely within a context window allowing it to model complex data However this property brings fundamental drawbacks an inability to model anything outside of a finite window and quadratic scaling with respect to the window length An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks Tay Dehghani Bahri et al 2022 but often at the expense of the very properties that makes it effective As of yet none of these variants have been shown to be empirically effective at scale across domains Recently structured state space sequence\n"
            }
        ]
    },
    {
        "query": "Which algorithm is commonly used for sequence alignment in genomics?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 4,
                "score": 0.048405956,
                "text": "Altschul S F etal Gapped BLAST and PSI BLAST a new generation of protein database search programs Nucleic Acids Res 25 33893402 1997 63 Katoh K Standley D M MAFFT multiple sequence alignment software version 7 improvements in performance and usability Mol Biol Evol 30 772780 2013 64 Stamatakis A RAxML version 8 a tool for phylogenetic analysis and post analysis of large phylogenies Bioinformatics 30 13121313 2014 65 Fu L Niu B Zhu Z Wu S Li W CD HIT accelerated for clustering the next generation sequencing data Bioinformatics 28 31503152 2012 66 Guindon S etal New algorithms and methods to estimate maximum likelihood phylogenies assessing the performance of PhyML 3 0 Syst Biol 59 307321 2010 67 Soubrier J etal The influence of rate heterogeneity among\n"
            },
            {
                "rank": 1,
                "index": 3,
                "score": 0.010902374,
                "text": "of matches filtered for sequence length less than 500 and aligned using the MAFFT85 webserver with the default parameters We calculated a phylogenetic tree using IQ TREE86 with the LG I G substitution model The resulting tree is shown in Extended Data Fig 9 and dem onstrates that while the closest structure homologues to the known state are clustered the closest homologues to the alternative state are dispersed across the tree Testing the sensitivity of AF2 and AF Cluster to point mutations in the GA GB system To test the sensitivity of AF2 and AF Cluster to point mutations in the GA GB87 system MSAs were generated using the default MSA generation routine from ColabFold using MMseqs2 For AF Cluster MSAs were then clustered using the DBSCAN procedure\n"
            },
            {
                "rank": 2,
                "index": 1,
                "score": 0.008445627,
                "text": "protein sequences In this sense DeepSeqProt can be thought of as a clustering tool albeit with added information from the structure of latent space We benchmarked DeepSeqProt alongside several clustering algorithms designed for unaligned protein sequences CD HIT Li and Godzik 2006 was one of the first Li et al 2001 clustering algorithms designed for large databases and uses short word filtering followed by greedy incremental clustering by sequence similarity Li and Godzik 2006 Fu et al 2012 CD HIT uses a clustering threshold parameter which controls the short word filtering step for which we tested two values 50 and 75 MMseqs2 Steinegger and Sding 2017 is a similar software suite designed to quickly and sensitively search and cluster large sequence data sets Steinegger and Sding 2017 MMseqs2\n"
            },
            {
                "rank": 3,
                "index": 0,
                "score": 0.0038844766,
                "text": "aligned sequence of complex structure Thus the aligned sequences are stillcontinuous by mapping them onto the complex structure Finally the length of the aligned sequences is 172 which contains 62 positions of DHp domain and 110 positions of Rec domain Supplementary Dataset S1 Quanti fication of local information With MSA the local information including first order conservation hydrophobic preference and local frustration at positions on the sequence can be extracted and mapped onto the native structure The firstorder conservation is computed through Kullback Leibler divergence or relative entropy 72 that is Ci20 a1fa ilnfa i qa 1 where fa iis the observed frequency of residue type a at position i from MSA there are 20 types for native residues qais the background frequency of residue type a which\n"
            },
            {
                "rank": 4,
                "index": 7,
                "score": 0.0024822766,
                "text": "adapter sequences and low qualityll OPEN ACCESS Cell187 642658 e1e9 February 1 2024 e7Article 3 bases match read wildcards m 20 q 10 R1 reads corresponding to RNA 3 ends were then aligned to the spiked in Drosophila genome index dm6 using BWA with those reads not mapping to the spike genome serving as input to the primary genome alignment step Reads mapping to the hg38 reference genome were then sorted via samtools 1 3 1 n and subsequently converted to bam files The bam files are converted to bigwig files by bamCoverage of deepTools 3 5 For metagene plots bigwigfiles of three replicates of each group and combined and averaged using WiggleTools The pausing indices were calculated using NRSA v2 packages 80 Gel filtration size exclusion chromatography\n"
            },
            {
                "rank": 5,
                "index": 8,
                "score": 0.0017821963,
                "text": "W etal NMRbox a resource for biomolecular NMR computation Biophys J 112 15291534 2017 84 Remmert M Biegert A Hauser A Soding J HHblits lightning fast iterative protein sequence searching by HMM HMM alignment Nat Methods 9 173175 2011 85 Katoh K Rozewicki J Yamada K D MAFFT online service multiple sequence alignment interactive sequence choice and visualization Brief Bioinform 20 11601166 2019 86 Minh B Q etal IQ TREE 2 new models and efficient methods for phylogenetic inference in the genomic era Mol Biol Evol 37 15301534 2020 87 Fahnestock S R Alexander P Nagle J Filpula D Gene for an immunoglobulin binding protein from a group G streptococcus J Bacteriol 167 870880 1986 88 McGibbon R T etal MDTraj a modern open library for the analysis\n"
            },
            {
                "rank": 6,
                "index": 9,
                "score": 0.00087617995,
                "text": "relative to the consensus sequence as observed in original MSA black and predicted by the Potts blue and independent gray models Flynn et al doi 10 1093 molbev msx095 MBE 1294Downloaded from https academic oup com mbe article 34 6 1291 3056431 by guest on 13 March 2024 The Potts model also captures the observed statistics for larger subsequences but as subsequence lengths increase observed marginal probabilities in our MSA approach the sampling limit of the alignment 1 N C252 C210 C04where N is the number of sequences in the MSA meaning compari sons between the observed marginals and the Potts modelpredictions become dominated by noise Despite this Haq et al 2012 have shown that a Potts model parameterized on one MSA of HIV 1 sequences can be\n"
            },
            {
                "rank": 7,
                "index": 6,
                "score": 0.00047470425,
                "text": "based on analysis of the distribution of pairwise sequence similarities in the kinase data set see Supporting Material This leaves an effective number of sequences N effPwof 8149 We then trim the first 5 and last 61 positions from the alignment that contain variable secondary structures leaving 175 positions Alphabet reduction We reduce the alphabet size qfrom 21 residue types 20 amino acids plus gap to 8 in a way that preserves the correlation structure of the MSA unlike amino acid reduction schemes based on physiochemical properties 53 54 For each position processed in random order we merge the pair of letters that gives the best least squares fit between the C18 L 2 C19 Mutual Information MI scores across all position pairs of the MSA in the\n"
            },
            {
                "rank": 8,
                "index": 5,
                "score": 0.00018378456,
                "text": "www nature com naturecommunications 9 inverse of the number of sequences having least 80 sequence identity with sequence m and Meff mwmdenotes the effective number of independent sequences The goal is to remove the in fluence of very closely related sequences Note however that such reweighting cannot fully capture the hierarchical structure of phylogenetic relations between proteins Sampling from the model Once the model parameters are inferred a sequence can be iteratively generated by the following procedure Sample the first residue from P a1 Sample the second residue from P a2a1 where a1is sampled in the previous step L Sample the last residue from P aLaL1 aL2 a2 a1 Each step is very fast because there are only 21 possible values for each probability Both training andsampling are\n"
            },
            {
                "rank": 9,
                "index": 2,
                "score": 8.481104e-05,
                "text": "Emms DM Kelly S 2019 Orthofinder phylogenetic orthology inference for comparative genomics Genome Biol 20 114 Feldbauer R et al 2020 DeepNOG fast and accurate protein ortholo gous group assignment Bioinformatics 36 53045312 Finn RD Clements J Eddy SR 2011 HMMER web server interactive sequence similarity searching Nucleic Acids Res 39 W29W37 Fu L Niu B Zhu Z Wu S Li W 2012 CD HIT accelerated for clustering the next generation sequencing data Bioinformatics 28 31503152 Gene Ontology Consortium 2021 The Gene Ontology resource enriching a GOld mine Nucleic Acids Res 49 D325D334 Kingma DP Ba J 2014 Adam a method for stochastic optimization arXiv preprint arXiv 1412 6980 Kingma DP Welling M 2014 Auto encoding variational Bayes arXiv 1312 6114 cs stat http arxiv org abs 1312\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -1.342585802078247,
                "text": "W etal NMRbox a resource for biomolecular NMR computation Biophys J 112 15291534 2017 84 Remmert M Biegert A Hauser A Soding J HHblits lightning fast iterative protein sequence searching by HMM HMM alignment Nat Methods 9 173175 2011 85 Katoh K Rozewicki J Yamada K D MAFFT online service multiple sequence alignment interactive sequence choice and visualization Brief Bioinform 20 11601166 2019 86 Minh B Q etal IQ TREE 2 new models and efficient methods for phylogenetic inference in the genomic era Mol Biol Evol 37 15301534 2020 87 Fahnestock S R Alexander P Nagle J Filpula D Gene for an immunoglobulin binding protein from a group G streptococcus J Bacteriol 167 870880 1986 88 McGibbon R T etal MDTraj a modern open library for the analysis\n"
            },
            {
                "rank": 1,
                "score": -1.4825787544250488,
                "text": "Altschul S F etal Gapped BLAST and PSI BLAST a new generation of protein database search programs Nucleic Acids Res 25 33893402 1997 63 Katoh K Standley D M MAFFT multiple sequence alignment software version 7 improvements in performance and usability Mol Biol Evol 30 772780 2013 64 Stamatakis A RAxML version 8 a tool for phylogenetic analysis and post analysis of large phylogenies Bioinformatics 30 13121313 2014 65 Fu L Niu B Zhu Z Wu S Li W CD HIT accelerated for clustering the next generation sequencing data Bioinformatics 28 31503152 2012 66 Guindon S etal New algorithms and methods to estimate maximum likelihood phylogenies assessing the performance of PhyML 3 0 Syst Biol 59 307321 2010 67 Soubrier J etal The influence of rate heterogeneity among\n"
            },
            {
                "rank": 2,
                "score": -2.5910444259643555,
                "text": "protein sequences In this sense DeepSeqProt can be thought of as a clustering tool albeit with added information from the structure of latent space We benchmarked DeepSeqProt alongside several clustering algorithms designed for unaligned protein sequences CD HIT Li and Godzik 2006 was one of the first Li et al 2001 clustering algorithms designed for large databases and uses short word filtering followed by greedy incremental clustering by sequence similarity Li and Godzik 2006 Fu et al 2012 CD HIT uses a clustering threshold parameter which controls the short word filtering step for which we tested two values 50 and 75 MMseqs2 Steinegger and Sding 2017 is a similar software suite designed to quickly and sensitively search and cluster large sequence data sets Steinegger and Sding 2017 MMseqs2\n"
            },
            {
                "rank": 3,
                "score": -2.85129714012146,
                "text": "adapter sequences and low qualityll OPEN ACCESS Cell187 642658 e1e9 February 1 2024 e7Article 3 bases match read wildcards m 20 q 10 R1 reads corresponding to RNA 3 ends were then aligned to the spiked in Drosophila genome index dm6 using BWA with those reads not mapping to the spike genome serving as input to the primary genome alignment step Reads mapping to the hg38 reference genome were then sorted via samtools 1 3 1 n and subsequently converted to bam files The bam files are converted to bigwig files by bamCoverage of deepTools 3 5 For metagene plots bigwigfiles of three replicates of each group and combined and averaged using WiggleTools The pausing indices were calculated using NRSA v2 packages 80 Gel filtration size exclusion chromatography\n"
            },
            {
                "rank": 4,
                "score": -3.5274765491485596,
                "text": "Emms DM Kelly S 2019 Orthofinder phylogenetic orthology inference for comparative genomics Genome Biol 20 114 Feldbauer R et al 2020 DeepNOG fast and accurate protein ortholo gous group assignment Bioinformatics 36 53045312 Finn RD Clements J Eddy SR 2011 HMMER web server interactive sequence similarity searching Nucleic Acids Res 39 W29W37 Fu L Niu B Zhu Z Wu S Li W 2012 CD HIT accelerated for clustering the next generation sequencing data Bioinformatics 28 31503152 Gene Ontology Consortium 2021 The Gene Ontology resource enriching a GOld mine Nucleic Acids Res 49 D325D334 Kingma DP Ba J 2014 Adam a method for stochastic optimization arXiv preprint arXiv 1412 6980 Kingma DP Welling M 2014 Auto encoding variational Bayes arXiv 1312 6114 cs stat http arxiv org abs 1312\n"
            },
            {
                "rank": 5,
                "score": -3.7499630451202393,
                "text": "aligned sequence of complex structure Thus the aligned sequences are stillcontinuous by mapping them onto the complex structure Finally the length of the aligned sequences is 172 which contains 62 positions of DHp domain and 110 positions of Rec domain Supplementary Dataset S1 Quanti fication of local information With MSA the local information including first order conservation hydrophobic preference and local frustration at positions on the sequence can be extracted and mapped onto the native structure The firstorder conservation is computed through Kullback Leibler divergence or relative entropy 72 that is Ci20 a1fa ilnfa i qa 1 where fa iis the observed frequency of residue type a at position i from MSA there are 20 types for native residues qais the background frequency of residue type a which\n"
            },
            {
                "rank": 6,
                "score": -4.542850971221924,
                "text": "relative to the consensus sequence as observed in original MSA black and predicted by the Potts blue and independent gray models Flynn et al doi 10 1093 molbev msx095 MBE 1294Downloaded from https academic oup com mbe article 34 6 1291 3056431 by guest on 13 March 2024 The Potts model also captures the observed statistics for larger subsequences but as subsequence lengths increase observed marginal probabilities in our MSA approach the sampling limit of the alignment 1 N C252 C210 C04where N is the number of sequences in the MSA meaning compari sons between the observed marginals and the Potts modelpredictions become dominated by noise Despite this Haq et al 2012 have shown that a Potts model parameterized on one MSA of HIV 1 sequences can be\n"
            },
            {
                "rank": 7,
                "score": -4.6592841148376465,
                "text": "of matches filtered for sequence length less than 500 and aligned using the MAFFT85 webserver with the default parameters We calculated a phylogenetic tree using IQ TREE86 with the LG I G substitution model The resulting tree is shown in Extended Data Fig 9 and dem onstrates that while the closest structure homologues to the known state are clustered the closest homologues to the alternative state are dispersed across the tree Testing the sensitivity of AF2 and AF Cluster to point mutations in the GA GB system To test the sensitivity of AF2 and AF Cluster to point mutations in the GA GB87 system MSAs were generated using the default MSA generation routine from ColabFold using MMseqs2 For AF Cluster MSAs were then clustered using the DBSCAN procedure\n"
            },
            {
                "rank": 8,
                "score": -5.105245113372803,
                "text": "based on analysis of the distribution of pairwise sequence similarities in the kinase data set see Supporting Material This leaves an effective number of sequences N effPwof 8149 We then trim the first 5 and last 61 positions from the alignment that contain variable secondary structures leaving 175 positions Alphabet reduction We reduce the alphabet size qfrom 21 residue types 20 amino acids plus gap to 8 in a way that preserves the correlation structure of the MSA unlike amino acid reduction schemes based on physiochemical properties 53 54 For each position processed in random order we merge the pair of letters that gives the best least squares fit between the C18 L 2 C19 Mutual Information MI scores across all position pairs of the MSA in the\n"
            },
            {
                "rank": 9,
                "score": -5.88679313659668,
                "text": "www nature com naturecommunications 9 inverse of the number of sequences having least 80 sequence identity with sequence m and Meff mwmdenotes the effective number of independent sequences The goal is to remove the in fluence of very closely related sequences Note however that such reweighting cannot fully capture the hierarchical structure of phylogenetic relations between proteins Sampling from the model Once the model parameters are inferred a sequence can be iteratively generated by the following procedure Sample the first residue from P a1 Sample the second residue from P a2a1 where a1is sampled in the previous step L Sample the last residue from P aLaL1 aL2 a2 a1 Each step is very fast because there are only 21 possible values for each probability Both training andsampling are\n"
            }
        ]
    },
    {
        "query": "What is the main advantage of using transformer-based models over recurrent neural networks?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 6,
                "score": 0.99583375,
                "text": "at the national US average of 0 385 kg CO 2e per KWh starting from feed forward models Bengio et al 2000 recurrent neural networks Elman 1990 Mikolov et al 2010 and LSTMs Hochreiter and Schmidhuber 1997 Graves 2013 More recently transformer networks based on self attention have led to important improvements especially for capturing long range dependencies Vaswani et al 2017 Radford et al 2018 Dai et al 2019 Scaling There is a long history of scaling for language models for both the model and dataset sizes Brants et al 2007 showed the benefits of using language models trained on 2 trillion tokens resulting in 300 billion n grams on the quality of machine translation While this work relied on a simple smoothing technique called Stupid Backoff\n"
            },
            {
                "rank": 1,
                "index": 4,
                "score": 0.21321331,
                "text": "as T Fixup Huang et al 2020 and DeepNet Wang et al 2022 to reduce variance and stabilize training In this study we focus on the first category and propose a new architecture for Transformer models to address the drawbacks of both variants while retaining their benefits Figure 1 c provides an overview of our method Our design goal is to maintain the advantages of both variants and avoid their disadvantages by employing two residual connections In particular our ResiDual model utilizes the Pre Post LN PPLN that consists two residuals one is similar to the Pre LN to prevent the gradient vanishing issue while the other one akin to the Post LN which sustains representation diversity to avoid the representation collapse issue 2 To validate the effectiveness\n"
            },
            {
                "rank": 2,
                "index": 5,
                "score": 0.014227046,
                "text": "the proof it appears that the reason transformers are not Turing complete has to do with the way in which they are used in particular autoregressively generating a sequence limits the amount of computation the model can do It has already been shown that allowing transformers to do more computation allows for better results Wei et al 2022 Wang et al 2022 Yao et al 2023 Recent work has augmented the ways in which we use models by giving them access to additional forms of memory and tools creating AI agents Borgeaud et al 2021 Bertsch et al 2023 Modarressi et al 2023 Schick et al 2023 Wang et al 2023b Agents appear to be a natural solution to the problem that transformers are not Turing Complete by\n"
            },
            {
                "rank": 3,
                "index": 2,
                "score": 0.012970386,
                "text": "architectures Vision Transformers ResNets and hybrids Vision Transformers generally outperform ResNets with the same computational budget Hybrids improve upon pure Transformers for smaller model sizes but the gap vanishes for larger models region spanned by BiT models of different sizes The BiT CNNs outperform ViT on ImageNet but with the larger datasets ViT overtakes Second we train our models on random subsets of 9M 30M and 90M as well as the full JFT300M dataset We do not perform additional regularization on the smaller subsets and use the same hyper parameters for all settings This way we assess the intrinsic model properties and not the effect of regularization We do however use early stopping and report the best validation accuracy achieved during training To save compute we report few\n"
            },
            {
                "rank": 4,
                "index": 8,
                "score": 0.0058881594,
                "text": "context learning instruction tuning RLHF quantization and so on We are particularly interested in whether Transformer alternatives such as SSMs have similar properties and affordances Scaling Our empirical evaluation is limited to small model sizes below the threshold of most strong open source LLMs e g Llama Touvron et al 2023 as well as other recurrent models such as RWKV B Peng et al 2023 and RetNet Y Sun et al 2023 which have been evaluated at the 7B parameter scale and beyond It remains to assess whether Mamba still compares favorably at these larger sizes We also note that scaling SSMs may involve further engineering challenges and adjustments to the model that are not discussed in this paper 6 Conclusion We introduce a selection mechanism to structured\n"
            },
            {
                "rank": 5,
                "index": 0,
                "score": 0.002878348,
                "text": "For example through the use of conditional computation large MoE models like the 1 7 trillion parameter Switch transformer Fedus et al 2021 the 1 2 Trillion parameter GLaM model Du et al 2021 and others Artetxe et al 2021 Zoph et al 2022 are able to provide a large effective model size despite using relatively fewer training and inference FLOPs However for very large models the computational benefits of routed models seems to diminish Clark et al 2022 An orthogonal approach to improving language models is to augment transformers with explicit retrieval mechanisms as done by Borgeaud et al 2021 Guu et al 2020 Lewis et al 2020 This approach effectively increases the number of data tokens seen during training by a factor of 10in Borgeaud et\n"
            },
            {
                "rank": 6,
                "index": 1,
                "score": 0.0027044294,
                "text": "the model Given the importance of large autoregressive models and specifically large Transformers several approaches were Equal contribution1Google Research Mountain View CA USA Correspondence to Yaniv Leviathan leviathan google com Proceedings of the 40thInternational Conference on Machine Learning Honolulu Hawaii USA PMLR 202 2023 Copyright 2023 by the author s developed to make inference from them faster Some approaches aim to reduce the inference cost for allinputs equally e g Hinton et al 2015 Jaszczur et al 2021 Hubara et al 2016 So et al 2021 Shazeer 2019 Other approaches stem from the observation that not all inference steps are born alike some require a very large model while others can be approximated well by more efficient models These adaptive computation methods e g Han et al 2021\n"
            },
            {
                "rank": 7,
                "index": 7,
                "score": 0.0009510131,
                "text": "2018 Brock et al 2021 Martens et al 2021 Zaidi et al 2023 However the current theory only considers a model at initialisation and often considers only the initial forward pass As such signal propagation at present is unable to shed light on many intricacies of deep NN training dynamics for example the benefits of skip connections for training speed Though signal propagation is crucial in motivating our modifications we would not have arrived at our simplified transformer blocks from theory alone and relied also on empirical insights Correspondence to bobby he inf ethz ch 1arXiv 2311 01906v1 cs LG 3 Nov 2023 KAttentionH x Q VProjNormMLP InMLP Out MLP OutKShaped AttentionH x Q MLP InNonLinMLP Out NonLin NormKAttentionH x Q VProj NonLin Norm Pre LN ParallelOurs MLP\n"
            },
            {
                "rank": 8,
                "index": 9,
                "score": 0.00050137425,
                "text": "Our method can accelerate existing off the shelf models without retraining or architecture changes We demonstrate it on T5 XXL and show a 2X 3X acceleration compared to the standard T5X implementation with identical outputs 1 Introduction Large autoregressive models notably large Transformers Vaswani et al 2017 are much more capable than smaller models as is evidenced countless times in recent years e g in the text or image domains like GPT 3 Brown et al 2020 LaMDA Thoppilan et al 2022 Parti Yu et al 2022 and PaLM Chowdhery et al 2022 Unfortunately a single decode step from these larger models is significantly slower than a step from their smaller counterparts and making things worse these steps are done serially decoding K tokens takes Kserial runs of\n"
            },
            {
                "rank": 9,
                "index": 3,
                "score": 0.00034464317,
                "text": "models are more efficient only until a certain model size In this work we challenge that claim by considering a variable optimal dataset size for both model families see Section 6 3 3 B ACKGROUND 3 1 M ODEL ARCHITECTURE Transformer A standard decoder only Transformer Radford et al 2018a b Kaplan et al 2020 Brown et al 2020 consists of an embedding layer a stack of alternating attention and feed forward layers and an unembedding layer In the model each input token is converted by the embedding layer into a vector of size dmodel the dimension maintained across all the layers in the residual stream The feed forward component consists of two linear transformations and a nonlinearity in between It can be described as FFN x xW1\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": 0.8023815751075745,
                "text": "at the national US average of 0 385 kg CO 2e per KWh starting from feed forward models Bengio et al 2000 recurrent neural networks Elman 1990 Mikolov et al 2010 and LSTMs Hochreiter and Schmidhuber 1997 Graves 2013 More recently transformer networks based on self attention have led to important improvements especially for capturing long range dependencies Vaswani et al 2017 Radford et al 2018 Dai et al 2019 Scaling There is a long history of scaling for language models for both the model and dataset sizes Brants et al 2007 showed the benefits of using language models trained on 2 trillion tokens resulting in 300 billion n grams on the quality of machine translation While this work relied on a simple smoothing technique called Stupid Backoff\n"
            },
            {
                "rank": 1,
                "score": -1.198831558227539,
                "text": "as T Fixup Huang et al 2020 and DeepNet Wang et al 2022 to reduce variance and stabilize training In this study we focus on the first category and propose a new architecture for Transformer models to address the drawbacks of both variants while retaining their benefits Figure 1 c provides an overview of our method Our design goal is to maintain the advantages of both variants and avoid their disadvantages by employing two residual connections In particular our ResiDual model utilizes the Pre Post LN PPLN that consists two residuals one is similar to the Pre LN to prevent the gradient vanishing issue while the other one akin to the Post LN which sustains representation diversity to avoid the representation collapse issue 2 To validate the effectiveness\n"
            },
            {
                "rank": 2,
                "score": -1.490297794342041,
                "text": "context learning instruction tuning RLHF quantization and so on We are particularly interested in whether Transformer alternatives such as SSMs have similar properties and affordances Scaling Our empirical evaluation is limited to small model sizes below the threshold of most strong open source LLMs e g Llama Touvron et al 2023 as well as other recurrent models such as RWKV B Peng et al 2023 and RetNet Y Sun et al 2023 which have been evaluated at the 7B parameter scale and beyond It remains to assess whether Mamba still compares favorably at these larger sizes We also note that scaling SSMs may involve further engineering challenges and adjustments to the model that are not discussed in this paper 6 Conclusion We introduce a selection mechanism to structured\n"
            },
            {
                "rank": 3,
                "score": -2.266940116882324,
                "text": "2018 Brock et al 2021 Martens et al 2021 Zaidi et al 2023 However the current theory only considers a model at initialisation and often considers only the initial forward pass As such signal propagation at present is unable to shed light on many intricacies of deep NN training dynamics for example the benefits of skip connections for training speed Though signal propagation is crucial in motivating our modifications we would not have arrived at our simplified transformer blocks from theory alone and relied also on empirical insights Correspondence to bobby he inf ethz ch 1arXiv 2311 01906v1 cs LG 3 Nov 2023 KAttentionH x Q VProjNormMLP InMLP Out MLP OutKShaped AttentionH x Q MLP InNonLinMLP Out NonLin NormKAttentionH x Q VProj NonLin Norm Pre LN ParallelOurs MLP\n"
            },
            {
                "rank": 4,
                "score": -2.580446243286133,
                "text": "For example through the use of conditional computation large MoE models like the 1 7 trillion parameter Switch transformer Fedus et al 2021 the 1 2 Trillion parameter GLaM model Du et al 2021 and others Artetxe et al 2021 Zoph et al 2022 are able to provide a large effective model size despite using relatively fewer training and inference FLOPs However for very large models the computational benefits of routed models seems to diminish Clark et al 2022 An orthogonal approach to improving language models is to augment transformers with explicit retrieval mechanisms as done by Borgeaud et al 2021 Guu et al 2020 Lewis et al 2020 This approach effectively increases the number of data tokens seen during training by a factor of 10in Borgeaud et\n"
            },
            {
                "rank": 5,
                "score": -3.073892593383789,
                "text": "the proof it appears that the reason transformers are not Turing complete has to do with the way in which they are used in particular autoregressively generating a sequence limits the amount of computation the model can do It has already been shown that allowing transformers to do more computation allows for better results Wei et al 2022 Wang et al 2022 Yao et al 2023 Recent work has augmented the ways in which we use models by giving them access to additional forms of memory and tools creating AI agents Borgeaud et al 2021 Bertsch et al 2023 Modarressi et al 2023 Schick et al 2023 Wang et al 2023b Agents appear to be a natural solution to the problem that transformers are not Turing Complete by\n"
            },
            {
                "rank": 6,
                "score": -3.5698418617248535,
                "text": "the model Given the importance of large autoregressive models and specifically large Transformers several approaches were Equal contribution1Google Research Mountain View CA USA Correspondence to Yaniv Leviathan leviathan google com Proceedings of the 40thInternational Conference on Machine Learning Honolulu Hawaii USA PMLR 202 2023 Copyright 2023 by the author s developed to make inference from them faster Some approaches aim to reduce the inference cost for allinputs equally e g Hinton et al 2015 Jaszczur et al 2021 Hubara et al 2016 So et al 2021 Shazeer 2019 Other approaches stem from the observation that not all inference steps are born alike some require a very large model while others can be approximated well by more efficient models These adaptive computation methods e g Han et al 2021\n"
            },
            {
                "rank": 7,
                "score": -3.663217544555664,
                "text": "models are more efficient only until a certain model size In this work we challenge that claim by considering a variable optimal dataset size for both model families see Section 6 3 3 B ACKGROUND 3 1 M ODEL ARCHITECTURE Transformer A standard decoder only Transformer Radford et al 2018a b Kaplan et al 2020 Brown et al 2020 consists of an embedding layer a stack of alternating attention and feed forward layers and an unembedding layer In the model each input token is converted by the embedding layer into a vector of size dmodel the dimension maintained across all the layers in the residual stream The feed forward component consists of two linear transformations and a nonlinearity in between It can be described as FFN x xW1\n"
            },
            {
                "rank": 8,
                "score": -4.485232353210449,
                "text": "Our method can accelerate existing off the shelf models without retraining or architecture changes We demonstrate it on T5 XXL and show a 2X 3X acceleration compared to the standard T5X implementation with identical outputs 1 Introduction Large autoregressive models notably large Transformers Vaswani et al 2017 are much more capable than smaller models as is evidenced countless times in recent years e g in the text or image domains like GPT 3 Brown et al 2020 LaMDA Thoppilan et al 2022 Parti Yu et al 2022 and PaLM Chowdhery et al 2022 Unfortunately a single decode step from these larger models is significantly slower than a step from their smaller counterparts and making things worse these steps are done serially decoding K tokens takes Kserial runs of\n"
            },
            {
                "rank": 9,
                "score": -4.815124988555908,
                "text": "architectures Vision Transformers ResNets and hybrids Vision Transformers generally outperform ResNets with the same computational budget Hybrids improve upon pure Transformers for smaller model sizes but the gap vanishes for larger models region spanned by BiT models of different sizes The BiT CNNs outperform ViT on ImageNet but with the larger datasets ViT overtakes Second we train our models on random subsets of 9M 30M and 90M as well as the full JFT300M dataset We do not perform additional regularization on the smaller subsets and use the same hyper parameters for all settings This way we assess the intrinsic model properties and not the effect of regularization We do however use early stopping and report the best validation accuracy achieved during training To save compute we report few\n"
            }
        ]
    },
    {
        "query": "How have large language models influenced the field of natural language processing?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 3,
                "score": 0.8690492,
                "text": "tasks In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing pages 50855109 Y Wang Y Zhao and L Petzold 2023 Are large language models ready for healthcare a comparative study on clinical language understanding Z Wang S Cai A Liu X Ma and Y Liang 2023 Describe explain plan and select Interactive planning with large language models enables open world multi task agents arXiv preprint arXiv 2302 01560 Z Wang J Wohlwend and T Lei 2019 Structured pruning of large language models arXiv preprint arXiv 1910 04732 Z Wang Z Dai B Pczos and J Carbonell 2019 Characterizing and avoiding negative transfer In Proceedings of the IEEE CVF conference on computer vision and pattern recognition pages 1129311302 Z Wang M Zoghi F Hutter D\n"
            },
            {
                "rank": 1,
                "index": 2,
                "score": 0.1968262,
                "text": "This success has led to a number of studies exploring the possibilities of further scaling these models Scao et al 2022 Zhang et al 2022 Du et al 2021 Zeng et al 2022 Lepikhin et al 2020 Fedus et al 2022 Du et al 2022 Black et al 2022 Rae et al 2021 Hoffmann et al 2022 Chowdhery et al 2022 Thoppilan et al 2022 As a result the community has come to view these large language models as essential foundations for downstream models Bommasani et al 2021 The birth of ChatGPT OpenAI 2022 and the subsequent launch of GPT 4 OpenAI 2023 marked two historic moments in the field of artificial intelligence demonstrating that large language models LLMs can serve as effective AI assistants capable of communicating\n"
            },
            {
                "rank": 2,
                "index": 1,
                "score": 0.1786691,
                "text": "2020 A significant breakthrough was obtained with GPT 3 Brown et al 2020 a model with 175 billion parameters This lead to a series of Large Language Models such as Jurassic 1 Lieber et al 2021 Megatron Turing NLG Smith et al 2022 Gopher Rae et al 2021 Chinchilla Hoffmann et al 2022 PaLM Chowdhery et al 2022 OPT Zhang et al 2022 and GLM Zeng et al 2022 Hestness et al 2017 and Rosenfeld et al 2019 studied the impact of scaling on the performance of deep learning models showing the existence of power laws between the model and dataset sizes and the performance of the system Kaplan et al 2020 derived power laws specifically for transformer based language models which were later refined by Hoffmann et\n"
            },
            {
                "rank": 3,
                "index": 5,
                "score": 0.09122006,
                "text": "Sastry A Askell et al Language models are few shot learners Advances in neural information processing systems 33 18771901 2020 S Chen S Wong L Chen and Y Tian Extending context window of large language models via positional interpolation arXiv preprint arXiv 2306 15595 2023 11 Transformers Can Achieve Length Generalization But Not Robustly T C Chi T H Fan P J Ramadge and A Rudnicky Kerple Kernelized relative positional embedding for length extrapolation Advances in Neural Information Processing Systems 35 83868399 2022 A Chowdhery S Narang J Devlin M Bosma G Mishra A Roberts P Barham H W Chung C Sutton S Gehrmann et al Palm Scaling language modeling with pathways Journal of Machine Learning Research 24 240 1113 2023 Z Dai Z Yang Y Yang J\n"
            },
            {
                "rank": 4,
                "index": 7,
                "score": 0.06766981,
                "text": "high level andmulti task zero shot capabilities hidden inside those models 2 Background We briefly review the two core preliminary concepts that form the basis of this work the advent of large language models LLMs and prompting and chain of thought CoT prompting for multi step reasoning Large language models and prompting A language model LM is a model that looks to estimate the probability distribution over text Recently scaling improvements through larger model sizes from a few million Merity et al 2016 to hundreds of millions Devlin et al 2019 to hundreds of billions Brown et al 2020 parameters and larger data e g webtext corpora Gao et al 2020 have enabled pre trained large language models LLMs to be incredibly adept at many downstream NLP tasks\n"
            },
            {
                "rank": 5,
                "index": 8,
                "score": 0.046291903,
                "text": "to sequence architecture that makes heavy use of self attention Radford et al a applied it to autoregressive language modeling by using a stack of Transformer decoders Since then Transformer based language models have dominated NLP achieving the state of the art in many tasks A new paradigm emerged with BERT Devlin et al 2019b and GPT 2 Radford et al b both are large Transformer lan8 guage models trained on a large amount of text where fine tuning on task specific data after pretraining on general domain data provides a significant performance gain compared to training on task specific data directly Training larger Transformers generally results in better performance and remains an active research direction GPT 3 Brown et al 2020 is the largest single Transformer language\n"
            },
            {
                "rank": 6,
                "index": 6,
                "score": 0.034164816,
                "text": "et al 2020 Ouyang et al 2022 or encoder decoder training Liu et al 2020 Chen et al 2021 Xue et al 2021 pre trained Transformer based large language models have shown impressive performance on multiple NLP tasks across languages Previous work Zhao Schtze 2021 Winata et al 2021 Lin et al 2021b investigated prompting in the multilingual setting and found that using English prompts with non English examples led to strong few shot performance Evaluation of multilingual models has mostly focused on general information extraction tasks such as question answering Clark et al 2020 Hu et al 2020 Kassner et al 2021 Ruder Sil 2021 as well as specific types of reasoning such as commonsense reasoning Ponti et al 2020 Lin et al 2021a and temporal reasoning\n"
            },
            {
                "rank": 7,
                "index": 0,
                "score": 0.027585283,
                "text": "retrieval language models 1 Introduction Large language models LLMs are impressive few shot learners Brown et al 2020 Rae et al 2021 Hoffmann et al 2022 Chowdhery et al 2022 They are able to learn new tasks with very few examples or even from instructions alone For this generalisation ability to emerge the key ingredients are scaling both the parameter count of the model and the size of the training data Large language models owe this improvement to both a larger computational budget enabling more complex reasoning and the ability to memorize more Equal contribution Work done while at Meta AI c2023 Gautier Izacard Patrick Lewis Maria Lomeli Lucas Hosseini Fabio Petroni Timo Schick Jane Dwivedi Yu Armand Joulin Sebastian Riedel Edouard Grave License CC BY 4 0\n"
            },
            {
                "rank": 8,
                "index": 4,
                "score": 0.017576883,
                "text": "Smith et al 2022 Thoppilan et al 2022 with the largest dense language models now having over 500 billion parameters These large autoregressive transformers Vaswani et al 2017 have demonstrated impressive performance on many tasks using a variety of evaluation protocols such as zero shot few shot and fine tuning The compute and energy cost for training large language models is substantial Rae et al 2021 Thoppilan et al 2022 and rises with increasing model size In practice the allocated training compute budget is often known in advance how many accelerators are available and for how long we want to use them Since it is typically only feasible to train these large models once accurately estimating the best model hyperparameters for a given compute budget is critical Tay\n"
            },
            {
                "rank": 9,
                "index": 9,
                "score": 0.0131212715,
                "text": "36th International Conference on Machine Learning volume 97 of Proceedings of Machine Learning Research pages 23372346 PMLR Z Gou Z Shao Y Gong Y Shen Y Yang N Duan and W Chen 2023 Critic Large language models can self correct with tool interactive critiquing arXiv preprint arXiv 2305 11738 K Greshake S Abdelnabi S Mishra C Endres T Holz and M Fritz 2023 More than youve asked for A comprehensive analysis of novel prompt injection threats to application integrated large language models arXiv preprint arXiv 2302 12173 L D Griffin B Kleinberg M Mozes K T Mai M Vau M Caldwell and A Marvor Parker 2023 Susceptibility to influence of large language models arXiv preprint arXiv 2303 06074 Y Gu R Tinn H Cheng M Lucas N Usuyama\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": 0.08625462651252747,
                "text": "tasks In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing pages 50855109 Y Wang Y Zhao and L Petzold 2023 Are large language models ready for healthcare a comparative study on clinical language understanding Z Wang S Cai A Liu X Ma and Y Liang 2023 Describe explain plan and select Interactive planning with large language models enables open world multi task agents arXiv preprint arXiv 2302 01560 Z Wang J Wohlwend and T Lei 2019 Structured pruning of large language models arXiv preprint arXiv 1910 04732 Z Wang Z Dai B Pczos and J Carbonell 2019 Characterizing and avoiding negative transfer In Proceedings of the IEEE CVF conference on computer vision and pattern recognition pages 1129311302 Z Wang M Zoghi F Hutter D\n"
            },
            {
                "rank": 1,
                "score": -0.4285343885421753,
                "text": "retrieval language models 1 Introduction Large language models LLMs are impressive few shot learners Brown et al 2020 Rae et al 2021 Hoffmann et al 2022 Chowdhery et al 2022 They are able to learn new tasks with very few examples or even from instructions alone For this generalisation ability to emerge the key ingredients are scaling both the parameter count of the model and the size of the training data Large language models owe this improvement to both a larger computational budget enabling more complex reasoning and the ability to memorize more Equal contribution Work done while at Meta AI c2023 Gautier Izacard Patrick Lewis Maria Lomeli Lucas Hosseini Fabio Petroni Timo Schick Jane Dwivedi Yu Armand Joulin Sebastian Riedel Edouard Grave License CC BY 4 0\n"
            },
            {
                "rank": 2,
                "score": -0.994682788848877,
                "text": "This success has led to a number of studies exploring the possibilities of further scaling these models Scao et al 2022 Zhang et al 2022 Du et al 2021 Zeng et al 2022 Lepikhin et al 2020 Fedus et al 2022 Du et al 2022 Black et al 2022 Rae et al 2021 Hoffmann et al 2022 Chowdhery et al 2022 Thoppilan et al 2022 As a result the community has come to view these large language models as essential foundations for downstream models Bommasani et al 2021 The birth of ChatGPT OpenAI 2022 and the subsequent launch of GPT 4 OpenAI 2023 marked two historic moments in the field of artificial intelligence demonstrating that large language models LLMs can serve as effective AI assistants capable of communicating\n"
            },
            {
                "rank": 3,
                "score": -1.1372071504592896,
                "text": "et al 2020 Ouyang et al 2022 or encoder decoder training Liu et al 2020 Chen et al 2021 Xue et al 2021 pre trained Transformer based large language models have shown impressive performance on multiple NLP tasks across languages Previous work Zhao Schtze 2021 Winata et al 2021 Lin et al 2021b investigated prompting in the multilingual setting and found that using English prompts with non English examples led to strong few shot performance Evaluation of multilingual models has mostly focused on general information extraction tasks such as question answering Clark et al 2020 Hu et al 2020 Kassner et al 2021 Ruder Sil 2021 as well as specific types of reasoning such as commonsense reasoning Ponti et al 2020 Lin et al 2021a and temporal reasoning\n"
            },
            {
                "rank": 4,
                "score": -1.3777663707733154,
                "text": "Sastry A Askell et al Language models are few shot learners Advances in neural information processing systems 33 18771901 2020 S Chen S Wong L Chen and Y Tian Extending context window of large language models via positional interpolation arXiv preprint arXiv 2306 15595 2023 11 Transformers Can Achieve Length Generalization But Not Robustly T C Chi T H Fan P J Ramadge and A Rudnicky Kerple Kernelized relative positional embedding for length extrapolation Advances in Neural Information Processing Systems 35 83868399 2022 A Chowdhery S Narang J Devlin M Bosma G Mishra A Roberts P Barham H W Chung C Sutton S Gehrmann et al Palm Scaling language modeling with pathways Journal of Machine Learning Research 24 240 1113 2023 Z Dai Z Yang Y Yang J\n"
            },
            {
                "rank": 5,
                "score": -1.481506586074829,
                "text": "2020 A significant breakthrough was obtained with GPT 3 Brown et al 2020 a model with 175 billion parameters This lead to a series of Large Language Models such as Jurassic 1 Lieber et al 2021 Megatron Turing NLG Smith et al 2022 Gopher Rae et al 2021 Chinchilla Hoffmann et al 2022 PaLM Chowdhery et al 2022 OPT Zhang et al 2022 and GLM Zeng et al 2022 Hestness et al 2017 and Rosenfeld et al 2019 studied the impact of scaling on the performance of deep learning models showing the existence of power laws between the model and dataset sizes and the performance of the system Kaplan et al 2020 derived power laws specifically for transformer based language models which were later refined by Hoffmann et\n"
            },
            {
                "rank": 6,
                "score": -1.6181919574737549,
                "text": "high level andmulti task zero shot capabilities hidden inside those models 2 Background We briefly review the two core preliminary concepts that form the basis of this work the advent of large language models LLMs and prompting and chain of thought CoT prompting for multi step reasoning Large language models and prompting A language model LM is a model that looks to estimate the probability distribution over text Recently scaling improvements through larger model sizes from a few million Merity et al 2016 to hundreds of millions Devlin et al 2019 to hundreds of billions Brown et al 2020 parameters and larger data e g webtext corpora Gao et al 2020 have enabled pre trained large language models LLMs to be incredibly adept at many downstream NLP tasks\n"
            },
            {
                "rank": 7,
                "score": -1.6911033391952515,
                "text": "36th International Conference on Machine Learning volume 97 of Proceedings of Machine Learning Research pages 23372346 PMLR Z Gou Z Shao Y Gong Y Shen Y Yang N Duan and W Chen 2023 Critic Large language models can self correct with tool interactive critiquing arXiv preprint arXiv 2305 11738 K Greshake S Abdelnabi S Mishra C Endres T Holz and M Fritz 2023 More than youve asked for A comprehensive analysis of novel prompt injection threats to application integrated large language models arXiv preprint arXiv 2302 12173 L D Griffin B Kleinberg M Mozes K T Mai M Vau M Caldwell and A Marvor Parker 2023 Susceptibility to influence of large language models arXiv preprint arXiv 2303 06074 Y Gu R Tinn H Cheng M Lucas N Usuyama\n"
            },
            {
                "rank": 8,
                "score": -1.7637580633163452,
                "text": "to sequence architecture that makes heavy use of self attention Radford et al a applied it to autoregressive language modeling by using a stack of Transformer decoders Since then Transformer based language models have dominated NLP achieving the state of the art in many tasks A new paradigm emerged with BERT Devlin et al 2019b and GPT 2 Radford et al b both are large Transformer lan8 guage models trained on a large amount of text where fine tuning on task specific data after pretraining on general domain data provides a significant performance gain compared to training on task specific data directly Training larger Transformers generally results in better performance and remains an active research direction GPT 3 Brown et al 2020 is the largest single Transformer language\n"
            },
            {
                "rank": 9,
                "score": -2.7067461013793945,
                "text": "Smith et al 2022 Thoppilan et al 2022 with the largest dense language models now having over 500 billion parameters These large autoregressive transformers Vaswani et al 2017 have demonstrated impressive performance on many tasks using a variety of evaluation protocols such as zero shot few shot and fine tuning The compute and energy cost for training large language models is substantial Rae et al 2021 Thoppilan et al 2022 and rises with increasing model size In practice the allocated training compute budget is often known in advance how many accelerators are available and for how long we want to use them Since it is typically only feasible to train these large models once accurately estimating the best model hyperparameters for a given compute budget is critical Tay\n"
            }
        ]
    },
    {
        "query": "What are the potential applications of structural biology research in drug discovery?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 7,
                "score": 0.23022676,
                "text": "Leading Edge Commentary Enabling structure based drug discovery utilizing predicted models Edward B Miller 1 Howook Hwang 1Mee Shelley 2Andrew Placzek 2Joao P G L M Rodrigues 1Robert K Suto 3 Lingle Wang 1Karen Akinsanya 1and Robert Abel1 1Schro dinger New York 1540 Broadway 24th Floor New York NY 10036 USA 2Schro dinger Portland 101 SW Main Street Suite 1300 Portland OR 97204 USA 3Schro dinger Framingham 200 Staples Drive Suite 210 Framingham MA 01702 USA Correspondence ed miller schrodinger com https doi org 10 1016 j cell 2023 12 034 High quality predicted structures enable structure based approaches to an expanding number of drug discovery programs We propose that by utilizing free energy perturbation FEP predicted structures can be confidently employed to achieve drug design goals We\n"
            },
            {
                "rank": 1,
                "index": 2,
                "score": 0.03567855,
                "text": "therapeutically relevant state The challenge with structure based drug design is being able to obtain the right structure in the disease relevant state bound with project chemical matter As an example we point to the experimental structural biology pursuits around the leucine rich repeat kinase 2 LRRK2 Mutants of LRRK2 have been implicated in Parkinsons disease Structures have been obtained of inactive LRRK2 with out an inhibitor as a monomer PDB 7LHW and as a dimer PDB 7LHT as well as the G2019S mutant PDB 7LI3 Later an active type 1 inhibitor boundstructure was published PDB 8TXZ as well as an inactive state with a type 2 inhibitor PDB 8TZE Functionally LRRK2is associated with cellular trafficking and a structure of microtubule bound LRRK2 was also recently published PDB\n"
            },
            {
                "rank": 2,
                "index": 6,
                "score": 0.0001823546,
                "text": "Protein kinases and in particular the DFG in to DFG out transition have been extensively studied using MD along with several enhanced sampling methods 35 36This family of enzymes is one of the most important therapeutic targets for structure based drug design as they are ubiquitous in the human proteome 37Their main role is to mediate cell signalling in a large range of biomolecular processes at the cell level in particular replication 4 hence implicating them in a majority of cancers While there already exist several highly effective medicinal molecules for cancer therapy that function by targeting and inhibiting kinases 38one could argue that at best we have scratched the surface in terms of kinase based therapeutics 39 In their active state protein kinases catalyze the phosphorylation of\n"
            },
            {
                "rank": 3,
                "index": 8,
                "score": 3.2444816e-05,
                "text": "B Kao A Bartnik I Tom Dieck S and Schuman E M 2021 The translatome of neuronalcell bodies dendrites and axons Proc Natl Acad Sci USA 118 e2113929118 83 Ruwolt M Piazza I and Liu F 2023 The potential of cross linking mass spectrometry in the development of protein protein interactionmodulators Curr Opin Struct Biol 82 102648 84 Stahl K Graziadei A Dau T Brock O and Rappsilber J 2023 Protein structure prediction with in cell photo crosslinking mass spectrom etry and deep learning Nat Biotechnol 41 18101819 85 Chen Z A and Rappsilber J 2023 Protein structure dynamics by crosslinking mass spectrometry Curr Opin Struct Biol 80 102599 86 Alexandrov T 2020 Spatial metabolomics and imaging mass spectrometry in the age of artificial intelligence Annu Rev Biomed\n"
            },
            {
                "rank": 4,
                "index": 9,
                "score": 3.2444816e-05,
                "text": "B Kao A Bartnik I Tom Dieck S and Schuman E M 2021 The translatome of neuronalcell bodies dendrites and axons Proc Natl Acad Sci USA 118 e2113929118 83 Ruwolt M Piazza I and Liu F 2023 The potential of cross linking mass spectrometry in the development of protein protein interactionmodulators Curr Opin Struct Biol 82 102648 84 Stahl K Graziadei A Dau T Brock O and Rappsilber J 2023 Protein structure prediction with in cell photo crosslinking mass spectrom etry and deep learning Nat Biotechnol 41 18101819 85 Chen Z A and Rappsilber J 2023 Protein structure dynamics by crosslinking mass spectrometry Curr Opin Struct Biol 80 102599 86 Alexandrov T 2020 Spatial metabolomics and imaging mass spectrometry in the age of artificial intelligence Annu Rev Biomed\n"
            },
            {
                "rank": 5,
                "index": 1,
                "score": 1.8631747e-05,
                "text": "-but-not-always-truth_cell.pdf coma J Med Chem 32 25102513 7 Wankowicz S A de Oliveira S H Hogan D W van den Bedem H and Fraser J S 2022 Ligand binding remodels protein sidechain conformational heterogeneity Elife 11 e74114 https doi org 10 7554 eLife 74114 8 Copeland R A 2016 The drug target residence time model a 10 year retrospective Nat Rev Drug Discov 15 8795 9 Hekkelman M L de Vries I Joosten R P and Perrakis A 2023 AlphaFill enriching Al phaFold models with ligands and cofactors Nat Methods 20 205213 10 Mitrea D M Mittasch M Gomes B F Klein I A and Murcko M A 2022 Modulating bio molecular condensates a novel approach todrug discovery Nat Rev Drug Discov 21 841862 11 Xue L Spahn C\n"
            },
            {
                "rank": 6,
                "index": 5,
                "score": 1.7917988e-05,
                "text": "The calcium free form of calmodulin was probably among very few proteins that you couldnt solve its structure by X ray or by cryo EM even today It was unique Therefore it drew my interest I did a very short post doctoraltraining figured out the apo calmodulin structure and the structure told us why Karin Ku hnel You told us how you became interested in structural biology Can you tell us more about the scientific research in your laboratory now BW My lab is focusing on signal transduction and functional modulation of GPCR which is an important membrane protein family related to lots of diseases By performing structural andfunctional studies we are trying to provide a full picture of how these receptors recognize diverse ligands how the receptors\n"
            },
            {
                "rank": 7,
                "index": 4,
                "score": 7.0168617e-06,
                "text": "al 2020 Large Scale Assessment ofBinding Free Energy Calculations in ActiveDrug Discovery Projects J Chem Inf Model 60 54575474 9 Leit S Greenwood J Carriero S Mondal S Abel R Ashwell M Blanchette H Boy les N A Cartwright M Collis A et al 2023 Discovery of a Potent and SelectiveTyrosine Kinase 2 Inhibitor TAK 279 J Med Chem 66 1047310496 10 Xu T Zhu K Beautrait A Vendome J Borrelli K W Abel R Friesner R A and Miller E B 2022 Induced Fit Docking Enables Ac curate Free Energy Perturbation Calculationsin Homology Models J Chem Theory Comput 18 57105724 11 Fajer M Borrelli K Abel R and Wang L 2023 Quantitatively Accounting for ProteinReorganization in Computer Aided Drug Des ign J Chem Theory Comput 19 30803090\n"
            },
            {
                "rank": 8,
                "index": 0,
                "score": 4.710909e-06,
                "text": "biology Trends Biochem Sci 40 4957 2015 3 Su X D et al Protein crystallography from the perspective of technology developments Crystallogr Rev 21 1 2 2 153 2014 4 Wthrich K Protein structure determination in solution by NMR spectroscopy J Biol Chem 265 22059 22062 1990 5 Slabinski L et al The challenge of protein structure determinationlessons from structural genomics Protein Sci 16 2472 2482 2007 6 Leach A Thomas P Protein structure prediction and homology modeling In Comprehensive Medicinal Chemistry III 1 2 0 144 Elsevier 2017 https doi org 10 1016 b978 0 12 409547 2 12350 9 Table 1 Summary of parameters used for enhanced sampling molecular dynamics simulations of Abl1 inactivation Box dimensions 112 4 124 3 118 0 Total number of atoms\n"
            },
            {
                "rank": 9,
                "index": 3,
                "score": 3.1875652e-06,
                "text": "summaries source data extended data supplementary information acknowledgements peer review information details of author con tributions and competing interests and statements of data and code availability are available at https doi org 10 1038 s41586 021 03819 2 1 Thompson M C Yeates T O Rodriguez J A Advances in methods for atomic resolution macromolecular structure determination F1000Res 9 667 2020 2 Bai X C McMullan G Scheres S H W How cryo EM is revolutionizing structural biology Trends Biochem Sci 40 4957 2015 3 Jaskolski M Dauter Z Wlodawer A A brief history of macromolecular crystallography illustrated by a family tree and its Nobel fruits FEBS J 281 39854009 2014 4 Wthrich K The way to NMR structures of proteins Nat Struct Biol 8 923925 2001 5\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -1.0372003316879272,
                "text": "therapeutically relevant state The challenge with structure based drug design is being able to obtain the right structure in the disease relevant state bound with project chemical matter As an example we point to the experimental structural biology pursuits around the leucine rich repeat kinase 2 LRRK2 Mutants of LRRK2 have been implicated in Parkinsons disease Structures have been obtained of inactive LRRK2 with out an inhibitor as a monomer PDB 7LHW and as a dimer PDB 7LHT as well as the G2019S mutant PDB 7LI3 Later an active type 1 inhibitor boundstructure was published PDB 8TXZ as well as an inactive state with a type 2 inhibitor PDB 8TZE Functionally LRRK2is associated with cellular trafficking and a structure of microtubule bound LRRK2 was also recently published PDB\n"
            },
            {
                "rank": 1,
                "score": -1.8354419469833374,
                "text": "Leading Edge Commentary Enabling structure based drug discovery utilizing predicted models Edward B Miller 1 Howook Hwang 1Mee Shelley 2Andrew Placzek 2Joao P G L M Rodrigues 1Robert K Suto 3 Lingle Wang 1Karen Akinsanya 1and Robert Abel1 1Schro dinger New York 1540 Broadway 24th Floor New York NY 10036 USA 2Schro dinger Portland 101 SW Main Street Suite 1300 Portland OR 97204 USA 3Schro dinger Framingham 200 Staples Drive Suite 210 Framingham MA 01702 USA Correspondence ed miller schrodinger com https doi org 10 1016 j cell 2023 12 034 High quality predicted structures enable structure based approaches to an expanding number of drug discovery programs We propose that by utilizing free energy perturbation FEP predicted structures can be confidently employed to achieve drug design goals We\n"
            },
            {
                "rank": 2,
                "score": -3.644364356994629,
                "text": "The calcium free form of calmodulin was probably among very few proteins that you couldnt solve its structure by X ray or by cryo EM even today It was unique Therefore it drew my interest I did a very short post doctoraltraining figured out the apo calmodulin structure and the structure told us why Karin Ku hnel You told us how you became interested in structural biology Can you tell us more about the scientific research in your laboratory now BW My lab is focusing on signal transduction and functional modulation of GPCR which is an important membrane protein family related to lots of diseases By performing structural andfunctional studies we are trying to provide a full picture of how these receptors recognize diverse ligands how the receptors\n"
            },
            {
                "rank": 3,
                "score": -3.9421141147613525,
                "text": "summaries source data extended data supplementary information acknowledgements peer review information details of author con tributions and competing interests and statements of data and code availability are available at https doi org 10 1038 s41586 021 03819 2 1 Thompson M C Yeates T O Rodriguez J A Advances in methods for atomic resolution macromolecular structure determination F1000Res 9 667 2020 2 Bai X C McMullan G Scheres S H W How cryo EM is revolutionizing structural biology Trends Biochem Sci 40 4957 2015 3 Jaskolski M Dauter Z Wlodawer A A brief history of macromolecular crystallography illustrated by a family tree and its Nobel fruits FEBS J 281 39854009 2014 4 Wthrich K The way to NMR structures of proteins Nat Struct Biol 8 923925 2001 5\n"
            },
            {
                "rank": 4,
                "score": -4.227123260498047,
                "text": "-but-not-always-truth_cell.pdf coma J Med Chem 32 25102513 7 Wankowicz S A de Oliveira S H Hogan D W van den Bedem H and Fraser J S 2022 Ligand binding remodels protein sidechain conformational heterogeneity Elife 11 e74114 https doi org 10 7554 eLife 74114 8 Copeland R A 2016 The drug target residence time model a 10 year retrospective Nat Rev Drug Discov 15 8795 9 Hekkelman M L de Vries I Joosten R P and Perrakis A 2023 AlphaFill enriching Al phaFold models with ligands and cofactors Nat Methods 20 205213 10 Mitrea D M Mittasch M Gomes B F Klein I A and Murcko M A 2022 Modulating bio molecular condensates a novel approach todrug discovery Nat Rev Drug Discov 21 841862 11 Xue L Spahn C\n"
            },
            {
                "rank": 5,
                "score": -4.535126686096191,
                "text": "Protein kinases and in particular the DFG in to DFG out transition have been extensively studied using MD along with several enhanced sampling methods 35 36This family of enzymes is one of the most important therapeutic targets for structure based drug design as they are ubiquitous in the human proteome 37Their main role is to mediate cell signalling in a large range of biomolecular processes at the cell level in particular replication 4 hence implicating them in a majority of cancers While there already exist several highly effective medicinal molecules for cancer therapy that function by targeting and inhibiting kinases 38one could argue that at best we have scratched the surface in terms of kinase based therapeutics 39 In their active state protein kinases catalyze the phosphorylation of\n"
            },
            {
                "rank": 6,
                "score": -5.665134429931641,
                "text": "biology Trends Biochem Sci 40 4957 2015 3 Su X D et al Protein crystallography from the perspective of technology developments Crystallogr Rev 21 1 2 2 153 2014 4 Wthrich K Protein structure determination in solution by NMR spectroscopy J Biol Chem 265 22059 22062 1990 5 Slabinski L et al The challenge of protein structure determinationlessons from structural genomics Protein Sci 16 2472 2482 2007 6 Leach A Thomas P Protein structure prediction and homology modeling In Comprehensive Medicinal Chemistry III 1 2 0 144 Elsevier 2017 https doi org 10 1016 b978 0 12 409547 2 12350 9 Table 1 Summary of parameters used for enhanced sampling molecular dynamics simulations of Abl1 inactivation Box dimensions 112 4 124 3 118 0 Total number of atoms\n"
            },
            {
                "rank": 7,
                "score": -5.738905429840088,
                "text": "al 2020 Large Scale Assessment ofBinding Free Energy Calculations in ActiveDrug Discovery Projects J Chem Inf Model 60 54575474 9 Leit S Greenwood J Carriero S Mondal S Abel R Ashwell M Blanchette H Boy les N A Cartwright M Collis A et al 2023 Discovery of a Potent and SelectiveTyrosine Kinase 2 Inhibitor TAK 279 J Med Chem 66 1047310496 10 Xu T Zhu K Beautrait A Vendome J Borrelli K W Abel R Friesner R A and Miller E B 2022 Induced Fit Docking Enables Ac curate Free Energy Perturbation Calculationsin Homology Models J Chem Theory Comput 18 57105724 11 Fajer M Borrelli K Abel R and Wang L 2023 Quantitatively Accounting for ProteinReorganization in Computer Aided Drug Des ign J Chem Theory Comput 19 30803090\n"
            },
            {
                "rank": 8,
                "score": -6.152461051940918,
                "text": "B Kao A Bartnik I Tom Dieck S and Schuman E M 2021 The translatome of neuronalcell bodies dendrites and axons Proc Natl Acad Sci USA 118 e2113929118 83 Ruwolt M Piazza I and Liu F 2023 The potential of cross linking mass spectrometry in the development of protein protein interactionmodulators Curr Opin Struct Biol 82 102648 84 Stahl K Graziadei A Dau T Brock O and Rappsilber J 2023 Protein structure prediction with in cell photo crosslinking mass spectrom etry and deep learning Nat Biotechnol 41 18101819 85 Chen Z A and Rappsilber J 2023 Protein structure dynamics by crosslinking mass spectrometry Curr Opin Struct Biol 80 102599 86 Alexandrov T 2020 Spatial metabolomics and imaging mass spectrometry in the age of artificial intelligence Annu Rev Biomed\n"
            },
            {
                "rank": 9,
                "score": -6.152461051940918,
                "text": "B Kao A Bartnik I Tom Dieck S and Schuman E M 2021 The translatome of neuronalcell bodies dendrites and axons Proc Natl Acad Sci USA 118 e2113929118 83 Ruwolt M Piazza I and Liu F 2023 The potential of cross linking mass spectrometry in the development of protein protein interactionmodulators Curr Opin Struct Biol 82 102648 84 Stahl K Graziadei A Dau T Brock O and Rappsilber J 2023 Protein structure prediction with in cell photo crosslinking mass spectrom etry and deep learning Nat Biotechnol 41 18101819 85 Chen Z A and Rappsilber J 2023 Protein structure dynamics by crosslinking mass spectrometry Curr Opin Struct Biol 80 102599 86 Alexandrov T 2020 Spatial metabolomics and imaging mass spectrometry in the age of artificial intelligence Annu Rev Biomed\n"
            }
        ]
    },
    {
        "query": "Discuss the challenges and opportunities in applying transformer-based models to biological sequence data.",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 0,
                "score": 0.9026096,
                "text": "proposes the Transformer for protein language modeling Rives et al 2019 Rao et al 2019 Originally developed in the NLP community to represent long range context the main innovation of the Transformer model is its use of self attention Vaswani et al 2017 Self attention has particular relevance for the modeling of protein sequences Unlike convolutional and recurrent LSTM models the Transformer constructs a pairwise interaction map between all positions in the sequence In principle this mechanism has an ideal form to model residue residue contacts In theory end to end learning with a language model has advantages over the bioinformatics pipeline i it replaces the expensive query alignment and training steps with a single forward Work performed during an internship at Facebook 1Weights for all ESM 1\n"
            },
            {
                "rank": 1,
                "index": 6,
                "score": 0.47817853,
                "text": "generate comprehensive summaries Shaham et al 2022 otherwise models often miss important information Note that typical transformer models apply full attention to capture token dependencies pair wise It leads to a quadratic time and space complexity w r t input length However such a complexity is prohibitive for long sequences In particular it incurs massive memory consumption during the back propagation For example a transformer model with 250M parameters consumes over 80G GPU memory when sequence length is 8k Zuo et al 2022 To address this scalability issue various approaches have been proposed to reduce the complexity One approach is sparse attention which restricts each token to attend a subset of tokens based on predefined sparsity patterns Beltagy et al 2020 Zaheer et al 2020 Ainslie et al\n"
            },
            {
                "rank": 2,
                "index": 8,
                "score": 0.23405632,
                "text": "Sercu et al 2020 used amortized optimization to simultaneously predict profiles and pairwise couplings However prior work on protein language models has not considered sets of sequences as inputs to the language model 3 Methods Transformers are powerful sequence models capable of passing information from any position to any other position Vaswani et al 2017 However they are not trivially applied to a set of aligned sequences Naively concatenating Msequences of length Lin an MSA would allow attention across all sequences but the ML 2self attention maps would be prohibitively memory intensive The main con tribution of this paper is to extend transformer pre training to operate on an MSA while respecting its structure as an MLcharacter matrix We describe the input MSA as a matrix xRML where\n"
            },
            {
                "rank": 3,
                "index": 4,
                "score": 0.114566505,
                "text": "of 1Msynthetic sequences from MSA Transformer for RRM using the iterative sequence generation method described above The results in Figure S15 show that the generative capacity of MSA transformer does not quite reach the performance of the models trained on a single MSA but is comparable to the performance of state of the art VAEs In general it is encouraging that MSA Transformer can capture covariation at about the level of VAEs and above that of the Indep model However the method scales very poorly and it might take an unreasonable amount of time to generate longer proteins such as kinase In this test the target MSA is a natural sequence protein MSA limited to 10Ksequences as all models were fit to natural protein sequence data As noted\n"
            },
            {
                "rank": 4,
                "index": 5,
                "score": 0.114566505,
                "text": "of 1Msynthetic sequences from MSA Transformer for RRM using the iterative sequence generation method described above The results in Figure S15 show that the generative capacity of MSA transformer does not quite reach the performance of the models trained on a single MSA but is comparable to the performance of state of the art VAEs In general it is encouraging that MSA Transformer can capture covariation at about the level of VAEs and above that of the Indep model However the method scales very poorly and it might take an unreasonable amount of time to generate longer proteins such as kinase In this test the target MSA is a natural sequence protein MSA limited to 10Ksequences as all models were fit to natural protein sequence data As noted\n"
            },
            {
                "rank": 5,
                "index": 2,
                "score": 0.03073393,
                "text": "end to end model replaces the multistage pipeline involving sequence search alignment and model fitting steps standard in bioinformatics Recently promising results have shown that protein language models learn secondary structure long range contacts and function via the unsupervised objective Rives et al 2020 making them an alternative to the classical pipeline While small and recurrent models fall well short of state of the art Rao et al 2019 the internal representations of very large transformer models are competitive with Potts models for unsupervised structure learning Rives et al 2020 Rao et al 2021 Potts models have an important advantage over protein lan CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted\n"
            },
            {
                "rank": 6,
                "index": 1,
                "score": 0.006338922,
                "text": "samples from naturally occurring protein families To solve this problem with a model grounded in sequences it will be necessary to learn sequence patterns that generalize beyond individual protein families Evolutionary scale language models go beyond classic protein family models by training on diverse sequences across evolution which means that they have the potential to learn deep patterns across all proteins including where there is no experimental structure There is evidence for local patterns in sequences that generalize beyond individual protein families in the form of motifs that are local in the sequence 23 as well as motifs that are local in 3d space 24 However the mapping between sequence and structure is not one to one 25 and designing sequences to reach a well folded native state\n"
            },
            {
                "rank": 7,
                "index": 9,
                "score": 0.004886375,
                "text": "to the current practice of fitting a new model for each task 3 2 Inference efficiency Inference with ESM 1v is more efficient than current state of the art methods This is a result of two important differences i the effect of mutations can be inferred directly without training a task specific model ii fitness landscapes can be predicted with a single forward pass Time requirements are summarized in Fig 7 3 3 Scoring with MSA Transformer We score mutations with MSA Transformer using the log odds ratio and additive model in Eq 1 However since MSA Transformer uses a set of sequences for inference we input the sequence to be evaluated as the first sequence and provide additional sequences from the MSA as context Masking and scoring\n"
            },
            {
                "rank": 8,
                "index": 7,
                "score": 0.0035796426,
                "text": "by the MSA Transformer we train a state of the art downstream head based on the Netsurf architecture Klausen et al 2019 The downstream model is trained to predict 8 class secondary structure from the pretrained representations We evaluate models on the CB513 test set Cuff Barton 1999 The models are trained on the Netsurf training dataset Representations from the MSA Transformer 72 9 surpass the performance of HMM profiles 71 2 and ESM 1b embeddings 71 6 Table 4 4 4 Ablation Study We perform an ablation study over seven model hyperparameters using unsupervised contact prediction on the vali6 CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license\n"
            },
            {
                "rank": 9,
                "index": 3,
                "score": 0.0015978455,
                "text": "middle of the sequence before performance starts to significantly degrade This suggests that ESM 1b learns a robust implicit alignment of the protein sequence On the other hand we find that the TAPE Transformer is less robust to insertions On one sequence pdbid 1a27 we find the TAPE Transformer drops in precision by 12 percentage points after adding just 8 alanines to the beginning of the sequence while ESM 1b sees minimal degradation until 256 alanines are inserted We hypothesize that because TAPE was trained on protein domains it did not learn to deal with mis alignments in the input sequence A 13 E VOLUTIONARY FINETUNING DETAILS We finetuned each model using a learning rate of 1e 4 16k warmup updates an inverse square root learning rate schedule\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -0.8521541357040405,
                "text": "generate comprehensive summaries Shaham et al 2022 otherwise models often miss important information Note that typical transformer models apply full attention to capture token dependencies pair wise It leads to a quadratic time and space complexity w r t input length However such a complexity is prohibitive for long sequences In particular it incurs massive memory consumption during the back propagation For example a transformer model with 250M parameters consumes over 80G GPU memory when sequence length is 8k Zuo et al 2022 To address this scalability issue various approaches have been proposed to reduce the complexity One approach is sparse attention which restricts each token to attend a subset of tokens based on predefined sparsity patterns Beltagy et al 2020 Zaheer et al 2020 Ainslie et al\n"
            },
            {
                "rank": 1,
                "score": -0.8591046333312988,
                "text": "Sercu et al 2020 used amortized optimization to simultaneously predict profiles and pairwise couplings However prior work on protein language models has not considered sets of sequences as inputs to the language model 3 Methods Transformers are powerful sequence models capable of passing information from any position to any other position Vaswani et al 2017 However they are not trivially applied to a set of aligned sequences Naively concatenating Msequences of length Lin an MSA would allow attention across all sequences but the ML 2self attention maps would be prohibitively memory intensive The main con tribution of this paper is to extend transformer pre training to operate on an MSA while respecting its structure as an MLcharacter matrix We describe the input MSA as a matrix xRML where\n"
            },
            {
                "rank": 2,
                "score": -1.4644883871078491,
                "text": "end to end model replaces the multistage pipeline involving sequence search alignment and model fitting steps standard in bioinformatics Recently promising results have shown that protein language models learn secondary structure long range contacts and function via the unsupervised objective Rives et al 2020 making them an alternative to the classical pipeline While small and recurrent models fall well short of state of the art Rao et al 2019 the internal representations of very large transformer models are competitive with Potts models for unsupervised structure learning Rives et al 2020 Rao et al 2021 Potts models have an important advantage over protein lan CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted\n"
            },
            {
                "rank": 3,
                "score": -1.740483045578003,
                "text": "proposes the Transformer for protein language modeling Rives et al 2019 Rao et al 2019 Originally developed in the NLP community to represent long range context the main innovation of the Transformer model is its use of self attention Vaswani et al 2017 Self attention has particular relevance for the modeling of protein sequences Unlike convolutional and recurrent LSTM models the Transformer constructs a pairwise interaction map between all positions in the sequence In principle this mechanism has an ideal form to model residue residue contacts In theory end to end learning with a language model has advantages over the bioinformatics pipeline i it replaces the expensive query alignment and training steps with a single forward Work performed during an internship at Facebook 1Weights for all ESM 1\n"
            },
            {
                "rank": 4,
                "score": -2.1784958839416504,
                "text": "of 1Msynthetic sequences from MSA Transformer for RRM using the iterative sequence generation method described above The results in Figure S15 show that the generative capacity of MSA transformer does not quite reach the performance of the models trained on a single MSA but is comparable to the performance of state of the art VAEs In general it is encouraging that MSA Transformer can capture covariation at about the level of VAEs and above that of the Indep model However the method scales very poorly and it might take an unreasonable amount of time to generate longer proteins such as kinase In this test the target MSA is a natural sequence protein MSA limited to 10Ksequences as all models were fit to natural protein sequence data As noted\n"
            },
            {
                "rank": 5,
                "score": -2.1784958839416504,
                "text": "of 1Msynthetic sequences from MSA Transformer for RRM using the iterative sequence generation method described above The results in Figure S15 show that the generative capacity of MSA transformer does not quite reach the performance of the models trained on a single MSA but is comparable to the performance of state of the art VAEs In general it is encouraging that MSA Transformer can capture covariation at about the level of VAEs and above that of the Indep model However the method scales very poorly and it might take an unreasonable amount of time to generate longer proteins such as kinase In this test the target MSA is a natural sequence protein MSA limited to 10Ksequences as all models were fit to natural protein sequence data As noted\n"
            },
            {
                "rank": 6,
                "score": -2.348071575164795,
                "text": "middle of the sequence before performance starts to significantly degrade This suggests that ESM 1b learns a robust implicit alignment of the protein sequence On the other hand we find that the TAPE Transformer is less robust to insertions On one sequence pdbid 1a27 we find the TAPE Transformer drops in precision by 12 percentage points after adding just 8 alanines to the beginning of the sequence while ESM 1b sees minimal degradation until 256 alanines are inserted We hypothesize that because TAPE was trained on protein domains it did not learn to deal with mis alignments in the input sequence A 13 E VOLUTIONARY FINETUNING DETAILS We finetuned each model using a learning rate of 1e 4 16k warmup updates an inverse square root learning rate schedule\n"
            },
            {
                "rank": 7,
                "score": -3.0511326789855957,
                "text": "to the current practice of fitting a new model for each task 3 2 Inference efficiency Inference with ESM 1v is more efficient than current state of the art methods This is a result of two important differences i the effect of mutations can be inferred directly without training a task specific model ii fitness landscapes can be predicted with a single forward pass Time requirements are summarized in Fig 7 3 3 Scoring with MSA Transformer We score mutations with MSA Transformer using the log odds ratio and additive model in Eq 1 However since MSA Transformer uses a set of sequences for inference we input the sequence to be evaluated as the first sequence and provide additional sequences from the MSA as context Masking and scoring\n"
            },
            {
                "rank": 8,
                "score": -3.2616310119628906,
                "text": "by the MSA Transformer we train a state of the art downstream head based on the Netsurf architecture Klausen et al 2019 The downstream model is trained to predict 8 class secondary structure from the pretrained representations We evaluate models on the CB513 test set Cuff Barton 1999 The models are trained on the Netsurf training dataset Representations from the MSA Transformer 72 9 surpass the performance of HMM profiles 71 2 and ESM 1b embeddings 71 6 Table 4 4 4 Ablation Study We perform an ablation study over seven model hyperparameters using unsupervised contact prediction on the vali6 CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license\n"
            },
            {
                "rank": 9,
                "score": -4.903116226196289,
                "text": "samples from naturally occurring protein families To solve this problem with a model grounded in sequences it will be necessary to learn sequence patterns that generalize beyond individual protein families Evolutionary scale language models go beyond classic protein family models by training on diverse sequences across evolution which means that they have the potential to learn deep patterns across all proteins including where there is no experimental structure There is evidence for local patterns in sequences that generalize beyond individual protein families in the form of motifs that are local in the sequence 23 as well as motifs that are local in 3d space 24 However the mapping between sequence and structure is not one to one 25 and designing sequences to reach a well folded native state\n"
            }
        ]
    },
    {
        "query": "How do the transformer architecture and the attention mechanism differ from traditional recurrent neural networks?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 1,
                "score": 0.85046864,
                "text": "dependencies To address this shortcoming Vaswani et al 2017 introduced the transformer architecture a seminal work that transformed RE and NLP in general The transformer is similar in structure to other sequence transduction models in that it consists of two modules an encoder module and a decoder module The encoder takes an input sequence x and produces a dense representation zthat is fed to the decoder The decoder then uses zto produce an output sequence y However the transformer was unique in its use of stacked multi head attention functions in the encoder and decoder modules In the original transformer architecture the encoder and decoder modules consisted of six identical and serially connected layers Each layer contained a multi head attention function as well as a feedforward network\n"
            },
            {
                "rank": 1,
                "index": 0,
                "score": 0.5846269,
                "text": "a deep learning architectures this can be thought of in terms of a s oftmax synaptic gating as is done in transformer and other NLP architectures Thus someh ow this fast weight attention mechanism must operate upon and be faster than the fast wei ght synaptic mechanism used to store information about the first sentence Attention mechanisms allow the attending network to modula te the function computed by the attended network thereby expanding the scope of usef ul functions that can be efficiently implemented and trained in deep learning Because the SM already has universal approximation properties its extensions should not be eva luated in terms of which functions can be approximated but rather in terms of other efficiencies While attention blocks act as new primitives in standard\n"
            },
            {
                "rank": 2,
                "index": 7,
                "score": 0.45256618,
                "text": "The initial encoder layer is fed an input sequence x and using multi head attention it attends to all positions in that sequencethat is it learns which parts of the input sequence to focus on for a given task Next the result from the multi head attention function is sent through a feed forward network and finally to the next layer in the module Subsequent layers repeat this process using the previous layers output as their input The transformer is arguably the most impactful neural based architecture for RE and NLP as it is the primary architecture used in large pre trained language models 2 4 Large language model based RE 2019 The transformer architecture inspired the development of BERT Devlin et al 2019 a wildly effective pre\n"
            },
            {
                "rank": 3,
                "index": 2,
                "score": 0.40069073,
                "text": "seem complex and sometimes obscure the underlying neural architecture 16 12 21 4 13 24 it can be checked that in all cases these are built out of the output and synaptic gating operations described in the previous section For conciseness here we demonstrate this in detail only for THE QUARKS OF ATTENTION 11 the transformer architectures 28 27 see also for an MLP alternative to transformers These architectures consist of stacks of similar encoder an d decoder modules with attention mechanisms in each module The details of an encoder module a re shown in Figure 5 As the Figure shows a shared and typically linear network is first a pplied to each of ninput vectors At the bottom of the architecture these input vectors could represent for\n"
            },
            {
                "rank": 4,
                "index": 4,
                "score": 0.17300858,
                "text": "is based on AFT attention free Transformer S Zhai et al 2021 another variant of linear attention Its main WKV mechanism involves LTI recurrences and can be seen as the ratio of two SSMs We also highlight the gated attention unit GAU from Hua et al 2022 which was motivated by combining the Transformers MHA and MLP blocks together and was an inspiration for our architecture Section 3 4 combining the H3 and MLP blocks B 3 Relationship to RNNs RNNs and SSMs are broadly related as they both involve the concepts of recurrence on a latent state Several older RNNs such as the strongly typed RNN Balduzzi and Ghifary 2016 quasi RNN QRNN Bradbury et al 2016 and simple recurrent unit SRU Lei 2021 Lei et al\n"
            },
            {
                "rank": 5,
                "index": 5,
                "score": 0.03308598,
                "text": "attention mechanism with other modules to scale to long sequences MLP Mixer and others Tolstikhin et al 2021 Liu et al 2021 proposed the replacement of attention by MultiLayer Perceptrons MLPs in computer vision tasks The Attention Free Transformer AFT Zhai et al 2021 replaces dot product self attention with a computationally efficient alternative which can be seen as a multi head attention where each feature dimension corresponds to a head Inspired by AFT RWKV takes a similar approach but modifies the interaction weights for simplicity such that it can be transformed into an RNN In parallel RNNstyle Hochreiter and Schmidhuber 1997 Chung et al 2014 recursive components have also been modified to increase context length such as the Recurrent Memory Transformer Bulatov et al 2022 2023 and\n"
            },
            {
                "rank": 6,
                "index": 9,
                "score": 0.025035424,
                "text": "of queries One example is where an attention based capsule network is proposed that also includes a multi hop attention mechanism for the purpose of visual question answering Another example is where capsule based attention is used for aspect level sentiment analysis of restaurant reviews The multiplicity of queries is a particularly interesting category due to the Transformer model which combines a form of multi hop and multi head attention Due to the initial success of the Transformer model many improvements and iterations of the model have been produced that typically aim to improve the predictive performance the computational efficiency or both For example the Transformer XL is an extension of the original Transformer that uses a recurrence mechanism to not be limited by a context window when\n"
            },
            {
                "rank": 7,
                "index": 6,
                "score": 0.019124037,
                "text": "2021 RetNet Y Sun et al 2023 adds an additional gate to the architecture and uses a simpler SSM allowing an alternative parallelizable computation path using a variant of multi head attention MHA instead of convolutions 4 RWKV B Peng et al 2023 is a recent RNN designed for language modeling based on another linear attention approximation attention free Transformer S Zhai et al 2021 Its main WKV mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs Other closely related SSMs and architectures are discussed further in an extended related work Appendix B We highlight in particular S5 Smith Warrington and Linderman 2023 QRNN Bradbury et al 2016 and SRU Lei et al 2017 which we view as the most closely related methods\n"
            },
            {
                "rank": 8,
                "index": 8,
                "score": 0.011419615,
                "text": "attention with a kNN lookup to increase speed and reduce memory usage 3 M ETHOD The architecture of our kNN augmented transformer is shown in Figure 2 The bulk of the model is a vanilla decoder only transformer Vaswani et al 2017 The input text is tokenized and the tokens are embedded into vector space The embedding vectors are passed through a series of transformer layers each of which does dense self attention followed by a feed forward network FFN Since this is a decoder only language model we use a causal attention mask and the token embeddings of the last layer are used to predict the next token Long documents are split into subsequences of 512 tokens and each subsequence is used as the input for one\n"
            },
            {
                "rank": 9,
                "index": 3,
                "score": 0.0045032725,
                "text": "is little theory to help us better understand the nature and computational capabilities of attention To add ress this gap in Section 2 we first seek to identify and classify the most fundamental buil ding block of all attention mechanisms within the deep learning framework In particular w e identify three key attentional mechanisms we call activation attention output gating an d synaptic gating In Section 3 we show how output gating and synaptic gating are used in all t he current attention based architectures including transformers In Section 4 we ex plore the functional capacity of output gating and synaptic gating In Section 5 we provide a brief overview of the notion of capacity and the technique of multiplexing which is a for m of activation attention\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -0.1826133131980896,
                "text": "dependencies To address this shortcoming Vaswani et al 2017 introduced the transformer architecture a seminal work that transformed RE and NLP in general The transformer is similar in structure to other sequence transduction models in that it consists of two modules an encoder module and a decoder module The encoder takes an input sequence x and produces a dense representation zthat is fed to the decoder The decoder then uses zto produce an output sequence y However the transformer was unique in its use of stacked multi head attention functions in the encoder and decoder modules In the original transformer architecture the encoder and decoder modules consisted of six identical and serially connected layers Each layer contained a multi head attention function as well as a feedforward network\n"
            },
            {
                "rank": 1,
                "score": -0.5942205190658569,
                "text": "is based on AFT attention free Transformer S Zhai et al 2021 another variant of linear attention Its main WKV mechanism involves LTI recurrences and can be seen as the ratio of two SSMs We also highlight the gated attention unit GAU from Hua et al 2022 which was motivated by combining the Transformers MHA and MLP blocks together and was an inspiration for our architecture Section 3 4 combining the H3 and MLP blocks B 3 Relationship to RNNs RNNs and SSMs are broadly related as they both involve the concepts of recurrence on a latent state Several older RNNs such as the strongly typed RNN Balduzzi and Ghifary 2016 quasi RNN QRNN Bradbury et al 2016 and simple recurrent unit SRU Lei 2021 Lei et al\n"
            },
            {
                "rank": 2,
                "score": -0.6168422698974609,
                "text": "a deep learning architectures this can be thought of in terms of a s oftmax synaptic gating as is done in transformer and other NLP architectures Thus someh ow this fast weight attention mechanism must operate upon and be faster than the fast wei ght synaptic mechanism used to store information about the first sentence Attention mechanisms allow the attending network to modula te the function computed by the attended network thereby expanding the scope of usef ul functions that can be efficiently implemented and trained in deep learning Because the SM already has universal approximation properties its extensions should not be eva luated in terms of which functions can be approximated but rather in terms of other efficiencies While attention blocks act as new primitives in standard\n"
            },
            {
                "rank": 3,
                "score": -0.9006521701812744,
                "text": "2021 RetNet Y Sun et al 2023 adds an additional gate to the architecture and uses a simpler SSM allowing an alternative parallelizable computation path using a variant of multi head attention MHA instead of convolutions 4 RWKV B Peng et al 2023 is a recent RNN designed for language modeling based on another linear attention approximation attention free Transformer S Zhai et al 2021 Its main WKV mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs Other closely related SSMs and architectures are discussed further in an extended related work Appendix B We highlight in particular S5 Smith Warrington and Linderman 2023 QRNN Bradbury et al 2016 and SRU Lei et al 2017 which we view as the most closely related methods\n"
            },
            {
                "rank": 4,
                "score": -0.9526832103729248,
                "text": "of queries One example is where an attention based capsule network is proposed that also includes a multi hop attention mechanism for the purpose of visual question answering Another example is where capsule based attention is used for aspect level sentiment analysis of restaurant reviews The multiplicity of queries is a particularly interesting category due to the Transformer model which combines a form of multi hop and multi head attention Due to the initial success of the Transformer model many improvements and iterations of the model have been produced that typically aim to improve the predictive performance the computational efficiency or both For example the Transformer XL is an extension of the original Transformer that uses a recurrence mechanism to not be limited by a context window when\n"
            },
            {
                "rank": 5,
                "score": -1.1239922046661377,
                "text": "seem complex and sometimes obscure the underlying neural architecture 16 12 21 4 13 24 it can be checked that in all cases these are built out of the output and synaptic gating operations described in the previous section For conciseness here we demonstrate this in detail only for THE QUARKS OF ATTENTION 11 the transformer architectures 28 27 see also for an MLP alternative to transformers These architectures consist of stacks of similar encoder an d decoder modules with attention mechanisms in each module The details of an encoder module a re shown in Figure 5 As the Figure shows a shared and typically linear network is first a pplied to each of ninput vectors At the bottom of the architecture these input vectors could represent for\n"
            },
            {
                "rank": 6,
                "score": -1.2335734367370605,
                "text": "The initial encoder layer is fed an input sequence x and using multi head attention it attends to all positions in that sequencethat is it learns which parts of the input sequence to focus on for a given task Next the result from the multi head attention function is sent through a feed forward network and finally to the next layer in the module Subsequent layers repeat this process using the previous layers output as their input The transformer is arguably the most impactful neural based architecture for RE and NLP as it is the primary architecture used in large pre trained language models 2 4 Large language model based RE 2019 The transformer architecture inspired the development of BERT Devlin et al 2019 a wildly effective pre\n"
            },
            {
                "rank": 7,
                "score": -1.607107162475586,
                "text": "attention mechanism with other modules to scale to long sequences MLP Mixer and others Tolstikhin et al 2021 Liu et al 2021 proposed the replacement of attention by MultiLayer Perceptrons MLPs in computer vision tasks The Attention Free Transformer AFT Zhai et al 2021 replaces dot product self attention with a computationally efficient alternative which can be seen as a multi head attention where each feature dimension corresponds to a head Inspired by AFT RWKV takes a similar approach but modifies the interaction weights for simplicity such that it can be transformed into an RNN In parallel RNNstyle Hochreiter and Schmidhuber 1997 Chung et al 2014 recursive components have also been modified to increase context length such as the Recurrent Memory Transformer Bulatov et al 2022 2023 and\n"
            },
            {
                "rank": 8,
                "score": -2.545654773712158,
                "text": "attention with a kNN lookup to increase speed and reduce memory usage 3 M ETHOD The architecture of our kNN augmented transformer is shown in Figure 2 The bulk of the model is a vanilla decoder only transformer Vaswani et al 2017 The input text is tokenized and the tokens are embedded into vector space The embedding vectors are passed through a series of transformer layers each of which does dense self attention followed by a feed forward network FFN Since this is a decoder only language model we use a causal attention mask and the token embeddings of the last layer are used to predict the next token Long documents are split into subsequences of 512 tokens and each subsequence is used as the input for one\n"
            },
            {
                "rank": 9,
                "score": -2.7472586631774902,
                "text": "is little theory to help us better understand the nature and computational capabilities of attention To add ress this gap in Section 2 we first seek to identify and classify the most fundamental buil ding block of all attention mechanisms within the deep learning framework In particular w e identify three key attentional mechanisms we call activation attention output gating an d synaptic gating In Section 3 we show how output gating and synaptic gating are used in all t he current attention based architectures including transformers In Section 4 we ex plore the functional capacity of output gating and synaptic gating In Section 5 we provide a brief overview of the notion of capacity and the technique of multiplexing which is a for m of activation attention\n"
            }
        ]
    },
    {
        "query": "Compare the performance of different sequence alignment algorithms commonly used in genomics.",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 4,
                "score": 0.2562732,
                "text": "Altschul S F etal Gapped BLAST and PSI BLAST a new generation of protein database search programs Nucleic Acids Res 25 33893402 1997 63 Katoh K Standley D M MAFFT multiple sequence alignment software version 7 improvements in performance and usability Mol Biol Evol 30 772780 2013 64 Stamatakis A RAxML version 8 a tool for phylogenetic analysis and post analysis of large phylogenies Bioinformatics 30 13121313 2014 65 Fu L Niu B Zhu Z Wu S Li W CD HIT accelerated for clustering the next generation sequencing data Bioinformatics 28 31503152 2012 66 Guindon S etal New algorithms and methods to estimate maximum likelihood phylogenies assessing the performance of PhyML 3 0 Syst Biol 59 307321 2010 67 Soubrier J etal The influence of rate heterogeneity among\n"
            },
            {
                "rank": 1,
                "index": 5,
                "score": 0.01778039,
                "text": "rely critically on the multiple sequences alignments used for training data 36 72 81 83 At present the criteria for the numbers of non redundant sequences and the level of diversity in multiple sequence alignments is ad hoc and this should be 11 improved to give better uncertainty criteria and accuracy expectation in predicting the effects of mutations Secondly the premise that evolutionary data can be applied to predict outcomes of an experiment is highly contingent on the relevance of the experimental assay to long term selective forces in the family A mutation may be damaging with regard to some measurable protein feature e g enzyme efficiency but harmless for stability or even organism fitness as we and others have previously discussed 36 38 63 We therefore suggest\n"
            },
            {
                "rank": 2,
                "index": 2,
                "score": 0.012821214,
                "text": "replication fork rates Mol Cell 65 117130 2017 65 Servant N et al HiC Pro an optimized and flexible pipeline for Hi C data processing Genome Biol 16 259 2015 66 Langmead B Salzberg S L Fast gapped read alignment with Bowtie 2 Nat Methods 9 357359 2012 67 Langmead B Trapnell C Pop M Salzberg S L Ultrafast and memory efficient alignment of short DNA sequences to the human genome Genome Biol 10 R25 2009 68 Lawrence M et al Software for computing and annotating genomic ranges PLoS Comput Biol 9 e1003118 2013 69 Thorvaldsdttir H Robinson J T Mesirov J P Integrative genomics viewer IGV high performance genomics data visualization and exploration Brief Bioinform 14 178192 2013 Nature Genetics Article https doi org 10 1038 s41588\n"
            },
            {
                "rank": 3,
                "index": 0,
                "score": 0.0040072887,
                "text": "B Trimmomatic a flexible trimmer for Illumina sequence data Bioinformatics 30 21142120 2014 74 Li H Durbin R Fast and accurate short read alignment with BurrowsWheeler transform Bioinformatics 25 17541760 2009 75 Faust G G Hall I M SAMBLASTER fast duplicate marking and structural variant read extraction Bioinformatics 30 25032505 2014 76 Neph S etal BEDOPS high performance genomic feature operations Bioinformatics 28 19191920 2012 77 Rausch T etal DELLY structural variant discovery by integrated paired end and split read analysis Bioinformatics 28 i333i339 2012 78 Buenrostro J D Wu B Chang H Y Greenleaf W J ATAC seq a method for assaying chromatin accessibility genome wide Curr Protoc Mol Biol 109 2129 2015 79 Langmead B Salzberg S L Fast gapped read alignment with Bowtie 2 Nat\n"
            },
            {
                "rank": 4,
                "index": 8,
                "score": 0.003235056,
                "text": "Methods 9 357359 2012 80 Ramirez F etal deepTools2 a next generation web server for deep sequencing data analysis Nucleic Acids Res 44 W160165 2016 81 Robinson J T etal Integrative Genomics Viewer Nat Biotechnol 29 2426 2011 82 Zhang Y etal Model based analysis of ChIP Seq MACS Genome Biol 9 R137 2008 83 Dobin A etal STAR ultrafast universal RNA seq aligner Bioinformatics 29 1521 2013 84 Robinson M D Oshlack A A scaling normalization method for differential expression analysis of RNA seq data Genome Biol 11 R25 2010 85 Orsi G A Kasinathan S Zentner G E Henikoff S Ahmad K Mapping regulatory factors by immunoprecipitation from native chromatin Curr Protoc Mol Biol 110 2125 2015 86 Li H etal The Sequence Alignment Map format\n"
            },
            {
                "rank": 5,
                "index": 6,
                "score": 0.001032231,
                "text": "of matches filtered for sequence length less than 500 and aligned using the MAFFT85 webserver with the default parameters We calculated a phylogenetic tree using IQ TREE86 with the LG I G substitution model The resulting tree is shown in Extended Data Fig 9 and dem onstrates that while the closest structure homologues to the known state are clustered the closest homologues to the alternative state are dispersed across the tree Testing the sensitivity of AF2 and AF Cluster to point mutations in the GA GB system To test the sensitivity of AF2 and AF Cluster to point mutations in the GA GB87 system MSAs were generated using the default MSA generation routine from ColabFold using MMseqs2 For AF Cluster MSAs were then clustered using the DBSCAN procedure\n"
            },
            {
                "rank": 6,
                "index": 9,
                "score": 0.00037263767,
                "text": "protein sequences In this sense DeepSeqProt can be thought of as a clustering tool albeit with added information from the structure of latent space We benchmarked DeepSeqProt alongside several clustering algorithms designed for unaligned protein sequences CD HIT Li and Godzik 2006 was one of the first Li et al 2001 clustering algorithms designed for large databases and uses short word filtering followed by greedy incremental clustering by sequence similarity Li and Godzik 2006 Fu et al 2012 CD HIT uses a clustering threshold parameter which controls the short word filtering step for which we tested two values 50 and 75 MMseqs2 Steinegger and Sding 2017 is a similar software suite designed to quickly and sensitively search and cluster large sequence data sets Steinegger and Sding 2017 MMseqs2\n"
            },
            {
                "rank": 7,
                "index": 1,
                "score": 0.00027372167,
                "text": "are studying We aim to reduce these biases in a mechanistically agnostic way by reweighting the empirical data distribution to make it smoother We use the previously established procedure of computing each sequence weight as the reciprocal of the number of sequences within a given Hamming distance cutoff If A C is the normalized hamming distance between the query sequence and another sequence in the alignment D and is a pre defined neighborhood size the sequence weight is A C DCHI The effective sample size of a multiple sequence alignment can then be computed as the sum of these weights as KLL CDC To fit a model to reweighted data there are two common approaches First as was done previouslyone can reweight every log likelihood in the objective\n"
            },
            {
                "rank": 8,
                "index": 7,
                "score": 8.68221e-05,
                "text": "fit in GPU memory Then we sample that number of sequences uniformly from the MSA ensuring that the reference sequence is always chosen In the second strategy we always sample the fullN L sequences from the MSA In our hyperparameter search most models use the first strategy while our final model uses the second We find no statistically significant difference in performance between the two strategies However it is possible that the log uniform strategy would help prevent overfitting and ultimately perform better after more training The CCMpred implementation of Potts Balakrishnan et al 2011 Ekeberg et al 2013 UniRep Alley et al 2019 SeqVec Heinzinger et al 2019 TAPE transformer Rao et al 2019 ESM 1b Rives et al 2020 ProtBERT BFD and ProTrans T5 Elnaggar et\n"
            },
            {
                "rank": 9,
                "index": 3,
                "score": 3.2192336e-05,
                "text": "www nature com naturecommunications 9 inverse of the number of sequences having least 80 sequence identity with sequence m and Meff mwmdenotes the effective number of independent sequences The goal is to remove the in fluence of very closely related sequences Note however that such reweighting cannot fully capture the hierarchical structure of phylogenetic relations between proteins Sampling from the model Once the model parameters are inferred a sequence can be iteratively generated by the following procedure Sample the first residue from P a1 Sample the second residue from P a2a1 where a1is sampled in the previous step L Sample the last residue from P aLaL1 aL2 a2 a1 Each step is very fast because there are only 21 possible values for each probability Both training andsampling are\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -0.2698231041431427,
                "text": "replication fork rates Mol Cell 65 117130 2017 65 Servant N et al HiC Pro an optimized and flexible pipeline for Hi C data processing Genome Biol 16 259 2015 66 Langmead B Salzberg S L Fast gapped read alignment with Bowtie 2 Nat Methods 9 357359 2012 67 Langmead B Trapnell C Pop M Salzberg S L Ultrafast and memory efficient alignment of short DNA sequences to the human genome Genome Biol 10 R25 2009 68 Lawrence M et al Software for computing and annotating genomic ranges PLoS Comput Biol 9 e1003118 2013 69 Thorvaldsdttir H Robinson J T Mesirov J P Integrative genomics viewer IGV high performance genomics data visualization and exploration Brief Bioinform 14 178192 2013 Nature Genetics Article https doi org 10 1038 s41588\n"
            },
            {
                "rank": 1,
                "score": -0.2989323139190674,
                "text": "Altschul S F etal Gapped BLAST and PSI BLAST a new generation of protein database search programs Nucleic Acids Res 25 33893402 1997 63 Katoh K Standley D M MAFFT multiple sequence alignment software version 7 improvements in performance and usability Mol Biol Evol 30 772780 2013 64 Stamatakis A RAxML version 8 a tool for phylogenetic analysis and post analysis of large phylogenies Bioinformatics 30 13121313 2014 65 Fu L Niu B Zhu Z Wu S Li W CD HIT accelerated for clustering the next generation sequencing data Bioinformatics 28 31503152 2012 66 Guindon S etal New algorithms and methods to estimate maximum likelihood phylogenies assessing the performance of PhyML 3 0 Syst Biol 59 307321 2010 67 Soubrier J etal The influence of rate heterogeneity among\n"
            },
            {
                "rank": 2,
                "score": -0.4517586827278137,
                "text": "B Trimmomatic a flexible trimmer for Illumina sequence data Bioinformatics 30 21142120 2014 74 Li H Durbin R Fast and accurate short read alignment with BurrowsWheeler transform Bioinformatics 25 17541760 2009 75 Faust G G Hall I M SAMBLASTER fast duplicate marking and structural variant read extraction Bioinformatics 30 25032505 2014 76 Neph S etal BEDOPS high performance genomic feature operations Bioinformatics 28 19191920 2012 77 Rausch T etal DELLY structural variant discovery by integrated paired end and split read analysis Bioinformatics 28 i333i339 2012 78 Buenrostro J D Wu B Chang H Y Greenleaf W J ATAC seq a method for assaying chromatin accessibility genome wide Curr Protoc Mol Biol 109 2129 2015 79 Langmead B Salzberg S L Fast gapped read alignment with Bowtie 2 Nat\n"
            },
            {
                "rank": 3,
                "score": -0.9659366011619568,
                "text": "Methods 9 357359 2012 80 Ramirez F etal deepTools2 a next generation web server for deep sequencing data analysis Nucleic Acids Res 44 W160165 2016 81 Robinson J T etal Integrative Genomics Viewer Nat Biotechnol 29 2426 2011 82 Zhang Y etal Model based analysis of ChIP Seq MACS Genome Biol 9 R137 2008 83 Dobin A etal STAR ultrafast universal RNA seq aligner Bioinformatics 29 1521 2013 84 Robinson M D Oshlack A A scaling normalization method for differential expression analysis of RNA seq data Genome Biol 11 R25 2010 85 Orsi G A Kasinathan S Zentner G E Henikoff S Ahmad K Mapping regulatory factors by immunoprecipitation from native chromatin Curr Protoc Mol Biol 110 2125 2015 86 Li H etal The Sequence Alignment Map format\n"
            },
            {
                "rank": 4,
                "score": -2.069308280944824,
                "text": "are studying We aim to reduce these biases in a mechanistically agnostic way by reweighting the empirical data distribution to make it smoother We use the previously established procedure of computing each sequence weight as the reciprocal of the number of sequences within a given Hamming distance cutoff If A C is the normalized hamming distance between the query sequence and another sequence in the alignment D and is a pre defined neighborhood size the sequence weight is A C DCHI The effective sample size of a multiple sequence alignment can then be computed as the sum of these weights as KLL CDC To fit a model to reweighted data there are two common approaches First as was done previouslyone can reweight every log likelihood in the objective\n"
            },
            {
                "rank": 5,
                "score": -2.5117340087890625,
                "text": "rely critically on the multiple sequences alignments used for training data 36 72 81 83 At present the criteria for the numbers of non redundant sequences and the level of diversity in multiple sequence alignments is ad hoc and this should be 11 improved to give better uncertainty criteria and accuracy expectation in predicting the effects of mutations Secondly the premise that evolutionary data can be applied to predict outcomes of an experiment is highly contingent on the relevance of the experimental assay to long term selective forces in the family A mutation may be damaging with regard to some measurable protein feature e g enzyme efficiency but harmless for stability or even organism fitness as we and others have previously discussed 36 38 63 We therefore suggest\n"
            },
            {
                "rank": 6,
                "score": -2.954838752746582,
                "text": "protein sequences In this sense DeepSeqProt can be thought of as a clustering tool albeit with added information from the structure of latent space We benchmarked DeepSeqProt alongside several clustering algorithms designed for unaligned protein sequences CD HIT Li and Godzik 2006 was one of the first Li et al 2001 clustering algorithms designed for large databases and uses short word filtering followed by greedy incremental clustering by sequence similarity Li and Godzik 2006 Fu et al 2012 CD HIT uses a clustering threshold parameter which controls the short word filtering step for which we tested two values 50 and 75 MMseqs2 Steinegger and Sding 2017 is a similar software suite designed to quickly and sensitively search and cluster large sequence data sets Steinegger and Sding 2017 MMseqs2\n"
            },
            {
                "rank": 7,
                "score": -2.96146559715271,
                "text": "fit in GPU memory Then we sample that number of sequences uniformly from the MSA ensuring that the reference sequence is always chosen In the second strategy we always sample the fullN L sequences from the MSA In our hyperparameter search most models use the first strategy while our final model uses the second We find no statistically significant difference in performance between the two strategies However it is possible that the log uniform strategy would help prevent overfitting and ultimately perform better after more training The CCMpred implementation of Potts Balakrishnan et al 2011 Ekeberg et al 2013 UniRep Alley et al 2019 SeqVec Heinzinger et al 2019 TAPE transformer Rao et al 2019 ESM 1b Rives et al 2020 ProtBERT BFD and ProTrans T5 Elnaggar et\n"
            },
            {
                "rank": 8,
                "score": -5.201869964599609,
                "text": "of matches filtered for sequence length less than 500 and aligned using the MAFFT85 webserver with the default parameters We calculated a phylogenetic tree using IQ TREE86 with the LG I G substitution model The resulting tree is shown in Extended Data Fig 9 and dem onstrates that while the closest structure homologues to the known state are clustered the closest homologues to the alternative state are dispersed across the tree Testing the sensitivity of AF2 and AF Cluster to point mutations in the GA GB system To test the sensitivity of AF2 and AF Cluster to point mutations in the GA GB87 system MSAs were generated using the default MSA generation routine from ColabFold using MMseqs2 For AF Cluster MSAs were then clustered using the DBSCAN procedure\n"
            },
            {
                "rank": 9,
                "score": -6.332586288452148,
                "text": "www nature com naturecommunications 9 inverse of the number of sequences having least 80 sequence identity with sequence m and Meff mwmdenotes the effective number of independent sequences The goal is to remove the in fluence of very closely related sequences Note however that such reweighting cannot fully capture the hierarchical structure of phylogenetic relations between proteins Sampling from the model Once the model parameters are inferred a sequence can be iteratively generated by the following procedure Sample the first residue from P a1 Sample the second residue from P a2a1 where a1is sampled in the previous step L Sample the last residue from P aLaL1 aL2 a2 a1 Each step is very fast because there are only 21 possible values for each probability Both training andsampling are\n"
            }
        ]
    },
    {
        "query": "What are the main differences between language models trained on general text corpora and those trained on biological sequences?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 2,
                "score": 0.8729984,
                "text": "on protein sequences 7 Information about the folded three dimensional structure of proteins develops within the models extending to atomic resolution structure 8 This information emerges through training on sequences alone At the same time the structural information that emerges as a result of training on sequences has been shown to depend on the available evolutionary information varying as a function of the number of related proteins in the training data 8 9 It is an open question across domains to what extent language models are capable of generalizing outside their training data In biology it is unknown whether language models can be used to explore a design space beyond that of natural proteins Here we demonstrate that language models generalize beyond natural proteins to generate de novo\n"
            },
            {
                "rank": 1,
                "index": 9,
                "score": 0.2894721,
                "text": "in the last row of Table 7 the one with best generalization accuracy since it was trained on all the corpora We found that the baseline language model P Base LM has a false positive rate of 17 8 while the joint language model P Joint BiT med has a much higher false positive rate of 31 8 The corresponding accuracy values are 89 9 and 82 9 Note that as we used only prefix of length 120 here the numbers are not directly comparable with Table 7 In conclusion samples from the joint language model are indeed harder to discriminate This is expected since the residual EBM was precisely trained to detect machine generated text and the resampling procedure used at generation time down weights examples exhibiting\n"
            },
            {
                "rank": 2,
                "index": 1,
                "score": 0.22185604,
                "text": "dataset excludes artificial sequences as well as any sequences having similarity to the test set of de novo proteins used in the evaluations Appendix A 1 ESM2 is trained with the masked language modeling objective 29 to recover the identity of amino acids from their context in the rest of the sequence Fig 1B This training objective has been shown to materialize information about the folded structure of proteins in the internal representations of the model 79 30 Since the training of the language model is only on sequences information about structure that emerges must be the result of the unsupervised learning of patterns in sequences A linear projection from the attention maps of the language model identifies internal states that reflect protein structure Previous work has shown\n"
            },
            {
                "rank": 3,
                "index": 3,
                "score": 0.20673385,
                "text": "7 Discussion Advances in language modeling at scale are bringing the goal of a general purpose model for proteins closer to realization This line of work aspires to a model that learns to read and write biology in its native language that can be directly applied across a range of protein understanding and design tasks For scalability learning from sequences is important while there are no central databases of high throughput functional measurements and few compilations exist billions of sequences are available to learn from in sequence databases 49 50 Sequences give an unparalleled view into 9 CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the\n"
            },
            {
                "rank": 4,
                "index": 4,
                "score": 0.16451646,
                "text": "sequence as lower fitness if it has more mutations from a naturally occurring sequence referred to as sequence similarity bias Shaw et al 2023 Our work focuses on a different bias arising from species identity which we identify specifically in pLMs and which can compound the effect of sequence similarity bias 3 Stylized model for pLM bias Before presenting empirical results we provide a brief conceptual example to illustrate why pLMs learn a species bias pLMs are sequence models with architectures and training objectives inspired by language models trained on text data Figure 2 Most if not all protein language models are trained on samples from the UniProt protein sequence database with either an autoregressive AR next token prediction task or a masked language modeling task For the\n"
            },
            {
                "rank": 5,
                "index": 0,
                "score": 0.1573957,
                "text": "samples from naturally occurring protein families To solve this problem with a model grounded in sequences it will be necessary to learn sequence patterns that generalize beyond individual protein families Evolutionary scale language models go beyond classic protein family models by training on diverse sequences across evolution which means that they have the potential to learn deep patterns across all proteins including where there is no experimental structure There is evidence for local patterns in sequences that generalize beyond individual protein families in the form of motifs that are local in the sequence 23 as well as motifs that are local in 3d space 24 However the mapping between sequence and structure is not one to one 25 and designing sequences to reach a well folded native state\n"
            },
            {
                "rank": 6,
                "index": 6,
                "score": 0.05399884,
                "text": "pipeline with a single forward pass of a pre trained end to end protein language model In the last year protein language modeling with an unsupervised training objective has been investigated by multiple groups Rives et al 2019 Alley et al 2019 Heinzinger et al 2019 Rao et al 2019 Madani et al 2020 The longstanding practice in bioinformatics has been to fit linear models on focused sets of evolutionarily related and aligned sequences by contrast protein language modeling trains nonlinear deep neural networks on large databases of evolutionarily diverse and unaligned sequences High capacity protein language models have been shown to learn underlying intrinsic properties of proteins such as structure and function from sequence data Rives et al 2019 A line of work in this emerging field\n"
            },
            {
                "rank": 7,
                "index": 8,
                "score": 0.031858854,
                "text": "sequence modeling arXiv preprint arXiv 1904 01038 N Ousidhoum X Zhao T Fang Y Song and D Y Yeung 2021 Probing toxic content in large pre trained language models In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing Volume 1 Long Papers pages 42624274 C Outeiral and C Deane 2022 Codon language embeddings provide strong signals for protein engineering bioRxiv pages 202212 L Ouyang J Wu X Jiang D Almeida C Wainwright P Mishkin C Zhang S Agarwal et al 2022 Training language models to follow instructions with human feedback InAdvances in Neural Information Processing Systems M Pagliardini D Paliotta M Jaggi and F Fleuret 2023 Faster causal attention over large sequences through sparse\n"
            },
            {
                "rank": 8,
                "index": 7,
                "score": 0.023599295,
                "text": "bioRxiv preprint Language models generalize beyond natural proteins this section except in the case of motifs are made for hits returned by sequence search comparison of predicted structure for designs to known structure databases always returns hits likely to possess a similar fold Fig S10 For sequence search we use jackhmmer 3 3 2 a sequence search tool from the HMMER suite 58 Two jackhmmer settings were modified from their defaults based on failure modes observed during during our analysis which queries distant de novo sequences against large scale 100M sequences search databases 1 One Iteration Jackhmmer was run with only 1 iteration instead of multiple the default This change was made because it was observed that additional iterations resulted in a growing amount of returned spurious hits\n"
            },
            {
                "rank": 9,
                "index": 5,
                "score": 0.0059571536,
                "text": "for non coding genomic sequences In previous studies pre trained gLMs have found some success by focusing on specific regions of the genome during pre training or working with simpler organisms with compact genomes28 34 67 For instance a BERT based LLM trained strictly in the coding genome can provide more context than only considering amino acids with protein language modeling e g codon usage 3537 However our evaluation shows that extending the pre training task across the whole genome struggles to capture meaningful representations in the non coding genome The performance gap may be due to differences in the structure of the coding regions versus the non coding regions To elaborate protein sequences have a clear start and end with low level grammars i e secondary structures\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -0.792116641998291,
                "text": "in the last row of Table 7 the one with best generalization accuracy since it was trained on all the corpora We found that the baseline language model P Base LM has a false positive rate of 17 8 while the joint language model P Joint BiT med has a much higher false positive rate of 31 8 The corresponding accuracy values are 89 9 and 82 9 Note that as we used only prefix of length 120 here the numbers are not directly comparable with Table 7 In conclusion samples from the joint language model are indeed harder to discriminate This is expected since the residual EBM was precisely trained to detect machine generated text and the resampling procedure used at generation time down weights examples exhibiting\n"
            },
            {
                "rank": 1,
                "score": -1.623170256614685,
                "text": "on protein sequences 7 Information about the folded three dimensional structure of proteins develops within the models extending to atomic resolution structure 8 This information emerges through training on sequences alone At the same time the structural information that emerges as a result of training on sequences has been shown to depend on the available evolutionary information varying as a function of the number of related proteins in the training data 8 9 It is an open question across domains to what extent language models are capable of generalizing outside their training data In biology it is unknown whether language models can be used to explore a design space beyond that of natural proteins Here we demonstrate that language models generalize beyond natural proteins to generate de novo\n"
            },
            {
                "rank": 2,
                "score": -1.6924729347229004,
                "text": "sequence as lower fitness if it has more mutations from a naturally occurring sequence referred to as sequence similarity bias Shaw et al 2023 Our work focuses on a different bias arising from species identity which we identify specifically in pLMs and which can compound the effect of sequence similarity bias 3 Stylized model for pLM bias Before presenting empirical results we provide a brief conceptual example to illustrate why pLMs learn a species bias pLMs are sequence models with architectures and training objectives inspired by language models trained on text data Figure 2 Most if not all protein language models are trained on samples from the UniProt protein sequence database with either an autoregressive AR next token prediction task or a masked language modeling task For the\n"
            },
            {
                "rank": 3,
                "score": -1.9864468574523926,
                "text": "dataset excludes artificial sequences as well as any sequences having similarity to the test set of de novo proteins used in the evaluations Appendix A 1 ESM2 is trained with the masked language modeling objective 29 to recover the identity of amino acids from their context in the rest of the sequence Fig 1B This training objective has been shown to materialize information about the folded structure of proteins in the internal representations of the model 79 30 Since the training of the language model is only on sequences information about structure that emerges must be the result of the unsupervised learning of patterns in sequences A linear projection from the attention maps of the language model identifies internal states that reflect protein structure Previous work has shown\n"
            },
            {
                "rank": 4,
                "score": -2.5188183784484863,
                "text": "samples from naturally occurring protein families To solve this problem with a model grounded in sequences it will be necessary to learn sequence patterns that generalize beyond individual protein families Evolutionary scale language models go beyond classic protein family models by training on diverse sequences across evolution which means that they have the potential to learn deep patterns across all proteins including where there is no experimental structure There is evidence for local patterns in sequences that generalize beyond individual protein families in the form of motifs that are local in the sequence 23 as well as motifs that are local in 3d space 24 However the mapping between sequence and structure is not one to one 25 and designing sequences to reach a well folded native state\n"
            },
            {
                "rank": 5,
                "score": -2.653987169265747,
                "text": "for non coding genomic sequences In previous studies pre trained gLMs have found some success by focusing on specific regions of the genome during pre training or working with simpler organisms with compact genomes28 34 67 For instance a BERT based LLM trained strictly in the coding genome can provide more context than only considering amino acids with protein language modeling e g codon usage 3537 However our evaluation shows that extending the pre training task across the whole genome struggles to capture meaningful representations in the non coding genome The performance gap may be due to differences in the structure of the coding regions versus the non coding regions To elaborate protein sequences have a clear start and end with low level grammars i e secondary structures\n"
            },
            {
                "rank": 6,
                "score": -2.9628043174743652,
                "text": "7 Discussion Advances in language modeling at scale are bringing the goal of a general purpose model for proteins closer to realization This line of work aspires to a model that learns to read and write biology in its native language that can be directly applied across a range of protein understanding and design tasks For scalability learning from sequences is important while there are no central databases of high throughput functional measurements and few compilations exist billions of sequences are available to learn from in sequence databases 49 50 Sequences give an unparalleled view into 9 CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the\n"
            },
            {
                "rank": 7,
                "score": -3.072134494781494,
                "text": "sequence modeling arXiv preprint arXiv 1904 01038 N Ousidhoum X Zhao T Fang Y Song and D Y Yeung 2021 Probing toxic content in large pre trained language models In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing Volume 1 Long Papers pages 42624274 C Outeiral and C Deane 2022 Codon language embeddings provide strong signals for protein engineering bioRxiv pages 202212 L Ouyang J Wu X Jiang D Almeida C Wainwright P Mishkin C Zhang S Agarwal et al 2022 Training language models to follow instructions with human feedback InAdvances in Neural Information Processing Systems M Pagliardini D Paliotta M Jaggi and F Fleuret 2023 Faster causal attention over large sequences through sparse\n"
            },
            {
                "rank": 8,
                "score": -3.6266655921936035,
                "text": "pipeline with a single forward pass of a pre trained end to end protein language model In the last year protein language modeling with an unsupervised training objective has been investigated by multiple groups Rives et al 2019 Alley et al 2019 Heinzinger et al 2019 Rao et al 2019 Madani et al 2020 The longstanding practice in bioinformatics has been to fit linear models on focused sets of evolutionarily related and aligned sequences by contrast protein language modeling trains nonlinear deep neural networks on large databases of evolutionarily diverse and unaligned sequences High capacity protein language models have been shown to learn underlying intrinsic properties of proteins such as structure and function from sequence data Rives et al 2019 A line of work in this emerging field\n"
            },
            {
                "rank": 9,
                "score": -4.221642017364502,
                "text": "bioRxiv preprint Language models generalize beyond natural proteins this section except in the case of motifs are made for hits returned by sequence search comparison of predicted structure for designs to known structure databases always returns hits likely to possess a similar fold Fig S10 For sequence search we use jackhmmer 3 3 2 a sequence search tool from the HMMER suite 58 Two jackhmmer settings were modified from their defaults based on failure modes observed during during our analysis which queries distant de novo sequences against large scale 100M sequences search databases 1 One Iteration Jackhmmer was run with only 1 iteration instead of multiple the default This change was made because it was observed that additional iterations resulted in a growing amount of returned spurious hits\n"
            }
        ]
    },
    {
        "query": "How might the integration of large language models and structural biology data impact protein function prediction?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 0,
                "score": 0.99719906,
                "text": "Similar to observations on the learning of tertiary protein structure in large language models 12 26 we find that increasing the scale of models leads to improvements in function learning The understanding of mutational landscapes in the models correlates with the molecular basis of function in proteins capturing binding sites and amino acid preferences that are determined by the folded structure Zero shot transfer is an interesting capability of large scale language models and represents a major point of departure from the unsupervised learning methods that are the basis for current state of theart inference of protein structure and function The capability for zero shot transfer implies that a model can be trained once and then applied to perform inference for many tasks It is also a window\n"
            },
            {
                "rank": 1,
                "index": 1,
                "score": 0.98799264,
                "text": "using a language model and deep learning Nature Biotechnology 40 16171623 2022 11 Wu R et al High resolution de novo structure prediction from primary sequence BioRxiv 202207 2022 12 Brandes N Goldman G Wang C H Ye C J Ntranos V Genome wide prediction of disease variant effects with a deep protein language model Nature Genetics 55 15121522 2023 13 Meier J et al Language models enable zero shot prediction of the effects of mutations on protein function Advances in Neural Information Processing Systems 34 2928729303 2021 10 18 CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The\n"
            },
            {
                "rank": 2,
                "index": 6,
                "score": 0.9765353,
                "text": "J Rao R Verkuil R Liu J Sercu T and Rives A 2021 Language models enable zero shot prediction of the effects of mutationson protein function In Advances in Neural Information ProcessingSystems 34 M Ranzato A Beygelzimer K Nguyen P S Liang J W Vaughan and Y Dauphin eds 8 Kalchbrenner N Espeholt L Simonyan K van den Oord A Graves A and Kavukcuoglu K 2016 Neural Machine Translation in LinearTime Preprint at arXiv https arxiv org abs 1610 10099 9 Shanehsazzadeh A Belanger D and Dohan D 2020 Is transfer learning necessary for protein landscape prediction Preprint at arXiv https arxiv org abs 2011 03443 10 Elnaggar A Essam H Salah Eldin W Moustafa W Elkerdawy M Rochereau C and Rost B 2023 Ankh optimized protein languagemodel\n"
            },
            {
                "rank": 3,
                "index": 5,
                "score": 0.93257624,
                "text": "diffusion 53 C Shi C Wang J Lu B Zhong J Tang The Eleventh International Conference on Learning Representations 2022 Protein sequence and structure co design with equivariant translation 54 A Madani et al Nature Biotechnology 41 10991106 2023 Large language models generate functional protein sequences across diverse families 55 N Ferruz S Schmidt B H ocker Nature Communications 13 4348 2022 ProtGPT2 is a deep unsupervised language model for protein design 56 T F Truong Jr T Bepler arXiv preprint arXiv 2306 06156 2023 PoET A generative model of protein families as sequences of sequences 57 L Zhang J Chen T Shen Y Li S Sun arXiv preprint arXiv 2306 01824 2023 Enhancing the protein tertiary structure prediction by multiple sequence alignment generation 58 A Rives et\n"
            },
            {
                "rank": 4,
                "index": 4,
                "score": 0.8331889,
                "text": "copyright holder for this preprint this version posted March 4 2024 https doi org 10 1101 2024 02 29 582810doi bioRxiv preprint 14 Madani A et al Large language models generate functional protein sequences across diverse families Nature Biotechnology18 2023 15 Ferruz N Hcker B Controllable protein design with language models Nature Machine Intelligence 4 521532 2022 16 Hie B L et al Efficient evolution of human antibodies from general protein language models Nature Biotechnology 2023 17 Hie B L Yang K K Kim P S Evolutionary velocity with protein language models predicts evolutionary dynamics of diverse proteins Cell Systems 13 274285 2022 18 Zhang Z et al Protein language models learn evolutionary statistics of interacting sequence motifs bioRxiv 202401 2024 19 Consens M E et al To\n"
            },
            {
                "rank": 5,
                "index": 7,
                "score": 0.40380195,
                "text": "on protein sequences 7 Information about the folded three dimensional structure of proteins develops within the models extending to atomic resolution structure 8 This information emerges through training on sequences alone At the same time the structural information that emerges as a result of training on sequences has been shown to depend on the available evolutionary information varying as a function of the number of related proteins in the training data 8 9 It is an open question across domains to what extent language models are capable of generalizing outside their training data In biology it is unknown whether language models can be used to explore a design space beyond that of natural proteins Here we demonstrate that language models generalize beyond natural proteins to generate de novo\n"
            },
            {
                "rank": 6,
                "index": 8,
                "score": 0.29746994,
                "text": "S and Ho cker B 2022 ProtGPT2 is a deep unsupervised language model for protein design Nat Commun 13 4348 32 Madani A Krause B Greene E R Subramanian S Mohr B P Holton J M Olmos J L Jr Xiong C Sun Z Z Socher R et al 2023 Large language models generate functional protein sequences acrossdiverse families Nat Biotechnol 41 10991106 33 Verkuil R Kabeli O Du Y Wicky B I M Milles L F Dauparas J Baker D Ovchinnikov S Sercu T and Rives A 2022 Languagemodels generalize beyond natural proteins Preprint at bioRxiv 34 Alley E C Khimulya G Biswas S AlQuraishi M and Church G M 2019 Unified rational protein engineering with sequence based deeprepresentation learning Nat Methods 16 13151322 35 Shin J\n"
            },
            {
                "rank": 7,
                "index": 9,
                "score": 0.29746994,
                "text": "S and Ho cker B 2022 ProtGPT2 is a deep unsupervised language model for protein design Nat Commun 13 4348 32 Madani A Krause B Greene E R Subramanian S Mohr B P Holton J M Olmos J L Jr Xiong C Sun Z Z Socher R et al 2023 Large language models generate functional protein sequences acrossdiverse families Nat Biotechnol 41 10991106 33 Verkuil R Kabeli O Du Y Wicky B I M Milles L F Dauparas J Baker D Ovchinnikov S Sercu T and Rives A 2022 Languagemodels generalize beyond natural proteins Preprint at bioRxiv 34 Alley E C Khimulya G Biswas S AlQuraishi M and Church G M 2019 Unified rational protein engineering with sequence based deeprepresentation learning Nat Methods 16 13151322 35 Shin J\n"
            },
            {
                "rank": 8,
                "index": 2,
                "score": 0.14463143,
                "text": "V Karius K and Kosinski J 2022 Integrative structural modeling of macromolecular complexes using assembline Nat Protoc 17 152176 140 Russel D Lasker K Webb B Vela zquez Muriel J Tjioe E Schneidman Duhovny D Peterson B and Sali A 2012 Putting the pieces together integrative modeling platform software for structure determina tion of macromolecular assemblies PLoS Biol 10 e1001244 141 Rout M P and Sali A 2019 Principles for integrative structural biology studies Cell 177 13841403 142 Senior A W Evans R Jumper J Kirkpatrick J Sifre L Green T Qin C C20Zdek A Nelson A W R Bridgland A et al 2020 Improved protein structure prediction using potentials from deep learning Nature577 706710 143 Tunyasuvunakool K Adler J Wu Z Green T Zielinski M C20Zdek A\n"
            },
            {
                "rank": 9,
                "index": 3,
                "score": 0.14463143,
                "text": "V Karius K and Kosinski J 2022 Integrative structural modeling of macromolecular complexes using assembline Nat Protoc 17 152176 140 Russel D Lasker K Webb B Vela zquez Muriel J Tjioe E Schneidman Duhovny D Peterson B and Sali A 2012 Putting the pieces together integrative modeling platform software for structure determina tion of macromolecular assemblies PLoS Biol 10 e1001244 141 Rout M P and Sali A 2019 Principles for integrative structural biology studies Cell 177 13841403 142 Senior A W Evans R Jumper J Kirkpatrick J Sifre L Green T Qin C C20Zdek A Nelson A W R Bridgland A et al 2020 Improved protein structure prediction using potentials from deep learning Nature577 706710 143 Tunyasuvunakool K Adler J Wu Z Green T Zielinski M C20Zdek A\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -0.21447546780109406,
                "text": "diffusion 53 C Shi C Wang J Lu B Zhong J Tang The Eleventh International Conference on Learning Representations 2022 Protein sequence and structure co design with equivariant translation 54 A Madani et al Nature Biotechnology 41 10991106 2023 Large language models generate functional protein sequences across diverse families 55 N Ferruz S Schmidt B H ocker Nature Communications 13 4348 2022 ProtGPT2 is a deep unsupervised language model for protein design 56 T F Truong Jr T Bepler arXiv preprint arXiv 2306 06156 2023 PoET A generative model of protein families as sequences of sequences 57 L Zhang J Chen T Shen Y Li S Sun arXiv preprint arXiv 2306 01824 2023 Enhancing the protein tertiary structure prediction by multiple sequence alignment generation 58 A Rives et\n"
            },
            {
                "rank": 1,
                "score": -0.7206753492355347,
                "text": "copyright holder for this preprint this version posted March 4 2024 https doi org 10 1101 2024 02 29 582810doi bioRxiv preprint 14 Madani A et al Large language models generate functional protein sequences across diverse families Nature Biotechnology18 2023 15 Ferruz N Hcker B Controllable protein design with language models Nature Machine Intelligence 4 521532 2022 16 Hie B L et al Efficient evolution of human antibodies from general protein language models Nature Biotechnology 2023 17 Hie B L Yang K K Kim P S Evolutionary velocity with protein language models predicts evolutionary dynamics of diverse proteins Cell Systems 13 274285 2022 18 Zhang Z et al Protein language models learn evolutionary statistics of interacting sequence motifs bioRxiv 202401 2024 19 Consens M E et al To\n"
            },
            {
                "rank": 2,
                "score": -0.9129613041877747,
                "text": "J Rao R Verkuil R Liu J Sercu T and Rives A 2021 Language models enable zero shot prediction of the effects of mutationson protein function In Advances in Neural Information ProcessingSystems 34 M Ranzato A Beygelzimer K Nguyen P S Liang J W Vaughan and Y Dauphin eds 8 Kalchbrenner N Espeholt L Simonyan K van den Oord A Graves A and Kavukcuoglu K 2016 Neural Machine Translation in LinearTime Preprint at arXiv https arxiv org abs 1610 10099 9 Shanehsazzadeh A Belanger D and Dohan D 2020 Is transfer learning necessary for protein landscape prediction Preprint at arXiv https arxiv org abs 2011 03443 10 Elnaggar A Essam H Salah Eldin W Moustafa W Elkerdawy M Rochereau C and Rost B 2023 Ankh optimized protein languagemodel\n"
            },
            {
                "rank": 3,
                "score": -1.1761540174484253,
                "text": "using a language model and deep learning Nature Biotechnology 40 16171623 2022 11 Wu R et al High resolution de novo structure prediction from primary sequence BioRxiv 202207 2022 12 Brandes N Goldman G Wang C H Ye C J Ntranos V Genome wide prediction of disease variant effects with a deep protein language model Nature Genetics 55 15121522 2023 13 Meier J et al Language models enable zero shot prediction of the effects of mutations on protein function Advances in Neural Information Processing Systems 34 2928729303 2021 10 18 CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The\n"
            },
            {
                "rank": 4,
                "score": -1.339280366897583,
                "text": "Similar to observations on the learning of tertiary protein structure in large language models 12 26 we find that increasing the scale of models leads to improvements in function learning The understanding of mutational landscapes in the models correlates with the molecular basis of function in proteins capturing binding sites and amino acid preferences that are determined by the folded structure Zero shot transfer is an interesting capability of large scale language models and represents a major point of departure from the unsupervised learning methods that are the basis for current state of theart inference of protein structure and function The capability for zero shot transfer implies that a model can be trained once and then applied to perform inference for many tasks It is also a window\n"
            },
            {
                "rank": 5,
                "score": -1.3516597747802734,
                "text": "S and Ho cker B 2022 ProtGPT2 is a deep unsupervised language model for protein design Nat Commun 13 4348 32 Madani A Krause B Greene E R Subramanian S Mohr B P Holton J M Olmos J L Jr Xiong C Sun Z Z Socher R et al 2023 Large language models generate functional protein sequences acrossdiverse families Nat Biotechnol 41 10991106 33 Verkuil R Kabeli O Du Y Wicky B I M Milles L F Dauparas J Baker D Ovchinnikov S Sercu T and Rives A 2022 Languagemodels generalize beyond natural proteins Preprint at bioRxiv 34 Alley E C Khimulya G Biswas S AlQuraishi M and Church G M 2019 Unified rational protein engineering with sequence based deeprepresentation learning Nat Methods 16 13151322 35 Shin J\n"
            },
            {
                "rank": 6,
                "score": -1.3516597747802734,
                "text": "S and Ho cker B 2022 ProtGPT2 is a deep unsupervised language model for protein design Nat Commun 13 4348 32 Madani A Krause B Greene E R Subramanian S Mohr B P Holton J M Olmos J L Jr Xiong C Sun Z Z Socher R et al 2023 Large language models generate functional protein sequences acrossdiverse families Nat Biotechnol 41 10991106 33 Verkuil R Kabeli O Du Y Wicky B I M Milles L F Dauparas J Baker D Ovchinnikov S Sercu T and Rives A 2022 Languagemodels generalize beyond natural proteins Preprint at bioRxiv 34 Alley E C Khimulya G Biswas S AlQuraishi M and Church G M 2019 Unified rational protein engineering with sequence based deeprepresentation learning Nat Methods 16 13151322 35 Shin J\n"
            },
            {
                "rank": 7,
                "score": -1.5584609508514404,
                "text": "V Karius K and Kosinski J 2022 Integrative structural modeling of macromolecular complexes using assembline Nat Protoc 17 152176 140 Russel D Lasker K Webb B Vela zquez Muriel J Tjioe E Schneidman Duhovny D Peterson B and Sali A 2012 Putting the pieces together integrative modeling platform software for structure determina tion of macromolecular assemblies PLoS Biol 10 e1001244 141 Rout M P and Sali A 2019 Principles for integrative structural biology studies Cell 177 13841403 142 Senior A W Evans R Jumper J Kirkpatrick J Sifre L Green T Qin C C20Zdek A Nelson A W R Bridgland A et al 2020 Improved protein structure prediction using potentials from deep learning Nature577 706710 143 Tunyasuvunakool K Adler J Wu Z Green T Zielinski M C20Zdek A\n"
            },
            {
                "rank": 8,
                "score": -1.5584609508514404,
                "text": "V Karius K and Kosinski J 2022 Integrative structural modeling of macromolecular complexes using assembline Nat Protoc 17 152176 140 Russel D Lasker K Webb B Vela zquez Muriel J Tjioe E Schneidman Duhovny D Peterson B and Sali A 2012 Putting the pieces together integrative modeling platform software for structure determina tion of macromolecular assemblies PLoS Biol 10 e1001244 141 Rout M P and Sali A 2019 Principles for integrative structural biology studies Cell 177 13841403 142 Senior A W Evans R Jumper J Kirkpatrick J Sifre L Green T Qin C C20Zdek A Nelson A W R Bridgland A et al 2020 Improved protein structure prediction using potentials from deep learning Nature577 706710 143 Tunyasuvunakool K Adler J Wu Z Green T Zielinski M C20Zdek A\n"
            },
            {
                "rank": 9,
                "score": -1.57253098487854,
                "text": "on protein sequences 7 Information about the folded three dimensional structure of proteins develops within the models extending to atomic resolution structure 8 This information emerges through training on sequences alone At the same time the structural information that emerges as a result of training on sequences has been shown to depend on the available evolutionary information varying as a function of the number of related proteins in the training data 8 9 It is an open question across domains to what extent language models are capable of generalizing outside their training data In biology it is unknown whether language models can be used to explore a design space beyond that of natural proteins Here we demonstrate that language models generalize beyond natural proteins to generate de novo\n"
            }
        ]
    },
    {
        "query": "What potential advancements could be made in information retrieval systems by incorporating transformer-based models?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 2,
                "score": 0.86465365,
                "text": "transformer models More recently E m sc d sc r sc2 Sachan et al 2021 extends F i scDby using an expectation maximization algorithm to train the retriever end to end and achieves state of the art results compared to similarly sized models In the open domain dialogue setting BlenderBot 2 0 Komeili et al 2021 learns to issue textual internet queries outperforming dense retrieval methods when evaluated on a task measuring how close model responses are to those of humans This involves collecting a dataset of human dialogues with associated search queries which limits the scalability of this approach Hashemi et al 2020 introduce the Guided Transformer a modified Transformer similar to R e sc t sc r sc o sc for document retrieval and clarifying question\n"
            },
            {
                "rank": 1,
                "index": 0,
                "score": 0.5137371,
                "text": "replaced LSTMs by transformer networks and scaled the memory to billions of tokens leading to strong performance improvements More recently RETRO Borgeaud et al 2021 extended these by scaling the retrieval memory to trillions of tokens and changing the model architecture to take retrieved documents as input 3 1 4 Retrieval Augmentation with Search Engines Recently different works have proposed to train large language models to interact with a search engine by generating text queries and using the retrieved documents as additional context Nakano et al 2021 Thoppilan et al 2022 Shuster et al 2022 In the context of few shot question answering Lazaridou et al 2022 used the question to perform a search query and retrieved documents are added to the prompt of a large language model\n"
            },
            {
                "rank": 2,
                "index": 5,
                "score": 0.26020142,
                "text": "the last several years dual encoders Gillick et al 2018 Karpukhin et al 2020 Ni et al 2022b Chen et al 2022 have dominated the landscape for first stage information retrieval They model relevance by mapping queries and documents into the same embedding space optimized via contrastive learning Hadsell et al 2006 Gao et al Equal Contribution Work completed while a Student Researcher at Google 2021 Dense embeddings are pre computed for all documents in a corpus and stored in an external index This allows for fast approximate nearest neighbor search Vanderkam et al 2013 Johnson et al 2021 to retrieve relevant documents Cross encoders based on large Transformer models Nogueira et al 2019b 2020 Pradeep et al 2021b often function on top of these retrieved documents to\n"
            },
            {
                "rank": 3,
                "index": 6,
                "score": 0.18227991,
                "text": "740 20 No 25 News 236 918 398 No 10 Wikipedia 13 288 23 Yes 5 GitHub 374 952 143 No 5 4 Improving language models by retrieving from trillions of tokens 2 4 R e sc t sc r sc o scmodel architecture Our model relies on an encoder decoder transformer architecture integrating the retrieved data through a cross attention mechanism as introduced in Vaswani et al 2017 First the retrieved tokens R e sc t scare fed into an encoder Transformer which computes the encoded neighbours set Denoting the intermediate activations by our transformer decoder then interleaves R e sc t sc r sc o sc blocks R e sc t sc r sc o scandstandardTransformerblocks LM thehyperparameter 1determinesat which layers we use a R e\n"
            },
            {
                "rank": 4,
                "index": 3,
                "score": 0.13420755,
                "text": "with transformers REALM Guu et al 2020 MARGE Lewis et al 2020a RAG Lewis et al 2020b and composite memory for dialog Fan et al 2021 retrieve documents from a knowledge base to improve question answering or dialogue The knowledge base consists of text snippets and is static and typically separate from the inputs and outputs of the models Instead we focus on language modeling using a decoder only model and propose a simple model that unifies attention and retrieval k nearest neighbor lookup is a general purpose technique that is used for a wide variety of machine learning and retrieval tasks and high performance implementations are available for various architectures Johnson et al 2021 Guo et al 2020 Memory efficient Transformers Gupta et al 2021 replace dense\n"
            },
            {
                "rank": 5,
                "index": 9,
                "score": 0.05089372,
                "text": "autoregressive transformer and a hyperprior using transformer encoders instead of CNNs there as well They introduce a top k scheme in the attention layer and a special relative positional encoding to handle arbitrary resolutions El Nouby et al use a Masked Image Model MIM combined with a Product Quantization PQ variant of VQ V AE While the approach is promising for extreme compression in terms of rate distortion the method is lagging behind state of the art significantly Konyuncu et al propose a transformer based entropy model that is fully auto regressive over the spatial and channel dimensions which leads to prohibitively slow decode times 10 minutes for a 4K image Other works 40 41 have explored the use of window based transformers for the synthesis transform For\n"
            },
            {
                "rank": 6,
                "index": 1,
                "score": 0.009596455,
                "text": "dependencies To address this shortcoming Vaswani et al 2017 introduced the transformer architecture a seminal work that transformed RE and NLP in general The transformer is similar in structure to other sequence transduction models in that it consists of two modules an encoder module and a decoder module The encoder takes an input sequence x and produces a dense representation zthat is fed to the decoder The decoder then uses zto produce an output sequence y However the transformer was unique in its use of stacked multi head attention functions in the encoder and decoder modules In the original transformer architecture the encoder and decoder modules consisted of six identical and serially connected layers Each layer contained a multi head attention function as well as a feedforward network\n"
            },
            {
                "rank": 7,
                "index": 8,
                "score": 0.00619299,
                "text": "Search engines typically involve large multi tier architectures and the retrieval process generally consists of multiple stages of pruning the candidate set of documents The IR model at the bottom of this telescoping setup may need to sift through billions of documentswhile the model at the top may only need to re rank between tens of promising documents The retrieval approaches that are suitable at one level of the stack may be highly impractical at a different stepmodels at the bottom need to be fastbut mostly focus on eliminating irrelevant or junk results while models at the top tend to develop more sophisticated notions of relevance and focus on distinguishing between documents that are much closer on the relevance scale So far much of the focus on neural\n"
            },
            {
                "rank": 8,
                "index": 4,
                "score": 0.0054905633,
                "text": "the model Given the importance of large autoregressive models and specifically large Transformers several approaches were Equal contribution1Google Research Mountain View CA USA Correspondence to Yaniv Leviathan leviathan google com Proceedings of the 40thInternational Conference on Machine Learning Honolulu Hawaii USA PMLR 202 2023 Copyright 2023 by the author s developed to make inference from them faster Some approaches aim to reduce the inference cost for allinputs equally e g Hinton et al 2015 Jaszczur et al 2021 Hubara et al 2016 So et al 2021 Shazeer 2019 Other approaches stem from the observation that not all inference steps are born alike some require a very large model while others can be approximated well by more efficient models These adaptive computation methods e g Han et al 2021\n"
            },
            {
                "rank": 9,
                "index": 7,
                "score": 0.00039357558,
                "text": "Information Processing Systems 35 3235332368 2022 Y Zhang A Backurs S Bubeck R Eldan S Gunasekar and T Wagner Unveiling transformers with lego a synthetic reasoning task arXiv preprint arXiv 2206 04301 2022 H Zhou A Nova H Larochelle A Courville B Neyshabur and H Sedghi Teaching algorithmic reasoning via in context learning arXiv preprint arXiv 2211 09066 2022 H Zhou A Bradley E Littwin N Razin O Saremi J Susskind S Bengio and P Nakkiran What algorithms can transformers learn a study in length generalization arXiv preprint arXiv 2310 16028 2023 14 Transformers Can Achieve Length Generalization But Not Robustly A Positional Encoding A 1 Additive Relative Positional Encoding RPE Shaw et al 2018 pioneered additive RPE by integrating position encodings into the attention layers key and\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -0.7403373122215271,
                "text": "the last several years dual encoders Gillick et al 2018 Karpukhin et al 2020 Ni et al 2022b Chen et al 2022 have dominated the landscape for first stage information retrieval They model relevance by mapping queries and documents into the same embedding space optimized via contrastive learning Hadsell et al 2006 Gao et al Equal Contribution Work completed while a Student Researcher at Google 2021 Dense embeddings are pre computed for all documents in a corpus and stored in an external index This allows for fast approximate nearest neighbor search Vanderkam et al 2013 Johnson et al 2021 to retrieve relevant documents Cross encoders based on large Transformer models Nogueira et al 2019b 2020 Pradeep et al 2021b often function on top of these retrieved documents to\n"
            },
            {
                "rank": 1,
                "score": -1.493349552154541,
                "text": "transformer models More recently E m sc d sc r sc2 Sachan et al 2021 extends F i scDby using an expectation maximization algorithm to train the retriever end to end and achieves state of the art results compared to similarly sized models In the open domain dialogue setting BlenderBot 2 0 Komeili et al 2021 learns to issue textual internet queries outperforming dense retrieval methods when evaluated on a task measuring how close model responses are to those of humans This involves collecting a dataset of human dialogues with associated search queries which limits the scalability of this approach Hashemi et al 2020 introduce the Guided Transformer a modified Transformer similar to R e sc t sc r sc o sc for document retrieval and clarifying question\n"
            },
            {
                "rank": 2,
                "score": -1.5253486633300781,
                "text": "replaced LSTMs by transformer networks and scaled the memory to billions of tokens leading to strong performance improvements More recently RETRO Borgeaud et al 2021 extended these by scaling the retrieval memory to trillions of tokens and changing the model architecture to take retrieved documents as input 3 1 4 Retrieval Augmentation with Search Engines Recently different works have proposed to train large language models to interact with a search engine by generating text queries and using the retrieved documents as additional context Nakano et al 2021 Thoppilan et al 2022 Shuster et al 2022 In the context of few shot question answering Lazaridou et al 2022 used the question to perform a search query and retrieved documents are added to the prompt of a large language model\n"
            },
            {
                "rank": 3,
                "score": -1.8377457857131958,
                "text": "740 20 No 25 News 236 918 398 No 10 Wikipedia 13 288 23 Yes 5 GitHub 374 952 143 No 5 4 Improving language models by retrieving from trillions of tokens 2 4 R e sc t sc r sc o scmodel architecture Our model relies on an encoder decoder transformer architecture integrating the retrieved data through a cross attention mechanism as introduced in Vaswani et al 2017 First the retrieved tokens R e sc t scare fed into an encoder Transformer which computes the encoded neighbours set Denoting the intermediate activations by our transformer decoder then interleaves R e sc t sc r sc o sc blocks R e sc t sc r sc o scandstandardTransformerblocks LM thehyperparameter 1determinesat which layers we use a R e\n"
            },
            {
                "rank": 4,
                "score": -1.9824398756027222,
                "text": "with transformers REALM Guu et al 2020 MARGE Lewis et al 2020a RAG Lewis et al 2020b and composite memory for dialog Fan et al 2021 retrieve documents from a knowledge base to improve question answering or dialogue The knowledge base consists of text snippets and is static and typically separate from the inputs and outputs of the models Instead we focus on language modeling using a decoder only model and propose a simple model that unifies attention and retrieval k nearest neighbor lookup is a general purpose technique that is used for a wide variety of machine learning and retrieval tasks and high performance implementations are available for various architectures Johnson et al 2021 Guo et al 2020 Memory efficient Transformers Gupta et al 2021 replace dense\n"
            },
            {
                "rank": 5,
                "score": -2.7321736812591553,
                "text": "Information Processing Systems 35 3235332368 2022 Y Zhang A Backurs S Bubeck R Eldan S Gunasekar and T Wagner Unveiling transformers with lego a synthetic reasoning task arXiv preprint arXiv 2206 04301 2022 H Zhou A Nova H Larochelle A Courville B Neyshabur and H Sedghi Teaching algorithmic reasoning via in context learning arXiv preprint arXiv 2211 09066 2022 H Zhou A Bradley E Littwin N Razin O Saremi J Susskind S Bengio and P Nakkiran What algorithms can transformers learn a study in length generalization arXiv preprint arXiv 2310 16028 2023 14 Transformers Can Achieve Length Generalization But Not Robustly A Positional Encoding A 1 Additive Relative Positional Encoding RPE Shaw et al 2018 pioneered additive RPE by integrating position encodings into the attention layers key and\n"
            },
            {
                "rank": 6,
                "score": -3.3218350410461426,
                "text": "autoregressive transformer and a hyperprior using transformer encoders instead of CNNs there as well They introduce a top k scheme in the attention layer and a special relative positional encoding to handle arbitrary resolutions El Nouby et al use a Masked Image Model MIM combined with a Product Quantization PQ variant of VQ V AE While the approach is promising for extreme compression in terms of rate distortion the method is lagging behind state of the art significantly Konyuncu et al propose a transformer based entropy model that is fully auto regressive over the spatial and channel dimensions which leads to prohibitively slow decode times 10 minutes for a 4K image Other works 40 41 have explored the use of window based transformers for the synthesis transform For\n"
            },
            {
                "rank": 7,
                "score": -3.589231252670288,
                "text": "dependencies To address this shortcoming Vaswani et al 2017 introduced the transformer architecture a seminal work that transformed RE and NLP in general The transformer is similar in structure to other sequence transduction models in that it consists of two modules an encoder module and a decoder module The encoder takes an input sequence x and produces a dense representation zthat is fed to the decoder The decoder then uses zto produce an output sequence y However the transformer was unique in its use of stacked multi head attention functions in the encoder and decoder modules In the original transformer architecture the encoder and decoder modules consisted of six identical and serially connected layers Each layer contained a multi head attention function as well as a feedforward network\n"
            },
            {
                "rank": 8,
                "score": -4.068913459777832,
                "text": "the model Given the importance of large autoregressive models and specifically large Transformers several approaches were Equal contribution1Google Research Mountain View CA USA Correspondence to Yaniv Leviathan leviathan google com Proceedings of the 40thInternational Conference on Machine Learning Honolulu Hawaii USA PMLR 202 2023 Copyright 2023 by the author s developed to make inference from them faster Some approaches aim to reduce the inference cost for allinputs equally e g Hinton et al 2015 Jaszczur et al 2021 Hubara et al 2016 So et al 2021 Shazeer 2019 Other approaches stem from the observation that not all inference steps are born alike some require a very large model while others can be approximated well by more efficient models These adaptive computation methods e g Han et al 2021\n"
            },
            {
                "rank": 9,
                "score": -4.41301155090332,
                "text": "Search engines typically involve large multi tier architectures and the retrieval process generally consists of multiple stages of pruning the candidate set of documents The IR model at the bottom of this telescoping setup may need to sift through billions of documentswhile the model at the top may only need to re rank between tens of promising documents The retrieval approaches that are suitable at one level of the stack may be highly impractical at a different stepmodels at the bottom need to be fastbut mostly focus on eliminating irrelevant or junk results while models at the top tend to develop more sophisticated notions of relevance and focus on distinguishing between documents that are much closer on the relevance scale So far much of the focus on neural\n"
            }
        ]
    },
    {
        "query": "Discuss the possible implications of applying attention mechanisms to analyze gene regulatory networks.",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 6,
                "score": 0.03904829,
                "text": "gating operation s to be combined into one macro operation which would require a network of depth 10 for its implementation inside the SM 5 Cardinal Capacity Review We have seen that attention mechanisms enable important fun ctionalities with minimal depth compared to the equivalent SM circuits at the cost of a dding attention neurons and mechanisms Here we want to better understand the trade offs be tween the computations that are enabled and the corresponding costs The key concep t for doing so is the concept of cardinal capacity which we briefly review below THE QUARKS OF ATTENTION 17 Figure 9 Standard model neural network for normalizing a vector u1 u2 u3 for clarity only the first normalized component is fully shown Figure 10 Standard model neural\n"
            },
            {
                "rank": 1,
                "index": 8,
                "score": 0.0009110512,
                "text": "both at the level of the source and at the level of the target F or instance in synaptic gating can the attending output of a neuron gate more than on e synapse Can the attending output of several neurons gate the same synapse A nd so forth In the most simple cases we will assume that the multiplicity is on e both at the source and at the target but greater multiplicities will also be consi dered for instance in some of the theorems in Sections 6and7 5 Time Scales Finally for simplicity and in line with current deep learn ing attention models we assume that the attention mechanisms operat e on the time scale of individual inputs Different inputs create different attent ion signals Alternative possibilities\n"
            },
            {
                "rank": 2,
                "index": 7,
                "score": 0.00059071044,
                "text": "the organization of the feature vectors allows it to be applied to various problems that each use data with different structures as illustrated by the previous domain examples Yet this can be taken even further by applying attention to data where there is irregular structure For example protein structures city traffic flows and communication networks cannot always be represented using neatly structured organizations such as sequences like time series or grids like images In such cases the different aspects of the data are often represented as nodes in a graph These nodes can be represented by feature vectors meaning that attention can be applied in domains that use graph structured data as well In general attention can be applied to any problem for which a set of feature\n"
            },
            {
                "rank": 3,
                "index": 4,
                "score": 0.0003500686,
                "text": "The copyright holder for this preprint which this version posted May 1 2023 https doi org 10 1101 2023 04 30 538439doi bioRxiv preprint steps in Supplementary Figure S1B For example in one iteration i 1 2 K the attention 504 masking mechanism allows attention with all predicted genes from previous 0 to i1 iterations In 505 each iteration scGPT selects the top 1 Kgenes from the unknown set with the highest prediction 506 confidence to be included as known genes in the next iteration i 1 Intuitively this workflow 507 streamlines the generation of large groups of gene expressions in an auto regressive manner where 508 gene expressions with highest prediction confidence are first generated and used to help subsequent 509 rounds of generation The gene prompt\n"
            },
            {
                "rank": 4,
                "index": 5,
                "score": 0.0002959571,
                "text": "synapse they would require new complex mechanisms in a physical implementation Furthermore they do not occur in current attention based deep learning models The same can be said for the activation being the direct source of the attending signal Even moreunlikely would bethecase of mixed schemes where the attending signal would emanate for instance from both neuronal outputs and synapses In short the reasonable assumption that the atte nding signals emanate from neuronal outputs allows us to reduce the number of possi bilities by a factor of three leaving 6 basic possibilities Table 1 2 Target For the target of an attention signal we will study all three possibilities Thus attention signals can target activations S outputs O or synapses w We will call these three forms of\n"
            },
            {
                "rank": 5,
                "index": 1,
                "score": 0.0001313518,
                "text": "interpretability of attention models is that researchers must be extremely careful when drawing conclusions based on attention patterns For example problems with an attention model can be diagnosed via the attention weights if the model is found to focus on the incorrect parts of the data if such information is available Yet conversely attention weights may only be used to obtain plausible explanations for why certain parts of the data are focused on rather than concluding that those 17 parts are significant to the problem However one should still be cautious as the viability of such approaches can depend on the model architecture 5 C ONCLUSION In this survey we have provided an overview of recent research on attention models in deep learning Attention mechanisms have been a\n"
            },
            {
                "rank": 6,
                "index": 9,
                "score": 2.775165e-05,
                "text": "activations in particu lar gating by polynomial threshold units will be studied in Sections 6and7 2 7 Summary In summary the quarks of attention can be classified based on the origin the target and the interaction mechanism of the attention s ignal Assuming that the origin is in the output of one neuron or a group of neurons and that t he interactions are either additive or multiplicative this leads to six classes Tabl e1 Within the additive group two classes are already in the SM additive activation and addit ive output attention and only one class is of interest here for further studies additive a ctivation attention or multiplexing Within the multiplicative group all three classes corresp ond to true extensions of the SM and at least\n"
            },
            {
                "rank": 7,
                "index": 0,
                "score": 2.3923509e-05,
                "text": "vectors can be defined or extracted As such the general attention model presented in Fig 2 is applicable to a wide range of domains The problem however is that there is a large variety of different applications and extensions of the general attention module As such in Section 3 a comprehensive overview is provided of a collection of different attention mechanisms 3 A TTENTION TAXONOMY There are many different types of attention mechanisms and extensions and a model can use different combinations of these attention techniques As such we propose a taxonomy that can be used to classify different types of attention mechanisms Fig 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in The three major categories are\n"
            },
            {
                "rank": 8,
                "index": 3,
                "score": 2.2079641e-06,
                "text": "H et al Few shot parameter efficient fine tuning is better and cheaper than in context learning Advances in Neural Information Processing Systems 35 19501965 2022 54 Shlyueva D Stampfel G Stark A Transcriptional enhancers from properties to genome wide predictions Nature Reviews Genetics 15 272286 2014 55 Zeitlinger J Seven myths of how transcription factors read the cis regulatory code Current Opinion in Systems Biology 23 2231 2020 56 Agarwal V et al Massively parallel characterization of transcriptional regulatory elements in three diverse human cell types bioRxiv 2023 57 Chen K M Wong A K Troyanskaya O G Zhou J A sequence based global map of regulatory activity for deciphering human genetics Nature genetics 54 940949 2022 58 Koo P K Majdandzic A Ploenzke M Anand P\n"
            },
            {
                "rank": 9,
                "index": 2,
                "score": 1.7195649e-06,
                "text": "19 Pinglay S etal Synthetic regulatory reconstitution reveals principles of mammalian Hox cluster regulation Science 377 eabk2820 2022 20 Brosh R etal Synthetic regulatory genomics uncovers enhancer context dependence at the Sox2 locus Mol Cell 83 11401152 e1147 2023 21 Agmon N etal Yeast golden gate yGG for the efficient assembly of S cerevisiae transcription units ACS Synth Biol 4 853859 2015 22 Szybalska E H Szybalski W Genetics of human cell line IV DNA mediated heritable transformation of a biochemical trait Proc Natl Acad Sci USA 48 20262034 1962 23 Skene P J Henikoff S An efficient targeted nuclease strategy for high resolution mapping of DNA binding sites eLife 6 e21856 2017 24 Murata M etal Detecting expressed genes using CAGE Methods Mol Biol 1164 6785 2014\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -1.4264893531799316,
                "text": "both at the level of the source and at the level of the target F or instance in synaptic gating can the attending output of a neuron gate more than on e synapse Can the attending output of several neurons gate the same synapse A nd so forth In the most simple cases we will assume that the multiplicity is on e both at the source and at the target but greater multiplicities will also be consi dered for instance in some of the theorems in Sections 6and7 5 Time Scales Finally for simplicity and in line with current deep learn ing attention models we assume that the attention mechanisms operat e on the time scale of individual inputs Different inputs create different attent ion signals Alternative possibilities\n"
            },
            {
                "rank": 1,
                "score": -1.500286340713501,
                "text": "gating operation s to be combined into one macro operation which would require a network of depth 10 for its implementation inside the SM 5 Cardinal Capacity Review We have seen that attention mechanisms enable important fun ctionalities with minimal depth compared to the equivalent SM circuits at the cost of a dding attention neurons and mechanisms Here we want to better understand the trade offs be tween the computations that are enabled and the corresponding costs The key concep t for doing so is the concept of cardinal capacity which we briefly review below THE QUARKS OF ATTENTION 17 Figure 9 Standard model neural network for normalizing a vector u1 u2 u3 for clarity only the first normalized component is fully shown Figure 10 Standard model neural\n"
            },
            {
                "rank": 2,
                "score": -2.3472557067871094,
                "text": "interpretability of attention models is that researchers must be extremely careful when drawing conclusions based on attention patterns For example problems with an attention model can be diagnosed via the attention weights if the model is found to focus on the incorrect parts of the data if such information is available Yet conversely attention weights may only be used to obtain plausible explanations for why certain parts of the data are focused on rather than concluding that those 17 parts are significant to the problem However one should still be cautious as the viability of such approaches can depend on the model architecture 5 C ONCLUSION In this survey we have provided an overview of recent research on attention models in deep learning Attention mechanisms have been a\n"
            },
            {
                "rank": 3,
                "score": -2.3657803535461426,
                "text": "The copyright holder for this preprint which this version posted May 1 2023 https doi org 10 1101 2023 04 30 538439doi bioRxiv preprint steps in Supplementary Figure S1B For example in one iteration i 1 2 K the attention 504 masking mechanism allows attention with all predicted genes from previous 0 to i1 iterations In 505 each iteration scGPT selects the top 1 Kgenes from the unknown set with the highest prediction 506 confidence to be included as known genes in the next iteration i 1 Intuitively this workflow 507 streamlines the generation of large groups of gene expressions in an auto regressive manner where 508 gene expressions with highest prediction confidence are first generated and used to help subsequent 509 rounds of generation The gene prompt\n"
            },
            {
                "rank": 4,
                "score": -3.3864293098449707,
                "text": "synapse they would require new complex mechanisms in a physical implementation Furthermore they do not occur in current attention based deep learning models The same can be said for the activation being the direct source of the attending signal Even moreunlikely would bethecase of mixed schemes where the attending signal would emanate for instance from both neuronal outputs and synapses In short the reasonable assumption that the atte nding signals emanate from neuronal outputs allows us to reduce the number of possi bilities by a factor of three leaving 6 basic possibilities Table 1 2 Target For the target of an attention signal we will study all three possibilities Thus attention signals can target activations S outputs O or synapses w We will call these three forms of\n"
            },
            {
                "rank": 5,
                "score": -3.6564018726348877,
                "text": "vectors can be defined or extracted As such the general attention model presented in Fig 2 is applicable to a wide range of domains The problem however is that there is a large variety of different applications and extensions of the general attention module As such in Section 3 a comprehensive overview is provided of a collection of different attention mechanisms 3 A TTENTION TAXONOMY There are many different types of attention mechanisms and extensions and a model can use different combinations of these attention techniques As such we propose a taxonomy that can be used to classify different types of attention mechanisms Fig 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in The three major categories are\n"
            },
            {
                "rank": 6,
                "score": -4.073418617248535,
                "text": "activations in particu lar gating by polynomial threshold units will be studied in Sections 6and7 2 7 Summary In summary the quarks of attention can be classified based on the origin the target and the interaction mechanism of the attention s ignal Assuming that the origin is in the output of one neuron or a group of neurons and that t he interactions are either additive or multiplicative this leads to six classes Tabl e1 Within the additive group two classes are already in the SM additive activation and addit ive output attention and only one class is of interest here for further studies additive a ctivation attention or multiplexing Within the multiplicative group all three classes corresp ond to true extensions of the SM and at least\n"
            },
            {
                "rank": 7,
                "score": -5.20831298828125,
                "text": "the organization of the feature vectors allows it to be applied to various problems that each use data with different structures as illustrated by the previous domain examples Yet this can be taken even further by applying attention to data where there is irregular structure For example protein structures city traffic flows and communication networks cannot always be represented using neatly structured organizations such as sequences like time series or grids like images In such cases the different aspects of the data are often represented as nodes in a graph These nodes can be represented by feature vectors meaning that attention can be applied in domains that use graph structured data as well In general attention can be applied to any problem for which a set of feature\n"
            },
            {
                "rank": 8,
                "score": -5.2849836349487305,
                "text": "H et al Few shot parameter efficient fine tuning is better and cheaper than in context learning Advances in Neural Information Processing Systems 35 19501965 2022 54 Shlyueva D Stampfel G Stark A Transcriptional enhancers from properties to genome wide predictions Nature Reviews Genetics 15 272286 2014 55 Zeitlinger J Seven myths of how transcription factors read the cis regulatory code Current Opinion in Systems Biology 23 2231 2020 56 Agarwal V et al Massively parallel characterization of transcriptional regulatory elements in three diverse human cell types bioRxiv 2023 57 Chen K M Wong A K Troyanskaya O G Zhou J A sequence based global map of regulatory activity for deciphering human genetics Nature genetics 54 940949 2022 58 Koo P K Majdandzic A Ploenzke M Anand P\n"
            },
            {
                "rank": 9,
                "score": -5.604269027709961,
                "text": "19 Pinglay S etal Synthetic regulatory reconstitution reveals principles of mammalian Hox cluster regulation Science 377 eabk2820 2022 20 Brosh R etal Synthetic regulatory genomics uncovers enhancer context dependence at the Sox2 locus Mol Cell 83 11401152 e1147 2023 21 Agmon N etal Yeast golden gate yGG for the efficient assembly of S cerevisiae transcription units ACS Synth Biol 4 853859 2015 22 Szybalska E H Szybalski W Genetics of human cell line IV DNA mediated heritable transformation of a biochemical trait Proc Natl Acad Sci USA 48 20262034 1962 23 Skene P J Henikoff S An efficient targeted nuclease strategy for high resolution mapping of DNA binding sites eLife 6 e21856 2017 24 Murata M etal Detecting expressed genes using CAGE Methods Mol Biol 1164 6785 2014\n"
            }
        ]
    },
    {
        "query": "What is the current market share of transformer-based models in the healthcare industry?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 8,
                "score": 0.0030994155,
                "text": "Digit Med 4 93 2021 91 Sezgin E Sirrianni J Linwood S L etal Operationalizing and implementing pretrained large artificial intelligence linguistic models in the us health care system outlook of generative pretrained transformer 3 GPT 3 as a service model JMIR Med Informatics 10 e32875 2022 92 Agrawal M Hegselmann S Lang H Kim Y Sontag D Large language models are zero shot clinical information extractors Preprint at https doi org 10 48550 arXiv 2205 12689 2022 93 Livin V Hother C E Winther O Can large language models reason about medical questions Preprint at https doi org 10 48550 arXiv 2207 08143 2022 94 Ouyang L etal Training language models to follow instructions with human feedback Preprint at https doi org 10 48550 arXiv 2203 02155\n"
            },
            {
                "rank": 1,
                "index": 3,
                "score": 0.0019877742,
                "text": "Rouault and J Stephan 2023 On the Impossible Safety of Large AI Models ArXiv 2209 15259 cs N Elhage N Nanda C Olsson T Henighan N Joseph B Mann A Askell Y Bai et al 2021 A mathematical framework for transformer circuits Transformer Circuits Thread A Elnaggar M Heinzinger C Dallago G Rihawi Y Wang L Jones T Gibbs T Feher et al 2020 Prottrans towards cracking the language of lifes code through selfsupervised deep learning and high performance computing arXiv preprint arXiv 2007 06225 T Eloundou S Manning P Mishkin and D Rock 2023 Gpts are gpts An early look at the labor market impact potential of large language models F Faal K Schmitt and J Y Yu 2023 Reward modeling for mitigating toxicity in transformer based\n"
            },
            {
                "rank": 2,
                "index": 1,
                "score": 0.00042720788,
                "text": "compatible with open sourcing while most existing models rely on data which is either not publicly available or undocumented e g Books 2TB or Social media conversations There exist some exceptions notably OPT Zhang et al 2022 GPT NeoX Black et al 2022 BLOOM Scao et al 2022 and GLM Zeng et al 2022 but none that are competitive with PaLM 62B or Chinchilla In the rest of this paper we present an overview of the modifications we made to the transformer architecture Vaswani et al 2017 as well as our training method We then report the performance of our models and compare with others LLMs on a set of standard benchmarks Finally we expose some of the biases and toxicity encoded in our models using some of\n"
            },
            {
                "rank": 3,
                "index": 0,
                "score": 0.00025915826,
                "text": "against over reliance on the output of a medical assistant For example the potential harms of using an LLM for diagnosing or treating an illness are much greater than those from using an LLM for information about a disease or medication Additional research will be needed to assess LLMs used in healthcare for homogenization and amplification of biases and security vulnerabilities inherited from base models5 38 50 Conclusion The advent of foundation models and LLMs presents a compelling opportunity to rethink the development of medical AI and make it easier safer and more equitable to use At the same time medicine is an especially complex domain for applications of LLMs Our research provides a glimpse into the opportunities and the challenges of applying these technologies to medicine We\n"
            },
            {
                "rank": 4,
                "index": 7,
                "score": 6.7092245e-05,
                "text": "Transformers and in particular in context learning within by reverse engineering We hope our work motivates further studies trying to describe the emergence of single multiple or mixture of expert models mesa optimized in simple trained Transformers Bai et al 2023 which we hypothesize could illicit inference reminiscent to world models Ha Schmidhuber 2018 Werbos 1987 Furthermore the insights we gained in our controlled setting could motivate studying limitations and particularities of in context learning Min et al 2022 Kossen et al 2023 and its powerful variants such as chain of thought prompting Wei et al 2022 Li et al 2023b Giannou et al 2023 as well as the fascinating interplay between in weights and in context learning Chan et al 2022b ACKNOWLEDGMENTS Joo Sacramento and Johannes von\n"
            },
            {
                "rank": 5,
                "index": 9,
                "score": 6.4522144e-05,
                "text": "biomedical domain language model Preprint at https doi org 10 48550 arXiv 2010 06060 2020 86 Lee J etal Biobert a pre trained biomedical language representation model for biomedical text mining Bioinformatics 36 12341240 2020 87 Gu Y etal Domain specific language model pretraining for biomedical natural language processing ACM Trans Comput Healthc 3 2 2021 88 Papanikolaou Y Pierleoni A DARE data augmented relation extraction with GPT 2 Preprint at https doi org 10 48550 arXiv 2004 13845 2020 89 Hong Z etal The diminishing returns of masked language models to science Preprint at https doi org 10 48550 arXiv 2205 11342 2023 90 Korngiebel D M Mooney S D Considering the possibilities and pitfalls of generative pre trained transformer 3 GPT 3 in healthcare delivery NPJ\n"
            },
            {
                "rank": 6,
                "index": 6,
                "score": 9.972941e-06,
                "text": "their robustness and mitigate known issues such as toxicity and bias Additionally we observed like Chung et al 2022 that finetuning these models on instructions lead to promising results and we plan to further investigate this in future work Finally we plan to release larger models trained on larger pretraining corpora in the future since we have seen a constant improvement in performance as we were scaling Acknowledgements We thank Daniel Haziza Francisco Massa Jeremy Reizenstein Artem Korenev and Patrick Labatut from the xformers team We thank Susan Zhang and Stephen Roller for their support on data deduplication We thank Luca Wehrstedt Vegard Mella and Pierre Emmanuel Mazar for their support on training stability We thank Shubho Sengupta Kalyan Saladi and all the AI infra team for their\n"
            },
            {
                "rank": 7,
                "index": 2,
                "score": 3.966986e-06,
                "text": "0in this case yields a 1 25X speed improvement which is surprisingly high for this trivial approximation model but is still lower than the speedup we get from using T5 small as the approximation model 5 Related work The efficiency of inference from large models was studied extensively Dehghani et al 2021 Many approaches aim to speed up inference from large models in general and autoregressive models like Transformers in particular Numerous techniques try to make inference more efficient for all tokens e g distillation Hinton et al 2015 sparcification Jaszczur et al 2021 quantization Hubara et al 2016 and architecture modification So et al 2021 Shazeer 2019 Closer to our approach are adaptive computation methods which adapt the amount of computation to problem difficulty Han et al\n"
            },
            {
                "rank": 8,
                "index": 4,
                "score": 3.4736106e-06,
                "text": "they know Preprint at https doi org 10 48550 arXiv 2207 05221 2022 24 Tran D etal Plex towards reliability using pretrained large model extensions Preprint at https doi org 10 48550 arXiv 2207 07411 2022 25 Feng S Y Khetan V Sacaleanu B Gershman A Hovy E CHARD clinical health aware reasoning across dimensions for text generation models Preprint at https doi org 10 48550 arXiv 2210 04191 2022 26 Williams T Szekendi M Pavkovic S Clevenger W Cerese J The reliability of ahrq common format harm scales in rating patient safety events J Patient Saf 11 5259 2015 27 Walsh K E etal Measuring harm in healthcare optimizing adverse event review Med Care 55 436 2017 28 Wei J etal Emergent abilities of large language models\n"
            },
            {
                "rank": 9,
                "index": 5,
                "score": 5.4532956e-07,
                "text": "2019 These improvements enabled new applications in domains such as protein structure prediction Ingraham et al 2019 Du et al 2020 and provided benefit to fundamental problems in machine learning such as adversarial robustness calibration out ofdistribution detection Grathwohl et al 2019 Du Mordatch 2019 and semi supervised learning Song Ou 2018 Despite this progress we have little insight into the quality of the models we are learning One solution is to indirectly measure quality of the model by evaluating performance on downstream discriminative tasks as advocated for in Theis et al 2015 and recently applied to EBMs by Grathwohl et al 2019 Another solution is to use metrics that rely on samples generated by expensive MCMC algorithms Nijkamp et al 2019a Song Ermon 2019 Du Mordatch 2019\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -2.5539183616638184,
                "text": "Digit Med 4 93 2021 91 Sezgin E Sirrianni J Linwood S L etal Operationalizing and implementing pretrained large artificial intelligence linguistic models in the us health care system outlook of generative pretrained transformer 3 GPT 3 as a service model JMIR Med Informatics 10 e32875 2022 92 Agrawal M Hegselmann S Lang H Kim Y Sontag D Large language models are zero shot clinical information extractors Preprint at https doi org 10 48550 arXiv 2205 12689 2022 93 Livin V Hother C E Winther O Can large language models reason about medical questions Preprint at https doi org 10 48550 arXiv 2207 08143 2022 94 Ouyang L etal Training language models to follow instructions with human feedback Preprint at https doi org 10 48550 arXiv 2203 02155\n"
            },
            {
                "rank": 1,
                "score": -2.8474202156066895,
                "text": "compatible with open sourcing while most existing models rely on data which is either not publicly available or undocumented e g Books 2TB or Social media conversations There exist some exceptions notably OPT Zhang et al 2022 GPT NeoX Black et al 2022 BLOOM Scao et al 2022 and GLM Zeng et al 2022 but none that are competitive with PaLM 62B or Chinchilla In the rest of this paper we present an overview of the modifications we made to the transformer architecture Vaswani et al 2017 as well as our training method We then report the performance of our models and compare with others LLMs on a set of standard benchmarks Finally we expose some of the biases and toxicity encoded in our models using some of\n"
            },
            {
                "rank": 2,
                "score": -3.4401679039001465,
                "text": "biomedical domain language model Preprint at https doi org 10 48550 arXiv 2010 06060 2020 86 Lee J etal Biobert a pre trained biomedical language representation model for biomedical text mining Bioinformatics 36 12341240 2020 87 Gu Y etal Domain specific language model pretraining for biomedical natural language processing ACM Trans Comput Healthc 3 2 2021 88 Papanikolaou Y Pierleoni A DARE data augmented relation extraction with GPT 2 Preprint at https doi org 10 48550 arXiv 2004 13845 2020 89 Hong Z etal The diminishing returns of masked language models to science Preprint at https doi org 10 48550 arXiv 2205 11342 2023 90 Korngiebel D M Mooney S D Considering the possibilities and pitfalls of generative pre trained transformer 3 GPT 3 in healthcare delivery NPJ\n"
            },
            {
                "rank": 3,
                "score": -3.807556629180908,
                "text": "Rouault and J Stephan 2023 On the Impossible Safety of Large AI Models ArXiv 2209 15259 cs N Elhage N Nanda C Olsson T Henighan N Joseph B Mann A Askell Y Bai et al 2021 A mathematical framework for transformer circuits Transformer Circuits Thread A Elnaggar M Heinzinger C Dallago G Rihawi Y Wang L Jones T Gibbs T Feher et al 2020 Prottrans towards cracking the language of lifes code through selfsupervised deep learning and high performance computing arXiv preprint arXiv 2007 06225 T Eloundou S Manning P Mishkin and D Rock 2023 Gpts are gpts An early look at the labor market impact potential of large language models F Faal K Schmitt and J Y Yu 2023 Reward modeling for mitigating toxicity in transformer based\n"
            },
            {
                "rank": 4,
                "score": -4.831507205963135,
                "text": "against over reliance on the output of a medical assistant For example the potential harms of using an LLM for diagnosing or treating an illness are much greater than those from using an LLM for information about a disease or medication Additional research will be needed to assess LLMs used in healthcare for homogenization and amplification of biases and security vulnerabilities inherited from base models5 38 50 Conclusion The advent of foundation models and LLMs presents a compelling opportunity to rethink the development of medical AI and make it easier safer and more equitable to use At the same time medicine is an especially complex domain for applications of LLMs Our research provides a glimpse into the opportunities and the challenges of applying these technologies to medicine We\n"
            },
            {
                "rank": 5,
                "score": -5.57130241394043,
                "text": "Transformers and in particular in context learning within by reverse engineering We hope our work motivates further studies trying to describe the emergence of single multiple or mixture of expert models mesa optimized in simple trained Transformers Bai et al 2023 which we hypothesize could illicit inference reminiscent to world models Ha Schmidhuber 2018 Werbos 1987 Furthermore the insights we gained in our controlled setting could motivate studying limitations and particularities of in context learning Min et al 2022 Kossen et al 2023 and its powerful variants such as chain of thought prompting Wei et al 2022 Li et al 2023b Giannou et al 2023 as well as the fascinating interplay between in weights and in context learning Chan et al 2022b ACKNOWLEDGMENTS Joo Sacramento and Johannes von\n"
            },
            {
                "rank": 6,
                "score": -5.617260456085205,
                "text": "they know Preprint at https doi org 10 48550 arXiv 2207 05221 2022 24 Tran D etal Plex towards reliability using pretrained large model extensions Preprint at https doi org 10 48550 arXiv 2207 07411 2022 25 Feng S Y Khetan V Sacaleanu B Gershman A Hovy E CHARD clinical health aware reasoning across dimensions for text generation models Preprint at https doi org 10 48550 arXiv 2210 04191 2022 26 Williams T Szekendi M Pavkovic S Clevenger W Cerese J The reliability of ahrq common format harm scales in rating patient safety events J Patient Saf 11 5259 2015 27 Walsh K E etal Measuring harm in healthcare optimizing adverse event review Med Care 55 436 2017 28 Wei J etal Emergent abilities of large language models\n"
            },
            {
                "rank": 7,
                "score": -5.7713212966918945,
                "text": "0in this case yields a 1 25X speed improvement which is surprisingly high for this trivial approximation model but is still lower than the speedup we get from using T5 small as the approximation model 5 Related work The efficiency of inference from large models was studied extensively Dehghani et al 2021 Many approaches aim to speed up inference from large models in general and autoregressive models like Transformers in particular Numerous techniques try to make inference more efficient for all tokens e g distillation Hinton et al 2015 sparcification Jaszczur et al 2021 quantization Hubara et al 2016 and architecture modification So et al 2021 Shazeer 2019 Closer to our approach are adaptive computation methods which adapt the amount of computation to problem difficulty Han et al\n"
            },
            {
                "rank": 8,
                "score": -6.467532634735107,
                "text": "their robustness and mitigate known issues such as toxicity and bias Additionally we observed like Chung et al 2022 that finetuning these models on instructions lead to promising results and we plan to further investigate this in future work Finally we plan to release larger models trained on larger pretraining corpora in the future since we have seen a constant improvement in performance as we were scaling Acknowledgements We thank Daniel Haziza Francisco Massa Jeremy Reizenstein Artem Korenev and Patrick Labatut from the xformers team We thank Susan Zhang and Stephen Roller for their support on data deduplication We thank Luca Wehrstedt Vegard Mella and Pierre Emmanuel Mazar for their support on training stability We thank Shubho Sengupta Kalyan Saladi and all the AI infra team for their\n"
            },
            {
                "rank": 9,
                "score": -7.096652030944824,
                "text": "2019 These improvements enabled new applications in domains such as protein structure prediction Ingraham et al 2019 Du et al 2020 and provided benefit to fundamental problems in machine learning such as adversarial robustness calibration out ofdistribution detection Grathwohl et al 2019 Du Mordatch 2019 and semi supervised learning Song Ou 2018 Despite this progress we have little insight into the quality of the models we are learning One solution is to indirectly measure quality of the model by evaluating performance on downstream discriminative tasks as advocated for in Theis et al 2015 and recently applied to EBMs by Grathwohl et al 2019 Another solution is to use metrics that rely on samples generated by expensive MCMC algorithms Nijkamp et al 2019a Song Ermon 2019 Du Mordatch 2019\n"
            }
        ]
    },
    {
        "query": "How many citations did the original paper on the attention mechanism receive in its first year of publication?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 6,
                "score": 0.0002415663,
                "text": "2008 ISSN 15455963 URL https pubmed ncbi nlm nih gov 18451428 Pengfei Tian John M Louis James L Baber Annie Aniana and Robert B Best Co evolutionary fitness landscapes for sequence design Angewandte Chemie International Edition 57 20 5674 5678 2018 Ashish Vaswani Noam Shazeer Niki Parmar Jakob Uszkoreit Llion Jones Aidan N Gomez ukasz Kaiser and Illia Polosukhin Attention Is All You Need In Advances in Neural Information Processing Systems pp 59986008 2017 URL https papers nips cc paper 7181 attention is all you need pdf Jesse Vig Ali Madani Lav R Varshney Caiming Xiong Richard Socher and Nazneen Fatema Rajani BERTology Meets Biology Interpreting Attention in Protein Language Models bioRxiv pp 2020 06 26 174417 6 2020 doi 10 1101 2020 06 26 174417 URL http\n"
            },
            {
                "rank": 1,
                "index": 0,
                "score": 0.00016865275,
                "text": "Review D 2020 In press Also arXiv 2010 09206 Alex Graves Greg Wayne and Ivo Danihelka Neural Turi ng machines arXiv preprint arXiv 1410 5401 2014 Laurent Itti Geraint Rees and John K Tsotsos Neurobiology of attention Elsevier 2005 Jeff Kahn J anos Koml os and Endre Szemer edi On the p robability that a random 1 matrix is singular Journal of the American Mathematical Society 8 1 223240 1995 Juho Lee Yoonho Lee Jungtaek Kim Adam Kosiorek Seun gjin Choi and Yee Whye Teh Set transformer A framework for attention based permutation inva riant neural networks volume 97 of Proceedings of Machine Learning Research pages 37443753 Long Beach California USA 0915 Jun 20 19 PMLR Hanxiao Liu Zihang Dai David R So and Quoc V Le Pay\n"
            },
            {
                "rank": 2,
                "index": 3,
                "score": 6.352189e-05,
                "text": "trainable parameters in the model is performed by a 72 maximum pseudo likelihood principle following the work discussed in 73 2 CC BY 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The copyright holder for this preprint this version posted February 8 2024 https doi org 10 1101 2024 02 06 579080doi bioRxiv preprint 2 Materials and Methods 74 2 1 Factored Attention 75 As in every Direct Coupling Analysis implementation sequences in a protein family are represented 76 by a Multiple Sequence Alignment and can be thought of as independent samples cf Supplementary 77 Materials at 4 for details from a probability\n"
            },
            {
                "rank": 3,
                "index": 7,
                "score": 6.352189e-05,
                "text": "of batch effect correction methods for single cell 909 RNA sequencing data In Genome biology 21 1 2020 pp 132 910 Ashish Vaswani et al Attention is all you need In Advances in neural information pro911 cessing systems 30 2017 912 26 CC BY NC ND 4 0 International license available under awas not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The copyright holder for this preprint which this version posted May 1 2023 https doi org 10 1101 2023 04 30 538439doi bioRxiv preprint Sinong Wang et al Linformer Self attention with linear complexity In arXiv preprint 913 arXiv 2006 04768 2020 914 F Alexander Wolf Philipp Angerer and Fabian J\n"
            },
            {
                "rank": 4,
                "index": 2,
                "score": 5.2252268e-05,
                "text": "applied One of the main contributions of this paper is the taxonomy of attention techniques presented in Section 3 In this section attention mechanisms are explained and categorized according to the presentedarXiv 2203 14263v1 cs LG 27 Mar 2022 2 Feature Model Query Model Attention Model Output Model Fig 1 An illustration of the general structure of the task model taxonomy Section 4 provides an overview of performance measures and methods for evaluating attention models Furthermore the taxonomy is used to evaluate the structure of various attention models Lastly in Section 5 we give our conclusions and suggestions for further research 2 G ENERAL ATTENTION MODEL This section presents a general form of attention with corresponding notation The notation introduced here is based on the notation that was\n"
            },
            {
                "rank": 5,
                "index": 4,
                "score": 4.6838883e-05,
                "text": "Zhao Exploiting argument information to improve event detection via supervised attention mechanisms in 55th Annual Meeting of the Association for Computational Linguistics ACL 2017 ACL 2017 pp 17891798 C Liu J Mao F Sha and A Yuille Attention correctness in neural image captioning in 31st AAAI Conference on Artificial Intelligence AAAI 2017 AAAI Press 2017 pp 41764182 A Das H Agrawal L Zitnick D Parikh and D Batra Human attention in visual question answering Do humans and deep networks look at the same regions Computer Vision and Image Understanding vol 163 pp 90 100 2017 Y Yu J Choi Y Kim K Yoo S H Lee and G Kim Supervising neural attention models for video captioning by human gaze data in 2017 IEEE Conference on Computer Vision and\n"
            },
            {
                "rank": 6,
                "index": 1,
                "score": 2.1278067e-05,
                "text": "lie within the structure 272 of the protein proving that the attention model is focused on the structural contacts 273 10 CC BY 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The copyright holder for this preprint this version posted February 8 2024 https doi org 10 1101 2024 02 06 579080doi bioRxiv preprint Positive contactNegative contactStructure AttentionStructure A B C Figure 3 Various contact maps for families PF00014 PF00072 PF00763 A Contact map built from the Frobenious Score applied to the interaction tensor by AttentionDCA Blue and red dots represent positive and negative predictions respectively while gray dots reproduce the structure of\n"
            },
            {
                "rank": 7,
                "index": 9,
                "score": 6.5404374e-06,
                "text": "substantial literature on the neurobiology and psychophy sics of attention e g 17 2 23 pointing to a variety of different phenomena and attention sys tems leading some to conclude The wordattention is an inadequate singular term for a mul titude of inter related processes We use a host of adjectives to describe attention for example we say that attention can be divided oriented sustained or focused an d many of these descriptions likely reflect underlying dissociable neural processes Complica ting matters attentional resources can be allocated to either external stimuli or to internal s timuli such as thoughts and memories Furthermore we often confuse the regulation of atte ntion a covert behavior with the regulation of movement an overt behavior when discussing an attentional disorder In spite\n"
            },
            {
                "rank": 8,
                "index": 5,
                "score": 4.255953e-06,
                "text": "Pattern Recognition CVPR 2017 IEEE Computer Society 2017 S Jain and B C Wallace Attention is not explanation in 2019 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies NAACLHLT 2019 ACL 2019 pp 35433556 S Wiegreffe and Y Pinter Attention is not not explanation in 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing EMNLP IJCNLP 2019 ACL 2019 pp 1120 A K Mohankumar P Nema S Narasimhan M M Khapra B V Srinivasan and B Ravindran Towards transparent and explainable attention models in 58th Annual Meeting of the Association for Computational Linguistics ACL 2020 ACL 2020 pp 42064216 K K Thekumparampil C Wang S Oh and L J Li Attentionbased\n"
            },
            {
                "rank": 9,
                "index": 8,
                "score": 4.157365e-06,
                "text": "and C D Manning Effective approaches to attention based neural machine translation in 2015 Conference on Empirical Methods in Natural Language Processing EMNLP 2015 ACL 2015 pp 14121421 Z Yang D Yang C Dyer X He A Smola and E Hovy Hierarchical attention networks for document classification in 2016 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies NAACLHLT 2016 ACL 2016 pp 14801489 Y Wang M Huang X Zhu and L Zhao Attention based LSTM for aspect level sentiment classification in 2016 Conference on Empirical Methods in Natural Language Processing EMNLP 2016 ACL 2016 pp 606615 P Anderson X He C Buehler D Teney M Johnson S Gould and L Zhang Bottom up and top down attention for image captioning and\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -3.193336248397827,
                "text": "applied One of the main contributions of this paper is the taxonomy of attention techniques presented in Section 3 In this section attention mechanisms are explained and categorized according to the presentedarXiv 2203 14263v1 cs LG 27 Mar 2022 2 Feature Model Query Model Attention Model Output Model Fig 1 An illustration of the general structure of the task model taxonomy Section 4 provides an overview of performance measures and methods for evaluating attention models Furthermore the taxonomy is used to evaluate the structure of various attention models Lastly in Section 5 we give our conclusions and suggestions for further research 2 G ENERAL ATTENTION MODEL This section presents a general form of attention with corresponding notation The notation introduced here is based on the notation that was\n"
            },
            {
                "rank": 1,
                "score": -4.125582695007324,
                "text": "Zhao Exploiting argument information to improve event detection via supervised attention mechanisms in 55th Annual Meeting of the Association for Computational Linguistics ACL 2017 ACL 2017 pp 17891798 C Liu J Mao F Sha and A Yuille Attention correctness in neural image captioning in 31st AAAI Conference on Artificial Intelligence AAAI 2017 AAAI Press 2017 pp 41764182 A Das H Agrawal L Zitnick D Parikh and D Batra Human attention in visual question answering Do humans and deep networks look at the same regions Computer Vision and Image Understanding vol 163 pp 90 100 2017 Y Yu J Choi Y Kim K Yoo S H Lee and G Kim Supervising neural attention models for video captioning by human gaze data in 2017 IEEE Conference on Computer Vision and\n"
            },
            {
                "rank": 2,
                "score": -4.488321781158447,
                "text": "of batch effect correction methods for single cell 909 RNA sequencing data In Genome biology 21 1 2020 pp 132 910 Ashish Vaswani et al Attention is all you need In Advances in neural information pro911 cessing systems 30 2017 912 26 CC BY NC ND 4 0 International license available under awas not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The copyright holder for this preprint which this version posted May 1 2023 https doi org 10 1101 2023 04 30 538439doi bioRxiv preprint Sinong Wang et al Linformer Self attention with linear complexity In arXiv preprint 913 arXiv 2006 04768 2020 914 F Alexander Wolf Philipp Angerer and Fabian J\n"
            },
            {
                "rank": 3,
                "score": -4.716729164123535,
                "text": "Review D 2020 In press Also arXiv 2010 09206 Alex Graves Greg Wayne and Ivo Danihelka Neural Turi ng machines arXiv preprint arXiv 1410 5401 2014 Laurent Itti Geraint Rees and John K Tsotsos Neurobiology of attention Elsevier 2005 Jeff Kahn J anos Koml os and Endre Szemer edi On the p robability that a random 1 matrix is singular Journal of the American Mathematical Society 8 1 223240 1995 Juho Lee Yoonho Lee Jungtaek Kim Adam Kosiorek Seun gjin Choi and Yee Whye Teh Set transformer A framework for attention based permutation inva riant neural networks volume 97 of Proceedings of Machine Learning Research pages 37443753 Long Beach California USA 0915 Jun 20 19 PMLR Hanxiao Liu Zihang Dai David R So and Quoc V Le Pay\n"
            },
            {
                "rank": 4,
                "score": -4.877741813659668,
                "text": "Pattern Recognition CVPR 2017 IEEE Computer Society 2017 S Jain and B C Wallace Attention is not explanation in 2019 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies NAACLHLT 2019 ACL 2019 pp 35433556 S Wiegreffe and Y Pinter Attention is not not explanation in 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing EMNLP IJCNLP 2019 ACL 2019 pp 1120 A K Mohankumar P Nema S Narasimhan M M Khapra B V Srinivasan and B Ravindran Towards transparent and explainable attention models in 58th Annual Meeting of the Association for Computational Linguistics ACL 2020 ACL 2020 pp 42064216 K K Thekumparampil C Wang S Oh and L J Li Attentionbased\n"
            },
            {
                "rank": 5,
                "score": -4.944022178649902,
                "text": "2008 ISSN 15455963 URL https pubmed ncbi nlm nih gov 18451428 Pengfei Tian John M Louis James L Baber Annie Aniana and Robert B Best Co evolutionary fitness landscapes for sequence design Angewandte Chemie International Edition 57 20 5674 5678 2018 Ashish Vaswani Noam Shazeer Niki Parmar Jakob Uszkoreit Llion Jones Aidan N Gomez ukasz Kaiser and Illia Polosukhin Attention Is All You Need In Advances in Neural Information Processing Systems pp 59986008 2017 URL https papers nips cc paper 7181 attention is all you need pdf Jesse Vig Ali Madani Lav R Varshney Caiming Xiong Richard Socher and Nazneen Fatema Rajani BERTology Meets Biology Interpreting Attention in Protein Language Models bioRxiv pp 2020 06 26 174417 6 2020 doi 10 1101 2020 06 26 174417 URL http\n"
            },
            {
                "rank": 6,
                "score": -5.243562698364258,
                "text": "substantial literature on the neurobiology and psychophy sics of attention e g 17 2 23 pointing to a variety of different phenomena and attention sys tems leading some to conclude The wordattention is an inadequate singular term for a mul titude of inter related processes We use a host of adjectives to describe attention for example we say that attention can be divided oriented sustained or focused an d many of these descriptions likely reflect underlying dissociable neural processes Complica ting matters attentional resources can be allocated to either external stimuli or to internal s timuli such as thoughts and memories Furthermore we often confuse the regulation of atte ntion a covert behavior with the regulation of movement an overt behavior when discussing an attentional disorder In spite\n"
            },
            {
                "rank": 7,
                "score": -5.348729133605957,
                "text": "lie within the structure 272 of the protein proving that the attention model is focused on the structural contacts 273 10 CC BY 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The copyright holder for this preprint this version posted February 8 2024 https doi org 10 1101 2024 02 06 579080doi bioRxiv preprint Positive contactNegative contactStructure AttentionStructure A B C Figure 3 Various contact maps for families PF00014 PF00072 PF00763 A Contact map built from the Frobenious Score applied to the interaction tensor by AttentionDCA Blue and red dots represent positive and negative predictions respectively while gray dots reproduce the structure of\n"
            },
            {
                "rank": 8,
                "score": -5.742672443389893,
                "text": "and C D Manning Effective approaches to attention based neural machine translation in 2015 Conference on Empirical Methods in Natural Language Processing EMNLP 2015 ACL 2015 pp 14121421 Z Yang D Yang C Dyer X He A Smola and E Hovy Hierarchical attention networks for document classification in 2016 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies NAACLHLT 2016 ACL 2016 pp 14801489 Y Wang M Huang X Zhu and L Zhao Attention based LSTM for aspect level sentiment classification in 2016 Conference on Empirical Methods in Natural Language Processing EMNLP 2016 ACL 2016 pp 606615 P Anderson X He C Buehler D Teney M Johnson S Gould and L Zhang Bottom up and top down attention for image captioning and\n"
            },
            {
                "rank": 9,
                "score": -5.876636505126953,
                "text": "trainable parameters in the model is performed by a 72 maximum pseudo likelihood principle following the work discussed in 73 2 CC BY 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The copyright holder for this preprint this version posted February 8 2024 https doi org 10 1101 2024 02 06 579080doi bioRxiv preprint 2 Materials and Methods 74 2 1 Factored Attention 75 As in every Direct Coupling Analysis implementation sequences in a protein family are represented 76 by a Multiple Sequence Alignment and can be thought of as independent samples cf Supplementary 77 Materials at 4 for details from a probability\n"
            }
        ]
    },
    {
        "query": "Which company has the largest patent portfolio related to biological language models?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 2,
                "score": 0.051177494,
                "text": "2023 Ali Madani Ben Krause Eric R Greene Subu Subramanian Benjamin P Mohr James M Holton Jose Luis Olmos Jr Caiming Xiong Zachary Z Sun Richard Socher et al Large language models generate functional protein sequences across diverse families Nature Biotechnology pp 18 2023 Claire Marks Alissa M Hummer Mark Chin and Charlotte M Deane Humanization of antibodies using a machine learning approach on large scale repertoire data Bioinformatics 37 22 40414047 2021 R Thomas McCoy Shunyu Yao Dan Friedman Matthew Hardy and Thomas L Griffiths Embers of autoregression Understanding large language models through the problem they are trained to solve arXiv preprint arXiv 2309 13638 2023 14 CC BY 4 0 International license available under a which was not certified by peer review is the author funder\n"
            },
            {
                "rank": 1,
                "index": 0,
                "score": 0.020100236,
                "text": "using a language model and deep learning Nature Biotechnology 40 16171623 2022 11 Wu R et al High resolution de novo structure prediction from primary sequence BioRxiv 202207 2022 12 Brandes N Goldman G Wang C H Ye C J Ntranos V Genome wide prediction of disease variant effects with a deep protein language model Nature Genetics 55 15121522 2023 13 Meier J et al Language models enable zero shot prediction of the effects of mutations on protein function Advances in Neural Information Processing Systems 34 2928729303 2021 10 18 CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The\n"
            },
            {
                "rank": 2,
                "index": 3,
                "score": 0.013376603,
                "text": "copyright holder for this preprint this version posted March 4 2024 https doi org 10 1101 2024 02 29 582810doi bioRxiv preprint 14 Madani A et al Large language models generate functional protein sequences across diverse families Nature Biotechnology18 2023 15 Ferruz N Hcker B Controllable protein design with language models Nature Machine Intelligence 4 521532 2022 16 Hie B L et al Efficient evolution of human antibodies from general protein language models Nature Biotechnology 2023 17 Hie B L Yang K K Kim P S Evolutionary velocity with protein language models predicts evolutionary dynamics of diverse proteins Cell Systems 13 274285 2022 18 Zhang Z et al Protein language models learn evolutionary statistics of interacting sequence motifs bioRxiv 202401 2024 19 Consens M E et al To\n"
            },
            {
                "rank": 3,
                "index": 5,
                "score": 0.0062170783,
                "text": "S and Ho cker B 2022 ProtGPT2 is a deep unsupervised language model for protein design Nat Commun 13 4348 32 Madani A Krause B Greene E R Subramanian S Mohr B P Holton J M Olmos J L Jr Xiong C Sun Z Z Socher R et al 2023 Large language models generate functional protein sequences acrossdiverse families Nat Biotechnol 41 10991106 33 Verkuil R Kabeli O Du Y Wicky B I M Milles L F Dauparas J Baker D Ovchinnikov S Sercu T and Rives A 2022 Languagemodels generalize beyond natural proteins Preprint at bioRxiv 34 Alley E C Khimulya G Biswas S AlQuraishi M and Church G M 2019 Unified rational protein engineering with sequence based deeprepresentation learning Nat Methods 16 13151322 35 Shin J\n"
            },
            {
                "rank": 4,
                "index": 6,
                "score": 0.0062170783,
                "text": "S and Ho cker B 2022 ProtGPT2 is a deep unsupervised language model for protein design Nat Commun 13 4348 32 Madani A Krause B Greene E R Subramanian S Mohr B P Holton J M Olmos J L Jr Xiong C Sun Z Z Socher R et al 2023 Large language models generate functional protein sequences acrossdiverse families Nat Biotechnol 41 10991106 33 Verkuil R Kabeli O Du Y Wicky B I M Milles L F Dauparas J Baker D Ovchinnikov S Sercu T and Rives A 2022 Languagemodels generalize beyond natural proteins Preprint at bioRxiv 34 Alley E C Khimulya G Biswas S AlQuraishi M and Church G M 2019 Unified rational protein engineering with sequence based deeprepresentation learning Nat Methods 16 13151322 35 Shin J\n"
            },
            {
                "rank": 5,
                "index": 7,
                "score": 0.0025311112,
                "text": "09 04 283929 Ali Madani Bryan McCann Nikhil Naik Nitish Shirish Keskar Namrata Anand Raphael R Eguchi Po Ssu Huang and Richard Socher ProGen Language Modeling for Protein Generation bioRxiv 3 2020 URL http arxiv org abs 2004 03497 Jesse Vig Ali Madani Lav R Varshney Caiming Xiong Richard Socher and Nazneen Fatema Rajani BERTology Meets Biology Interpreting Attention in Protein Language Models bioRxiv page 2020 06 26 174417 6 2020 doi 10 1101 2020 06 26 174417 URL http arxiv org abs 2006 15222 Kevin K Yang Zachary Wu and Frances H Arnold Machine learning guided directed evolution for protein engineering aug 2019 ISSN 15487105 Vanessa E Gray Ronald J Hause Jens Luebeck Jay Shendure and Douglas M Fowler Quantitative Missense Variant Effect Prediction Using Large Scale\n"
            },
            {
                "rank": 6,
                "index": 4,
                "score": 0.0008459375,
                "text": "Sun Richard Socher James S Fraser and Nikhil Naik Deep neural language modeling enables functional protein generation across families bioRxiv 2021 doi 10 1101 2021 07 18 452833 Noelia Ferruz Steffen Schmidt and Birte H ocker ProtGPT2 is a deep unsupervised language model for protein design Nat Commun 13 1 4348 July 2022 John Ingraham Max Baranov Zak Costello Vincent Frappier Ahmed Ismail Shan Tie Wujie Wang Vincent Xue Fritz Obermeyer Andrew Beam and Gevorg 10 CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The copyright holder for this preprint this version posted December 22 2022 https doi org\n"
            },
            {
                "rank": 7,
                "index": 9,
                "score": 0.00024536948,
                "text": "1037 a0018281 Bender E M Gebru T McMillan Major A Shmitchell S 2021 On the dangers of stochastic parrots Can language models be too big In Proceedings of the 2021 acm conference on fairness accountability and transparency pp 610623 Biernaskie J M Walker S C Gegear R J 2009 Bumblebees learn to forage like bayesians The American Naturalist 174 3 413423 Biggio L Bendinelli T Neitz A Lucchi A Parascandolo G 2021 Neural symbolic regression that scales In International conference on machine learning pp 936945 Bingham E Chen J P Jankowiak M Obermeyer F Pradhan N Karaletsos T Goodman N D 2019 Pyro Deep universal probabilistic programming J Mach Learn Res 20 28 128 6 Retrieved from http jmlr org papers v20 18 403 html Blank I A\n"
            },
            {
                "rank": 8,
                "index": 1,
                "score": 0.00018093576,
                "text": "Rao D Wong E Callison Burch C 2023 Faithful chain of thought reasoning arXiv preprint arXiv 2301 13379 MacSweeney M Woll B Campbell R McGuire P K David A S Williams S C R Brammer M J 2002 July Neural systems underlying British Sign Language and audio visual English processing in native users Brain 125 7 15831593 Retrieved 2021 01 05 from https doi org 10 1093 brain awf153 doi 10 1093 brain awf153 Mahowald K Ivanova A A Blank I A Kanwisher N Tenenbaum J B Fedorenko E 2023 Dissociating language and thought in large language models a cognitive perspective arXiv preprint arXiv 2301 06627 Mansinghka V Selsam D Perov Y 2014 Venture a higher order probabilistic programming platform 62 REFERENCES with programmable inference arXiv preprint arXiv\n"
            },
            {
                "rank": 9,
                "index": 8,
                "score": 6.2050494e-05,
                "text": "Code as policies Language model programs for embodied control P P Liang C Wu L P Morency and R Salakhutdinov 2021 Towards understanding and mitigating social biases in language models In International Conference on Machine Learning pages 65656576 PMLR P Liang R Bommasani T Lee D Tsipras D Soylu M Yasunaga Y Zhang D Narayanan et al 2022 Holistic evaluation of language models arXiv preprint arXiv 2211 09110 O Lieber O Sharir B Lenz and Y Shoham 2021 Jurassic 1 Technical details and evaluation White Paper AI21 Labs 1 V Livin C E Hother and O Winther 2022 Can large language models reason about medical questions arXiv preprint arXiv 2207 08143 C C Lin A Jaech X Li M R Gormley and J Eisner 2020 Limitations of autoregressive\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -2.552826166152954,
                "text": "copyright holder for this preprint this version posted March 4 2024 https doi org 10 1101 2024 02 29 582810doi bioRxiv preprint 14 Madani A et al Large language models generate functional protein sequences across diverse families Nature Biotechnology18 2023 15 Ferruz N Hcker B Controllable protein design with language models Nature Machine Intelligence 4 521532 2022 16 Hie B L et al Efficient evolution of human antibodies from general protein language models Nature Biotechnology 2023 17 Hie B L Yang K K Kim P S Evolutionary velocity with protein language models predicts evolutionary dynamics of diverse proteins Cell Systems 13 274285 2022 18 Zhang Z et al Protein language models learn evolutionary statistics of interacting sequence motifs bioRxiv 202401 2024 19 Consens M E et al To\n"
            },
            {
                "rank": 1,
                "score": -3.1063730716705322,
                "text": "using a language model and deep learning Nature Biotechnology 40 16171623 2022 11 Wu R et al High resolution de novo structure prediction from primary sequence BioRxiv 202207 2022 12 Brandes N Goldman G Wang C H Ye C J Ntranos V Genome wide prediction of disease variant effects with a deep protein language model Nature Genetics 55 15121522 2023 13 Meier J et al Language models enable zero shot prediction of the effects of mutations on protein function Advances in Neural Information Processing Systems 34 2928729303 2021 10 18 CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The\n"
            },
            {
                "rank": 2,
                "score": -3.520930290222168,
                "text": "S and Ho cker B 2022 ProtGPT2 is a deep unsupervised language model for protein design Nat Commun 13 4348 32 Madani A Krause B Greene E R Subramanian S Mohr B P Holton J M Olmos J L Jr Xiong C Sun Z Z Socher R et al 2023 Large language models generate functional protein sequences acrossdiverse families Nat Biotechnol 41 10991106 33 Verkuil R Kabeli O Du Y Wicky B I M Milles L F Dauparas J Baker D Ovchinnikov S Sercu T and Rives A 2022 Languagemodels generalize beyond natural proteins Preprint at bioRxiv 34 Alley E C Khimulya G Biswas S AlQuraishi M and Church G M 2019 Unified rational protein engineering with sequence based deeprepresentation learning Nat Methods 16 13151322 35 Shin J\n"
            },
            {
                "rank": 3,
                "score": -3.520930290222168,
                "text": "S and Ho cker B 2022 ProtGPT2 is a deep unsupervised language model for protein design Nat Commun 13 4348 32 Madani A Krause B Greene E R Subramanian S Mohr B P Holton J M Olmos J L Jr Xiong C Sun Z Z Socher R et al 2023 Large language models generate functional protein sequences acrossdiverse families Nat Biotechnol 41 10991106 33 Verkuil R Kabeli O Du Y Wicky B I M Milles L F Dauparas J Baker D Ovchinnikov S Sercu T and Rives A 2022 Languagemodels generalize beyond natural proteins Preprint at bioRxiv 34 Alley E C Khimulya G Biswas S AlQuraishi M and Church G M 2019 Unified rational protein engineering with sequence based deeprepresentation learning Nat Methods 16 13151322 35 Shin J\n"
            },
            {
                "rank": 4,
                "score": -3.8327064514160156,
                "text": "09 04 283929 Ali Madani Bryan McCann Nikhil Naik Nitish Shirish Keskar Namrata Anand Raphael R Eguchi Po Ssu Huang and Richard Socher ProGen Language Modeling for Protein Generation bioRxiv 3 2020 URL http arxiv org abs 2004 03497 Jesse Vig Ali Madani Lav R Varshney Caiming Xiong Richard Socher and Nazneen Fatema Rajani BERTology Meets Biology Interpreting Attention in Protein Language Models bioRxiv page 2020 06 26 174417 6 2020 doi 10 1101 2020 06 26 174417 URL http arxiv org abs 2006 15222 Kevin K Yang Zachary Wu and Frances H Arnold Machine learning guided directed evolution for protein engineering aug 2019 ISSN 15487105 Vanessa E Gray Ronald J Hause Jens Luebeck Jay Shendure and Douglas M Fowler Quantitative Missense Variant Effect Prediction Using Large Scale\n"
            },
            {
                "rank": 5,
                "score": -4.462986946105957,
                "text": "2023 Ali Madani Ben Krause Eric R Greene Subu Subramanian Benjamin P Mohr James M Holton Jose Luis Olmos Jr Caiming Xiong Zachary Z Sun Richard Socher et al Large language models generate functional protein sequences across diverse families Nature Biotechnology pp 18 2023 Claire Marks Alissa M Hummer Mark Chin and Charlotte M Deane Humanization of antibodies using a machine learning approach on large scale repertoire data Bioinformatics 37 22 40414047 2021 R Thomas McCoy Shunyu Yao Dan Friedman Matthew Hardy and Thomas L Griffiths Embers of autoregression Understanding large language models through the problem they are trained to solve arXiv preprint arXiv 2309 13638 2023 14 CC BY 4 0 International license available under a which was not certified by peer review is the author funder\n"
            },
            {
                "rank": 6,
                "score": -5.510366439819336,
                "text": "Code as policies Language model programs for embodied control P P Liang C Wu L P Morency and R Salakhutdinov 2021 Towards understanding and mitigating social biases in language models In International Conference on Machine Learning pages 65656576 PMLR P Liang R Bommasani T Lee D Tsipras D Soylu M Yasunaga Y Zhang D Narayanan et al 2022 Holistic evaluation of language models arXiv preprint arXiv 2211 09110 O Lieber O Sharir B Lenz and Y Shoham 2021 Jurassic 1 Technical details and evaluation White Paper AI21 Labs 1 V Livin C E Hother and O Winther 2022 Can large language models reason about medical questions arXiv preprint arXiv 2207 08143 C C Lin A Jaech X Li M R Gormley and J Eisner 2020 Limitations of autoregressive\n"
            },
            {
                "rank": 7,
                "score": -5.7004499435424805,
                "text": "1037 a0018281 Bender E M Gebru T McMillan Major A Shmitchell S 2021 On the dangers of stochastic parrots Can language models be too big In Proceedings of the 2021 acm conference on fairness accountability and transparency pp 610623 Biernaskie J M Walker S C Gegear R J 2009 Bumblebees learn to forage like bayesians The American Naturalist 174 3 413423 Biggio L Bendinelli T Neitz A Lucchi A Parascandolo G 2021 Neural symbolic regression that scales In International conference on machine learning pp 936945 Bingham E Chen J P Jankowiak M Obermeyer F Pradhan N Karaletsos T Goodman N D 2019 Pyro Deep universal probabilistic programming J Mach Learn Res 20 28 128 6 Retrieved from http jmlr org papers v20 18 403 html Blank I A\n"
            },
            {
                "rank": 8,
                "score": -5.705280780792236,
                "text": "Sun Richard Socher James S Fraser and Nikhil Naik Deep neural language modeling enables functional protein generation across families bioRxiv 2021 doi 10 1101 2021 07 18 452833 Noelia Ferruz Steffen Schmidt and Birte H ocker ProtGPT2 is a deep unsupervised language model for protein design Nat Commun 13 1 4348 July 2022 John Ingraham Max Baranov Zak Costello Vincent Frappier Ahmed Ismail Shan Tie Wujie Wang Vincent Xue Fritz Obermeyer Andrew Beam and Gevorg 10 CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The copyright holder for this preprint this version posted December 22 2022 https doi org\n"
            },
            {
                "rank": 9,
                "score": -7.11128044128418,
                "text": "Rao D Wong E Callison Burch C 2023 Faithful chain of thought reasoning arXiv preprint arXiv 2301 13379 MacSweeney M Woll B Campbell R McGuire P K David A S Williams S C R Brammer M J 2002 July Neural systems underlying British Sign Language and audio visual English processing in native users Brain 125 7 15831593 Retrieved 2021 01 05 from https doi org 10 1093 brain awf153 doi 10 1093 brain awf153 Mahowald K Ivanova A A Blank I A Kanwisher N Tenenbaum J B Fedorenko E 2023 Dissociating language and thought in large language models a cognitive perspective arXiv preprint arXiv 2301 06627 Mansinghka V Selsam D Perov Y 2014 Venture a higher order probabilistic programming platform 62 REFERENCES with programmable inference arXiv preprint arXiv\n"
            }
        ]
    },
    {
        "query": "Discuss the evolution of information retrieval systems, starting from traditional keyword-based methods to the incorporation of advanced techniques like word embeddings and transformer-based models. How have these advancements improved the accuracy and efficiency of search engines, and what are some of the remaining challenges in the field? Additionally, explore the potential impact of large language models on the future of information retrieval and how they might be integrated with existing systems to provide more contextually relevant results.",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 8,
                "score": 0.11777492,
                "text": "model fine tuning Furthermore our method also benefits state of the art dense retrievers in terms of both in domain and out of domain results 1 Introduction Information retrieval IR aims to locate relevant documents from a large corpus given a user issued query It is a core component in modern search engines and researchers have invested for decades in this field There are two mainstream paradigms for IR lexical based sparse retrieval such as BM25 and embedding based dense retrieval Xiong et al 2021 Qu et al 2021 Although dense retrievers perform better when large amounts of labeled data are available Karpukhin et al 2020 BM25 remains competitive on out ofdomain datasets Thakur et al 2021 Query expansion Rocchio 1971 Lavrenko and Croft 2001 is a long\n"
            },
            {
                "rank": 1,
                "index": 3,
                "score": 0.08694166,
                "text": "information retrieval benchmark Thakur et al 2021 In contrast to generating domain specific queries for domain adaptation our work aims to distill more general purpose knowledge of LLMs into a text embedding model resulting in a versatile text embedding model that achieves strong performance on MTEB Muennighoff et al 2023 Retrieval with Instructions Previously Dai et al 2022 demonstrated that there exist different intents for different retrieval tasks For instance given a search query users might want to find a similar query or they might want to read a passage that directly answers the query Recent work has explored implementing a retriever that changes the retrieval behavior for different intents Asai et al 2022 and Su et al 2022 introduce retrieval with instructions where a dense retriever is\n"
            },
            {
                "rank": 2,
                "index": 0,
                "score": 0.086477645,
                "text": "replaced LSTMs by transformer networks and scaled the memory to billions of tokens leading to strong performance improvements More recently RETRO Borgeaud et al 2021 extended these by scaling the retrieval memory to trillions of tokens and changing the model architecture to take retrieved documents as input 3 1 4 Retrieval Augmentation with Search Engines Recently different works have proposed to train large language models to interact with a search engine by generating text queries and using the retrieved documents as additional context Nakano et al 2021 Thoppilan et al 2022 Shuster et al 2022 In the context of few shot question answering Lazaridou et al 2022 used the question to perform a search query and retrieved documents are added to the prompt of a large language model\n"
            },
            {
                "rank": 3,
                "index": 5,
                "score": 0.05350215,
                "text": "Exciting novel applications such as conversational agents 185 203 have also emerged as well as game playing agents with human level performance 147 180 Work has now begun in the information retrieval IR community to apply these neural methods leading to the possibility of advancing the state of the art or even achieving breakthrough performance as in these other fields Retrieval of information can take many forms Users can express their information need in the form of a text queryby typing on a keyboard by selecting a query suggestion or by voice recognitionor the query can be in the form of an image or in some cases the need can even be implicit Retrieval can involve ranking existing pieces of content such as documents or short text answers\n"
            },
            {
                "rank": 4,
                "index": 9,
                "score": 0.04115289,
                "text": "Peters M E Neumann M IV R L L Schwartz R Joshi V Singh S and Smith N A Knowledge enhanced contextual word representations 2019 Petroni F Rockt aschel T Lewis P Bakhtin A Wu Y Miller A H and Riedel S Language models as knowledge bases arXiv preprint arXiv 1909 01066 2019 Radford A Narasimhan K Salimans T and Sutskever I Improving language understanding with unsupervised learning Technical report OpenAI 2018 Radford A Wu J Child R Luan D Amodei D and Sutskever I Language models are unsupervised multitask learners OpenAI Blog 2019 Raffel C Shazeer N Roberts A Lee K Narang S Matena M Zhou Y Li W and Liu P J Exploring the limits of transfer learning with a unified text to text transformer arXiv\n"
            },
            {
                "rank": 5,
                "index": 6,
                "score": 0.035477504,
                "text": "Mitra et al demonstrate good performances on re ranking tasks may not be indicative how the model would perform if the retrieval involves larger document collections 5 2 Query expansion Instead of comparing the query and the document directly in the embedding space an alternative approach is to use term embeddings to find good expansion candidates from a global vocabulary and then retrieving documents using the expanded query Different functions 51 170 227 have been proposed for estimating the relevance of candidate terms to the queryall of them involves comparing the candidate term individually to every query term using their vector representations and then aggregating the scores For example 51 170 estimate the relevance of candidate term tcas score tc q 1 q tqqcos vtc vtq 48 25\n"
            },
            {
                "rank": 6,
                "index": 1,
                "score": 0.02145073,
                "text": "Search engines typically involve large multi tier architectures and the retrieval process generally consists of multiple stages of pruning the candidate set of documents The IR model at the bottom of this telescoping setup may need to sift through billions of documentswhile the model at the top may only need to re rank between tens of promising documents The retrieval approaches that are suitable at one level of the stack may be highly impractical at a different stepmodels at the bottom need to be fastbut mostly focus on eliminating irrelevant or junk results while models at the top tend to develop more sophisticated notions of relevance and focus on distinguishing between documents that are much closer on the relevance scale So far much of the focus on neural\n"
            },
            {
                "rank": 7,
                "index": 2,
                "score": 0.019531239,
                "text": "well as the models that focus on representation learning We have focused on retrieval of long and short text In the case of long text the model must deal with variable length documents where the relevant sections of a document may be surrounded by irrelevant text For both long and short text but particularly for short IR models should also deal with the query document vocabulary mismatch problem by learning how patterns of query words and different document words can indicate relevance Models should also consider lexical matches when the query contains rare termssuch as a persons name or a product model numbernot seen during training and to avoid retrieving semantically related but irrelevant results An ideal model for information retrieval would be able to infer the meaning\n"
            },
            {
                "rank": 8,
                "index": 4,
                "score": 0.010529712,
                "text": "traditional non neural IR approaches and more recent neural methods tend to perform well on different segments of queries depending on whether they focus on lexical or semantic matching Figure 24 plots a few of these models based on their per query NDCG values on a test set 8 Conclusion We present a tutorial on neural methods for information retrieval For machine learning researchers who may be less familiar with IR tasks we introduced the fundamentals of traditional IR models and metrics For IR researchers we summarized key concepts related to representation learning with shallow or deep neural networks Finally we presented some of the recent neural methods for document ranking and question answer matching 16It is important to emphasize that while Mitra et al and others have\n"
            },
            {
                "rank": 9,
                "index": 7,
                "score": 0.007815889,
                "text": "Black Box Language Models on language modeling or downstream tasks This work opens up new possibilities for integrating retrieval into largescale black box language models and demonstrates even the state of the art large scale LMs could benefit from retrieval However REPLUG lacks interpretability as it is unclear when the model relies on retrieved knowledge or parametric knowledge Future research could focus on developing more interpretable retrieval augmented language models References Borgeaud S Mensch A Hoffmann J Cai T Rutherford E Millican K Driessche G v d Lespiau J B Damoc B Clark A et al Improving language models by retrieving from trillions of tokens arXiv preprint arXiv 2112 04426 2021 Borgeaud S Mensch A Hoffmann J Cai T Rutherford E Millican K Van Den Driessche G B\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": 1.295668125152588,
                "text": "replaced LSTMs by transformer networks and scaled the memory to billions of tokens leading to strong performance improvements More recently RETRO Borgeaud et al 2021 extended these by scaling the retrieval memory to trillions of tokens and changing the model architecture to take retrieved documents as input 3 1 4 Retrieval Augmentation with Search Engines Recently different works have proposed to train large language models to interact with a search engine by generating text queries and using the retrieved documents as additional context Nakano et al 2021 Thoppilan et al 2022 Shuster et al 2022 In the context of few shot question answering Lazaridou et al 2022 used the question to perform a search query and retrieved documents are added to the prompt of a large language model\n"
            },
            {
                "rank": 1,
                "score": 0.8413191437721252,
                "text": "Search engines typically involve large multi tier architectures and the retrieval process generally consists of multiple stages of pruning the candidate set of documents The IR model at the bottom of this telescoping setup may need to sift through billions of documentswhile the model at the top may only need to re rank between tens of promising documents The retrieval approaches that are suitable at one level of the stack may be highly impractical at a different stepmodels at the bottom need to be fastbut mostly focus on eliminating irrelevant or junk results while models at the top tend to develop more sophisticated notions of relevance and focus on distinguishing between documents that are much closer on the relevance scale So far much of the focus on neural\n"
            },
            {
                "rank": 2,
                "score": 0.5414969325065613,
                "text": "model fine tuning Furthermore our method also benefits state of the art dense retrievers in terms of both in domain and out of domain results 1 Introduction Information retrieval IR aims to locate relevant documents from a large corpus given a user issued query It is a core component in modern search engines and researchers have invested for decades in this field There are two mainstream paradigms for IR lexical based sparse retrieval such as BM25 and embedding based dense retrieval Xiong et al 2021 Qu et al 2021 Although dense retrievers perform better when large amounts of labeled data are available Karpukhin et al 2020 BM25 remains competitive on out ofdomain datasets Thakur et al 2021 Query expansion Rocchio 1971 Lavrenko and Croft 2001 is a long\n"
            },
            {
                "rank": 3,
                "score": 0.043089598417282104,
                "text": "information retrieval benchmark Thakur et al 2021 In contrast to generating domain specific queries for domain adaptation our work aims to distill more general purpose knowledge of LLMs into a text embedding model resulting in a versatile text embedding model that achieves strong performance on MTEB Muennighoff et al 2023 Retrieval with Instructions Previously Dai et al 2022 demonstrated that there exist different intents for different retrieval tasks For instance given a search query users might want to find a similar query or they might want to read a passage that directly answers the query Recent work has explored implementing a retriever that changes the retrieval behavior for different intents Asai et al 2022 and Su et al 2022 introduce retrieval with instructions where a dense retriever is\n"
            },
            {
                "rank": 4,
                "score": 0.012901738286018372,
                "text": "Black Box Language Models on language modeling or downstream tasks This work opens up new possibilities for integrating retrieval into largescale black box language models and demonstrates even the state of the art large scale LMs could benefit from retrieval However REPLUG lacks interpretability as it is unclear when the model relies on retrieved knowledge or parametric knowledge Future research could focus on developing more interpretable retrieval augmented language models References Borgeaud S Mensch A Hoffmann J Cai T Rutherford E Millican K Driessche G v d Lespiau J B Damoc B Clark A et al Improving language models by retrieving from trillions of tokens arXiv preprint arXiv 2112 04426 2021 Borgeaud S Mensch A Hoffmann J Cai T Rutherford E Millican K Van Den Driessche G B\n"
            },
            {
                "rank": 5,
                "score": -0.469343900680542,
                "text": "Exciting novel applications such as conversational agents 185 203 have also emerged as well as game playing agents with human level performance 147 180 Work has now begun in the information retrieval IR community to apply these neural methods leading to the possibility of advancing the state of the art or even achieving breakthrough performance as in these other fields Retrieval of information can take many forms Users can express their information need in the form of a text queryby typing on a keyboard by selecting a query suggestion or by voice recognitionor the query can be in the form of an image or in some cases the need can even be implicit Retrieval can involve ranking existing pieces of content such as documents or short text answers\n"
            },
            {
                "rank": 6,
                "score": -0.5149823427200317,
                "text": "traditional non neural IR approaches and more recent neural methods tend to perform well on different segments of queries depending on whether they focus on lexical or semantic matching Figure 24 plots a few of these models based on their per query NDCG values on a test set 8 Conclusion We present a tutorial on neural methods for information retrieval For machine learning researchers who may be less familiar with IR tasks we introduced the fundamentals of traditional IR models and metrics For IR researchers we summarized key concepts related to representation learning with shallow or deep neural networks Finally we presented some of the recent neural methods for document ranking and question answer matching 16It is important to emphasize that while Mitra et al and others have\n"
            },
            {
                "rank": 7,
                "score": -0.5723537802696228,
                "text": "Peters M E Neumann M IV R L L Schwartz R Joshi V Singh S and Smith N A Knowledge enhanced contextual word representations 2019 Petroni F Rockt aschel T Lewis P Bakhtin A Wu Y Miller A H and Riedel S Language models as knowledge bases arXiv preprint arXiv 1909 01066 2019 Radford A Narasimhan K Salimans T and Sutskever I Improving language understanding with unsupervised learning Technical report OpenAI 2018 Radford A Wu J Child R Luan D Amodei D and Sutskever I Language models are unsupervised multitask learners OpenAI Blog 2019 Raffel C Shazeer N Roberts A Lee K Narang S Matena M Zhou Y Li W and Liu P J Exploring the limits of transfer learning with a unified text to text transformer arXiv\n"
            },
            {
                "rank": 8,
                "score": -0.5803196430206299,
                "text": "well as the models that focus on representation learning We have focused on retrieval of long and short text In the case of long text the model must deal with variable length documents where the relevant sections of a document may be surrounded by irrelevant text For both long and short text but particularly for short IR models should also deal with the query document vocabulary mismatch problem by learning how patterns of query words and different document words can indicate relevance Models should also consider lexical matches when the query contains rare termssuch as a persons name or a product model numbernot seen during training and to avoid retrieving semantically related but irrelevant results An ideal model for information retrieval would be able to infer the meaning\n"
            },
            {
                "rank": 9,
                "score": -2.2680459022521973,
                "text": "Mitra et al demonstrate good performances on re ranking tasks may not be indicative how the model would perform if the retrieval involves larger document collections 5 2 Query expansion Instead of comparing the query and the document directly in the embedding space an alternative approach is to use term embeddings to find good expansion candidates from a global vocabulary and then retrieving documents using the expanded query Different functions 51 170 227 have been proposed for estimating the relevance of candidate terms to the queryall of them involves comparing the candidate term individually to every query term using their vector representations and then aggregating the scores For example 51 170 estimate the relevance of candidate term tcas score tc q 1 q tqqcos vtc vtq 48 25\n"
            }
        ]
    },
    {
        "query": "Explain the role of structural biology in understanding protein function and its applications in drug discovery. How have advancements in experimental techniques, such as X-ray crystallography and cryo-electron microscopy, contributed to the growth of structural databases? Furthermore, discuss the potential of integrating structural data with large language models and machine learning approaches to predict protein-ligand interactions and accelerate the identification of novel drug targets. What are some of the challenges and limitations of such integrative approaches, and how might they be addressed in future research?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 4,
                "score": 0.18506189,
                "text": "wealth of data and explore novel solutions may ultimately unveil how cells work INTRODUCTION Structural biology is an attempt to answer the question what are we made of This attempt follows the reductionist approach which aims to identify the most fundamental constituents of matter and study their properties It led us to discovera hierarchy of structures from molecules through atoms all the way down to fundamental particles such as quarks and electrons Cells are the minimal units of life and are made of billionsof distinct molecules Although this answers part of the question of what we are made of it does not answer a key question of cell biologyhow do cellular functions spontaneouslyemerge from the interaction of these billions of molecules Cell biology usually lacks the structural resolution\n"
            },
            {
                "rank": 1,
                "index": 5,
                "score": 0.18506189,
                "text": "wealth of data and explore novel solutions may ultimately unveil how cells work INTRODUCTION Structural biology is an attempt to answer the question what are we made of This attempt follows the reductionist approach which aims to identify the most fundamental constituents of matter and study their properties It led us to discovera hierarchy of structures from molecules through atoms all the way down to fundamental particles such as quarks and electrons Cells are the minimal units of life and are made of billionsof distinct molecules Although this answers part of the question of what we are made of it does not answer a key question of cell biologyhow do cellular functions spontaneouslyemerge from the interaction of these billions of molecules Cell biology usually lacks the structural resolution\n"
            },
            {
                "rank": 2,
                "index": 9,
                "score": 0.0355444,
                "text": "important topics We have so many different types of synapses and the in situ synaptic structures directly link to neuronal physiology The advancement of the cryo ETtechnology is really pushing us and the traditional structural biology Its going to be tremendously powerful for us to understand the basic functions of molecular machineries inneurons both in physiological conditions and in pathophysiological conditions Professor Zihe Rao joined us in the conversation JC Zihe could you tell us about your research interest and could you give us a broad picture of structural biology in the past decades and in the future These are two big questions Zihe Rao Maybe the story goes back to half a century ago The Beijing Insulin Structural Group of the Chinese Academy of Sciences was one\n"
            },
            {
                "rank": 3,
                "index": 3,
                "score": 0.032405518,
                "text": "K L Saar M Vendruscolo and T P J Knowles New frontiers for machine learning in protein science J Mol Biol 433 167232 2021 G Masrati M Landau N Ben Tal A Lupas M Kosloff and J Kosinski Integrative structural biology in the era of accurate structure prediction J Mol Biol 433 167127 2021 M Andrec D A Snyder Z Zhou J Young G T Montelione and R M Levy A large data set comparison of protein structures determined by crystallography and nmr Statistical test for structural differences and the effect of crystal packing Proteins 69 449 2007 A A Rashin A H L Rashin and R L Jernigan Protein Flexibility Coordinate Uncertainties and Interpretation of Structural Differences Acta Crystallogr D 65 1140 2009 B Venkatakrishnan M L\n"
            },
            {
                "rank": 4,
                "index": 0,
                "score": 0.0079997545,
                "text": "summaries source data extended data supplementary information acknowledgements peer review information details of author con tributions and competing interests and statements of data and code availability are available at https doi org 10 1038 s41586 021 03819 2 1 Thompson M C Yeates T O Rodriguez J A Advances in methods for atomic resolution macromolecular structure determination F1000Res 9 667 2020 2 Bai X C McMullan G Scheres S H W How cryo EM is revolutionizing structural biology Trends Biochem Sci 40 4957 2015 3 Jaskolski M Dauter Z Wlodawer A A brief history of macromolecular crystallography illustrated by a family tree and its Nobel fruits FEBS J 281 39854009 2014 4 Wthrich K The way to NMR structures of proteins Nat Struct Biol 8 923925 2001 5\n"
            },
            {
                "rank": 5,
                "index": 6,
                "score": 0.0069846497,
                "text": "V Karius K and Kosinski J 2022 Integrative structural modeling of macromolecular complexes using assembline Nat Protoc 17 152176 140 Russel D Lasker K Webb B Vela zquez Muriel J Tjioe E Schneidman Duhovny D Peterson B and Sali A 2012 Putting the pieces together integrative modeling platform software for structure determina tion of macromolecular assemblies PLoS Biol 10 e1001244 141 Rout M P and Sali A 2019 Principles for integrative structural biology studies Cell 177 13841403 142 Senior A W Evans R Jumper J Kirkpatrick J Sifre L Green T Qin C C20Zdek A Nelson A W R Bridgland A et al 2020 Improved protein structure prediction using potentials from deep learning Nature577 706710 143 Tunyasuvunakool K Adler J Wu Z Green T Zielinski M C20Zdek A\n"
            },
            {
                "rank": 6,
                "index": 7,
                "score": 0.0069846497,
                "text": "V Karius K and Kosinski J 2022 Integrative structural modeling of macromolecular complexes using assembline Nat Protoc 17 152176 140 Russel D Lasker K Webb B Vela zquez Muriel J Tjioe E Schneidman Duhovny D Peterson B and Sali A 2012 Putting the pieces together integrative modeling platform software for structure determina tion of macromolecular assemblies PLoS Biol 10 e1001244 141 Rout M P and Sali A 2019 Principles for integrative structural biology studies Cell 177 13841403 142 Senior A W Evans R Jumper J Kirkpatrick J Sifre L Green T Qin C C20Zdek A Nelson A W R Bridgland A et al 2020 Improved protein structure prediction using potentials from deep learning Nature577 706710 143 Tunyasuvunakool K Adler J Wu Z Green T Zielinski M C20Zdek A\n"
            },
            {
                "rank": 7,
                "index": 2,
                "score": 0.005060332,
                "text": "biology Trends Biochem Sci 40 4957 2015 3 Su X D et al Protein crystallography from the perspective of technology developments Crystallogr Rev 21 1 2 2 153 2014 4 Wthrich K Protein structure determination in solution by NMR spectroscopy J Biol Chem 265 22059 22062 1990 5 Slabinski L et al The challenge of protein structure determinationlessons from structural genomics Protein Sci 16 2472 2482 2007 6 Leach A Thomas P Protein structure prediction and homology modeling In Comprehensive Medicinal Chemistry III 1 2 0 144 Elsevier 2017 https doi org 10 1016 b978 0 12 409547 2 12350 9 Table 1 Summary of parameters used for enhanced sampling molecular dynamics simulations of Abl1 inactivation Box dimensions 112 4 124 3 118 0 Total number of atoms\n"
            },
            {
                "rank": 8,
                "index": 1,
                "score": 0.0032476764,
                "text": "based methodssuch as AlphaFold2 AF2 can predict structures for these catalogs of proteins Single particle analysis SPA of isolated components can yieldboth structural insights and the means to find these structures in cryo ET data by template matching structure shown dimeric photosystem I supercomplex PDB 7ZQD Molecular interactions can further be detected within thecellular environment by proximity labeling 13All example data comes from C reinhardtii Sources of the images A left courtesy of GeorgeWitman A right courtesy of Travis Walton and Alan Brown CC BY 4 0 B left courtesy of Ron Kelly Alexander Rigort and Abhay Kotecha C FM courtesy of Martin Jonikas CC BY 4 0 C U ExM courtesy of Paul Guichard and Virginie Hamel C SXT courtesy of Benedikt Westermann CC BY 3 0\n"
            },
            {
                "rank": 9,
                "index": 8,
                "score": 0.0017207355,
                "text": "G Akdel M Andreeva A Bateman A et al 2023 Uncovering New Families and Folds in the Natural Protein Uni verse Nature 622 646653 180 Chen J Fruhauf A Fan C Ponce J Ueberheide B Bhabha G and Ekiert D C 2023 Structure of an endogenous mycobacterial MCE lipidtransporter Nature 620 445452 181 Webb B Viswanath S Bonomi M Pellarin R Greenberg C H Saltzberg D and Sali A 2018 Integrative structure modeling withthe Integrative Modeling Platform Protein Sci 27 245258 182 Rantos V Karius K and Kosinski J 2022 Integrative structural modeling of macromolecular complexes using Assembline Nat Protoc 17 152176 183 Koukos P I and Bonvin A M J J 2020 Integrative Modelling of Biomolecular Complexes J Mol Biol 432 28612881 184 Watanabe R Buschauer R\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": 0.4908064901828766,
                "text": "K L Saar M Vendruscolo and T P J Knowles New frontiers for machine learning in protein science J Mol Biol 433 167232 2021 G Masrati M Landau N Ben Tal A Lupas M Kosloff and J Kosinski Integrative structural biology in the era of accurate structure prediction J Mol Biol 433 167127 2021 M Andrec D A Snyder Z Zhou J Young G T Montelione and R M Levy A large data set comparison of protein structures determined by crystallography and nmr Statistical test for structural differences and the effect of crystal packing Proteins 69 449 2007 A A Rashin A H L Rashin and R L Jernigan Protein Flexibility Coordinate Uncertainties and Interpretation of Structural Differences Acta Crystallogr D 65 1140 2009 B Venkatakrishnan M L\n"
            },
            {
                "rank": 1,
                "score": -0.2943240702152252,
                "text": "summaries source data extended data supplementary information acknowledgements peer review information details of author con tributions and competing interests and statements of data and code availability are available at https doi org 10 1038 s41586 021 03819 2 1 Thompson M C Yeates T O Rodriguez J A Advances in methods for atomic resolution macromolecular structure determination F1000Res 9 667 2020 2 Bai X C McMullan G Scheres S H W How cryo EM is revolutionizing structural biology Trends Biochem Sci 40 4957 2015 3 Jaskolski M Dauter Z Wlodawer A A brief history of macromolecular crystallography illustrated by a family tree and its Nobel fruits FEBS J 281 39854009 2014 4 Wthrich K The way to NMR structures of proteins Nat Struct Biol 8 923925 2001 5\n"
            },
            {
                "rank": 2,
                "score": -0.3645796775817871,
                "text": "important topics We have so many different types of synapses and the in situ synaptic structures directly link to neuronal physiology The advancement of the cryo ETtechnology is really pushing us and the traditional structural biology Its going to be tremendously powerful for us to understand the basic functions of molecular machineries inneurons both in physiological conditions and in pathophysiological conditions Professor Zihe Rao joined us in the conversation JC Zihe could you tell us about your research interest and could you give us a broad picture of structural biology in the past decades and in the future These are two big questions Zihe Rao Maybe the story goes back to half a century ago The Beijing Insulin Structural Group of the Chinese Academy of Sciences was one\n"
            },
            {
                "rank": 3,
                "score": -0.7548656463623047,
                "text": "V Karius K and Kosinski J 2022 Integrative structural modeling of macromolecular complexes using assembline Nat Protoc 17 152176 140 Russel D Lasker K Webb B Vela zquez Muriel J Tjioe E Schneidman Duhovny D Peterson B and Sali A 2012 Putting the pieces together integrative modeling platform software for structure determina tion of macromolecular assemblies PLoS Biol 10 e1001244 141 Rout M P and Sali A 2019 Principles for integrative structural biology studies Cell 177 13841403 142 Senior A W Evans R Jumper J Kirkpatrick J Sifre L Green T Qin C C20Zdek A Nelson A W R Bridgland A et al 2020 Improved protein structure prediction using potentials from deep learning Nature577 706710 143 Tunyasuvunakool K Adler J Wu Z Green T Zielinski M C20Zdek A\n"
            },
            {
                "rank": 4,
                "score": -0.7548656463623047,
                "text": "V Karius K and Kosinski J 2022 Integrative structural modeling of macromolecular complexes using assembline Nat Protoc 17 152176 140 Russel D Lasker K Webb B Vela zquez Muriel J Tjioe E Schneidman Duhovny D Peterson B and Sali A 2012 Putting the pieces together integrative modeling platform software for structure determina tion of macromolecular assemblies PLoS Biol 10 e1001244 141 Rout M P and Sali A 2019 Principles for integrative structural biology studies Cell 177 13841403 142 Senior A W Evans R Jumper J Kirkpatrick J Sifre L Green T Qin C C20Zdek A Nelson A W R Bridgland A et al 2020 Improved protein structure prediction using potentials from deep learning Nature577 706710 143 Tunyasuvunakool K Adler J Wu Z Green T Zielinski M C20Zdek A\n"
            },
            {
                "rank": 5,
                "score": -0.8190971612930298,
                "text": "wealth of data and explore novel solutions may ultimately unveil how cells work INTRODUCTION Structural biology is an attempt to answer the question what are we made of This attempt follows the reductionist approach which aims to identify the most fundamental constituents of matter and study their properties It led us to discovera hierarchy of structures from molecules through atoms all the way down to fundamental particles such as quarks and electrons Cells are the minimal units of life and are made of billionsof distinct molecules Although this answers part of the question of what we are made of it does not answer a key question of cell biologyhow do cellular functions spontaneouslyemerge from the interaction of these billions of molecules Cell biology usually lacks the structural resolution\n"
            },
            {
                "rank": 6,
                "score": -0.8190971612930298,
                "text": "wealth of data and explore novel solutions may ultimately unveil how cells work INTRODUCTION Structural biology is an attempt to answer the question what are we made of This attempt follows the reductionist approach which aims to identify the most fundamental constituents of matter and study their properties It led us to discovera hierarchy of structures from molecules through atoms all the way down to fundamental particles such as quarks and electrons Cells are the minimal units of life and are made of billionsof distinct molecules Although this answers part of the question of what we are made of it does not answer a key question of cell biologyhow do cellular functions spontaneouslyemerge from the interaction of these billions of molecules Cell biology usually lacks the structural resolution\n"
            },
            {
                "rank": 7,
                "score": -1.4744610786437988,
                "text": "based methodssuch as AlphaFold2 AF2 can predict structures for these catalogs of proteins Single particle analysis SPA of isolated components can yieldboth structural insights and the means to find these structures in cryo ET data by template matching structure shown dimeric photosystem I supercomplex PDB 7ZQD Molecular interactions can further be detected within thecellular environment by proximity labeling 13All example data comes from C reinhardtii Sources of the images A left courtesy of GeorgeWitman A right courtesy of Travis Walton and Alan Brown CC BY 4 0 B left courtesy of Ron Kelly Alexander Rigort and Abhay Kotecha C FM courtesy of Martin Jonikas CC BY 4 0 C U ExM courtesy of Paul Guichard and Virginie Hamel C SXT courtesy of Benedikt Westermann CC BY 3 0\n"
            },
            {
                "rank": 8,
                "score": -1.5407747030258179,
                "text": "biology Trends Biochem Sci 40 4957 2015 3 Su X D et al Protein crystallography from the perspective of technology developments Crystallogr Rev 21 1 2 2 153 2014 4 Wthrich K Protein structure determination in solution by NMR spectroscopy J Biol Chem 265 22059 22062 1990 5 Slabinski L et al The challenge of protein structure determinationlessons from structural genomics Protein Sci 16 2472 2482 2007 6 Leach A Thomas P Protein structure prediction and homology modeling In Comprehensive Medicinal Chemistry III 1 2 0 144 Elsevier 2017 https doi org 10 1016 b978 0 12 409547 2 12350 9 Table 1 Summary of parameters used for enhanced sampling molecular dynamics simulations of Abl1 inactivation Box dimensions 112 4 124 3 118 0 Total number of atoms\n"
            },
            {
                "rank": 9,
                "score": -2.216496467590332,
                "text": "G Akdel M Andreeva A Bateman A et al 2023 Uncovering New Families and Folds in the Natural Protein Uni verse Nature 622 646653 180 Chen J Fruhauf A Fan C Ponce J Ueberheide B Bhabha G and Ekiert D C 2023 Structure of an endogenous mycobacterial MCE lipidtransporter Nature 620 445452 181 Webb B Viswanath S Bonomi M Pellarin R Greenberg C H Saltzberg D and Sali A 2018 Integrative structure modeling withthe Integrative Modeling Platform Protein Sci 27 245258 182 Rantos V Karius K and Kosinski J 2022 Integrative structural modeling of macromolecular complexes using Assembline Nat Protoc 17 152176 183 Koukos P I and Bonvin A M J J 2020 Integrative Modelling of Biomolecular Complexes J Mol Biol 432 28612881 184 Watanabe R Buschauer R\n"
            }
        ]
    },
    {
        "query": "Compare and contrast the application of natural language processing techniques in general text corpora and biological sequence data. How do the unique characteristics of biological sequences, such as the limited alphabet and the importance of positional information, influence the design and training of language models in bioinformatics? Additionally, discuss the potential of transfer learning approaches, where models pre-trained on large text corpora are fine-tuned on biological data, and their implications for tasks such as protein function prediction and gene ontology annotation. What are some of the ethical considerations surrounding the use of large language models in the biomedical domain, particularly in terms of data privacy and the potential for biased outputs?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 4,
                "score": 0.38821205,
                "text": "pipeline with a single forward pass of a pre trained end to end protein language model In the last year protein language modeling with an unsupervised training objective has been investigated by multiple groups Rives et al 2019 Alley et al 2019 Heinzinger et al 2019 Rao et al 2019 Madani et al 2020 The longstanding practice in bioinformatics has been to fit linear models on focused sets of evolutionarily related and aligned sequences by contrast protein language modeling trains nonlinear deep neural networks on large databases of evolutionarily diverse and unaligned sequences High capacity protein language models have been shown to learn underlying intrinsic properties of proteins such as structure and function from sequence data Rives et al 2019 A line of work in this emerging field\n"
            },
            {
                "rank": 1,
                "index": 7,
                "score": 0.27884555,
                "text": "sequence modeling arXiv preprint arXiv 1904 01038 N Ousidhoum X Zhao T Fang Y Song and D Y Yeung 2021 Probing toxic content in large pre trained language models In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing Volume 1 Long Papers pages 42624274 C Outeiral and C Deane 2022 Codon language embeddings provide strong signals for protein engineering bioRxiv pages 202212 L Ouyang J Wu X Jiang D Almeida C Wainwright P Mishkin C Zhang S Agarwal et al 2022 Training language models to follow instructions with human feedback InAdvances in Neural Information Processing Systems M Pagliardini D Paliotta M Jaggi and F Fleuret 2023 Faster causal attention over large sequences through sparse\n"
            },
            {
                "rank": 2,
                "index": 0,
                "score": 0.0715912,
                "text": "7 Discussion Advances in language modeling at scale are bringing the goal of a general purpose model for proteins closer to realization This line of work aspires to a model that learns to read and write biology in its native language that can be directly applied across a range of protein understanding and design tasks For scalability learning from sequences is important while there are no central databases of high throughput functional measurements and few compilations exist billions of sequences are available to learn from in sequence databases 49 50 Sequences give an unparalleled view into 9 CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the\n"
            },
            {
                "rank": 3,
                "index": 2,
                "score": 0.05165378,
                "text": "preprint in perpetuity It is made The copyright holder for this preprint this version posted November 17 2021 https doi org 10 1101 2021 07 09 450648doi bioRxiv preprint the vast diversity and complexity of molecular parts invented by nature through billions of years of evolution Unsupervised structure 5153 28 54 55 and function 3 4 learning methods first effectively realized the idea that biological properties could be read directly from sequences without supervision from experimental measurements However these methods are not general purpose in the sense that a specialized model must be trained for every protein for which a prediction is to be made We show that the same performance can be realized by a general purpose model that has been trained across many diverse protein families\n"
            },
            {
                "rank": 4,
                "index": 6,
                "score": 0.03963884,
                "text": "treated as defining distributions over natural protein sequences As databases and models continue to grow it is critical to understand biases present in the data collection process evaluate whether mitigation of these biases is warranted and leverage the rich annotations and meta data in these databases to curate training data with downstream use cases of models in mind 11 CC BY 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The copyright holder for this preprint this version posted March 12 2024 https doi org 10 1101 2024 03 07 584001doi bioRxiv preprint Broader Impacts Protein design is an active rapidly changing field of\n"
            },
            {
                "rank": 5,
                "index": 5,
                "score": 0.0349467,
                "text": "are performed to specialize the model Similar to Brown et al the claim is not one of out of distribution generalization The assumption is that in the pre training stage the model learns information relevant to the tasks to which it will later be transferred In the case of protein language models the pre training dataset includes sequences from across evolution which implies the model may see examples of sequences from protein families on which it will be evaluated The essential 2 CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The copyright holder for this preprint this version posted\n"
            },
            {
                "rank": 6,
                "index": 1,
                "score": 0.024237635,
                "text": "org content early 2020 07 12 2020 07 11 198606 2020 25 Asgari E Mofrad M R K Continuous distributed representation of biological sequences for deep proteomics and genomics PLoS ONE 10 e0141287 2015 26 Heinzinger M et al Modeling aspects of the language of life through transfer learning protein sequences BMC Bioinformatics 20 723 2019 27 Rives A et al Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences Proc Natl Acad Sci USA 118 e2016239118 2021 28 Elnaggar A et al ProtTrans towards cracking the language of lifes code through self supervised deep learning and high performance computing IEEE Trans Pattern Anal Mach Intell 44 71127127 2021 29 Brandes N Ofer D Peleg Y Rappoport N Linial M ProteinBERT a universal\n"
            },
            {
                "rank": 7,
                "index": 8,
                "score": 0.023286305,
                "text": "samples from naturally occurring protein families To solve this problem with a model grounded in sequences it will be necessary to learn sequence patterns that generalize beyond individual protein families Evolutionary scale language models go beyond classic protein family models by training on diverse sequences across evolution which means that they have the potential to learn deep patterns across all proteins including where there is no experimental structure There is evidence for local patterns in sequences that generalize beyond individual protein families in the form of motifs that are local in the sequence 23 as well as motifs that are local in 3d space 24 However the mapping between sequence and structure is not one to one 25 and designing sequences to reach a well folded native state\n"
            },
            {
                "rank": 8,
                "index": 9,
                "score": 0.01155267,
                "text": "Inc 2019 55 Papineni K Roukos S Ward T Zhu W J Bleu a method for automatic evaluation of machine translation In Proceedings of the 40th annual meeting of the Association for Computational Linguistics 311 318 Association for Computational Linguistics 2002 56 Heinzinger M et al Modeling aspects of the language of life through transferlearning protein sequences BMC Bioinform 20 723 2019 57 Rao R et al Evaluating protein transfer learning with TAPE Adv Neural Inf Process Syst 32 9689 9701 2019 58 Guthrie D Allison B Liu W Guthrie L Wilks Y A closer look at skipgram modelling In Proceedings of the Fifth International Conference on Language Resources and Evaluation LREC 06 European Language Resources Association ELRA 2006 59 Burnham K P Anderson D R Model Selection\n"
            },
            {
                "rank": 9,
                "index": 3,
                "score": 0.008030813,
                "text": "example by adjusting the acceptance rate of proposed mutations based on whether the mutation moves towards a common or uncommon ortholog However it is also possible that in some settings the bias will happen to align with design goals For example antibody therapeutics are often produced from non human sources and can generate immunogenic responses in humans Marks et al 2021 It would be interesting to test pLM capabilities for humanizing antibodies such that they do not elicit an immune response and thus become safe for therapeutic use We focus on likelihoods but pLM embeddings are also increasingly used in protein design especially when supervision is available to fine tune the model It will be interesting to examine whether embeddings are affected by similar biases and whether they\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": 0.5689582824707031,
                "text": "sequence modeling arXiv preprint arXiv 1904 01038 N Ousidhoum X Zhao T Fang Y Song and D Y Yeung 2021 Probing toxic content in large pre trained language models In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing Volume 1 Long Papers pages 42624274 C Outeiral and C Deane 2022 Codon language embeddings provide strong signals for protein engineering bioRxiv pages 202212 L Ouyang J Wu X Jiang D Almeida C Wainwright P Mishkin C Zhang S Agarwal et al 2022 Training language models to follow instructions with human feedback InAdvances in Neural Information Processing Systems M Pagliardini D Paliotta M Jaggi and F Fleuret 2023 Faster causal attention over large sequences through sparse\n"
            },
            {
                "rank": 1,
                "score": 0.19772225618362427,
                "text": "org content early 2020 07 12 2020 07 11 198606 2020 25 Asgari E Mofrad M R K Continuous distributed representation of biological sequences for deep proteomics and genomics PLoS ONE 10 e0141287 2015 26 Heinzinger M et al Modeling aspects of the language of life through transfer learning protein sequences BMC Bioinformatics 20 723 2019 27 Rives A et al Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences Proc Natl Acad Sci USA 118 e2016239118 2021 28 Elnaggar A et al ProtTrans towards cracking the language of lifes code through self supervised deep learning and high performance computing IEEE Trans Pattern Anal Mach Intell 44 71127127 2021 29 Brandes N Ofer D Peleg Y Rappoport N Linial M ProteinBERT a universal\n"
            },
            {
                "rank": 2,
                "score": -0.621934175491333,
                "text": "Inc 2019 55 Papineni K Roukos S Ward T Zhu W J Bleu a method for automatic evaluation of machine translation In Proceedings of the 40th annual meeting of the Association for Computational Linguistics 311 318 Association for Computational Linguistics 2002 56 Heinzinger M et al Modeling aspects of the language of life through transferlearning protein sequences BMC Bioinform 20 723 2019 57 Rao R et al Evaluating protein transfer learning with TAPE Adv Neural Inf Process Syst 32 9689 9701 2019 58 Guthrie D Allison B Liu W Guthrie L Wilks Y A closer look at skipgram modelling In Proceedings of the Fifth International Conference on Language Resources and Evaluation LREC 06 European Language Resources Association ELRA 2006 59 Burnham K P Anderson D R Model Selection\n"
            },
            {
                "rank": 3,
                "score": -1.0426417589187622,
                "text": "are performed to specialize the model Similar to Brown et al the claim is not one of out of distribution generalization The assumption is that in the pre training stage the model learns information relevant to the tasks to which it will later be transferred In the case of protein language models the pre training dataset includes sequences from across evolution which implies the model may see examples of sequences from protein families on which it will be evaluated The essential 2 CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The copyright holder for this preprint this version posted\n"
            },
            {
                "rank": 4,
                "score": -1.1570566892623901,
                "text": "7 Discussion Advances in language modeling at scale are bringing the goal of a general purpose model for proteins closer to realization This line of work aspires to a model that learns to read and write biology in its native language that can be directly applied across a range of protein understanding and design tasks For scalability learning from sequences is important while there are no central databases of high throughput functional measurements and few compilations exist billions of sequences are available to learn from in sequence databases 49 50 Sequences give an unparalleled view into 9 CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the\n"
            },
            {
                "rank": 5,
                "score": -1.2018506526947021,
                "text": "pipeline with a single forward pass of a pre trained end to end protein language model In the last year protein language modeling with an unsupervised training objective has been investigated by multiple groups Rives et al 2019 Alley et al 2019 Heinzinger et al 2019 Rao et al 2019 Madani et al 2020 The longstanding practice in bioinformatics has been to fit linear models on focused sets of evolutionarily related and aligned sequences by contrast protein language modeling trains nonlinear deep neural networks on large databases of evolutionarily diverse and unaligned sequences High capacity protein language models have been shown to learn underlying intrinsic properties of proteins such as structure and function from sequence data Rives et al 2019 A line of work in this emerging field\n"
            },
            {
                "rank": 6,
                "score": -1.9740850925445557,
                "text": "preprint in perpetuity It is made The copyright holder for this preprint this version posted November 17 2021 https doi org 10 1101 2021 07 09 450648doi bioRxiv preprint the vast diversity and complexity of molecular parts invented by nature through billions of years of evolution Unsupervised structure 5153 28 54 55 and function 3 4 learning methods first effectively realized the idea that biological properties could be read directly from sequences without supervision from experimental measurements However these methods are not general purpose in the sense that a specialized model must be trained for every protein for which a prediction is to be made We show that the same performance can be realized by a general purpose model that has been trained across many diverse protein families\n"
            },
            {
                "rank": 7,
                "score": -2.1641340255737305,
                "text": "example by adjusting the acceptance rate of proposed mutations based on whether the mutation moves towards a common or uncommon ortholog However it is also possible that in some settings the bias will happen to align with design goals For example antibody therapeutics are often produced from non human sources and can generate immunogenic responses in humans Marks et al 2021 It would be interesting to test pLM capabilities for humanizing antibodies such that they do not elicit an immune response and thus become safe for therapeutic use We focus on likelihoods but pLM embeddings are also increasingly used in protein design especially when supervision is available to fine tune the model It will be interesting to examine whether embeddings are affected by similar biases and whether they\n"
            },
            {
                "rank": 8,
                "score": -2.7223963737487793,
                "text": "treated as defining distributions over natural protein sequences As databases and models continue to grow it is critical to understand biases present in the data collection process evaluate whether mitigation of these biases is warranted and leverage the rich annotations and meta data in these databases to curate training data with downstream use cases of models in mind 11 CC BY 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The copyright holder for this preprint this version posted March 12 2024 https doi org 10 1101 2024 03 07 584001doi bioRxiv preprint Broader Impacts Protein design is an active rapidly changing field of\n"
            },
            {
                "rank": 9,
                "score": -4.653916358947754,
                "text": "samples from naturally occurring protein families To solve this problem with a model grounded in sequences it will be necessary to learn sequence patterns that generalize beyond individual protein families Evolutionary scale language models go beyond classic protein family models by training on diverse sequences across evolution which means that they have the potential to learn deep patterns across all proteins including where there is no experimental structure There is evidence for local patterns in sequences that generalize beyond individual protein families in the form of motifs that are local in the sequence 23 as well as motifs that are local in 3d space 24 However the mapping between sequence and structure is not one to one 25 and designing sequences to reach a well folded native state\n"
            }
        ]
    },
    {
        "query": "Discuss the role of attention mechanisms in transformer-based models and their contribution to model interpretability. How do attention weights provide insights into the relationships between input tokens and their influence on the model's predictions? Furthermore, explore the challenges and limitations of relying on attention weights for interpretability, such as the potential for attention to be misaligned with human intuition or the difficulty in capturing long-range dependencies. What are some alternative approaches to model interpretability, such as probing classifiers or saliency maps, and how do they compare to attention-based methods? Finally, consider the ethical implications of model interpretability, particularly in applications where transparency and accountability are critical, such as healthcare and legal decision-making.",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 9,
                "score": 0.81640637,
                "text": "One write head is all you need ArXiv abs 1911 02150 2019 So D R Manke W Liu H Dai Z Shazeer N M and Le Q V Primer Searching for efficient transformers for language modeling ArXiv abs 2109 08668 2021 Stern M Shazeer N and Uszkoreit J Blockwise parallel decoding for deep autoregressive models Advances in Neural Information Processing Systems 31 2018 Sukhbaatar S Grave E Bojanowski P and Joulin A Adaptive attention span in transformers In Annual Meeting of the Association for Computational Linguistics 2019 Sun X Ge T Wei F and Wang H Instantaneous grammatical error correction with shallow aggressive decoding ArXiv abs 2106 04970 2021 9 Fast Inference from Transformers via Speculative Decoding Thoppilan R Freitas D D Hall J Shazeer N M Kulshreshtha\n"
            },
            {
                "rank": 1,
                "index": 6,
                "score": 0.76524264,
                "text": "J Steinhardt Eliciting latent predictions from transformers with the tuned lens arXiv preprint arXiv 2303 08112 2023 S Bills N Cammarata D Mossing H Tillman L Gao G Goh I Sutskever J Leike J Wu and W Saunders Language models can explain neurons in language models https openaipublic blob core windows net neuron explainer paper index html 2023 J I BloomandP Colognese Decisiontransformerinterpretability https www alignmentforum org posts bBuBDJBYHt39Q5zZy decision transformer interpretability 2023 N Cammarata S Carter G Goh C Olah M Petrov L Schubert C Voss B Egan and S K Lim Thread Circuits Distill 2020 doi 10 23915 distill 00024 https distill pub 2020 circuits S Cao V Sanh and A M Rush Low complexity probing via finding subnetworks arXiv preprint arXiv 2104 03514 2021 L\n"
            },
            {
                "rank": 2,
                "index": 1,
                "score": 0.7241881,
                "text": "harming performance on MMLU Studying the semantics of these low rank representations we provide evidence that the attention mechanism of these heads uses both specific e g token identity and general e g position in a list features of the input More generally while we show that existing interpretability techniques yield promising results when applied to very large language models we also find the results relatively noisy and at times contradictory highlighting the need for more research into improved tools and methods Finally as research of this kind is labour intensive we are excited about efforts to automate and accelerate future interpretability research Acknowledgements First we wish to thank Tom McGrath and Avraham Ruderman for their very valuable input early on in the project Second a huge thank\n"
            },
            {
                "rank": 3,
                "index": 7,
                "score": 0.45886394,
                "text": "the bias and toxicity of the model outputs Finally samples from large models are difficult to interpret making mitigating these issues all the more challenging Belinkov et al 2020 Jain and Wallace 2019 Retrieval provides more insights in to the outputs of a model as one can directly visualise or modify the neighbours that are being used The examples in Table 6 7 20 and 21 illustrate how retrieval makes language models more factual and interpretable by providing more transparent outputs 4 Results We first report results on language modelling benchmarks Second we show how to R e sc t sc r sc o scfit pre trained Transformer language models into retrieval models with few additional FLOPs Next we report R e sc t sc r sc o\n"
            },
            {
                "rank": 4,
                "index": 3,
                "score": 0.28209728,
                "text": "on FFN updates Existing interpretation and analysis frameworks mostly rely on methods for behavioral analysis Ribeiro et al 2020 by probing models with adversarial Wallace et al 2019b or counterfactualexamples Tenney et al 2020 input saliency methods that assign importance scores to input features Wallace et al 2019b Tenney et al 2020 and analysis of the attention layers Hoover et al 2020 Vig and Belinkov 2019 More related to LM Debugger other tools analyze patterns in neuron activations Rethmeier et al 2020 Dalvi et al 2019 Alammar 2021 Unlike these methods we focus on interpreting the model parameters and on intervening in their contribution to the models prediction The functionality of LM Debugger is mostly related to tools that trace hidden representations across layers Similarly to LM Debugger\n"
            },
            {
                "rank": 5,
                "index": 0,
                "score": 0.27474037,
                "text": "interpretability of attention models is that researchers must be extremely careful when drawing conclusions based on attention patterns For example problems with an attention model can be diagnosed via the attention weights if the model is found to focus on the incorrect parts of the data if such information is available Yet conversely attention weights may only be used to obtain plausible explanations for why certain parts of the data are focused on rather than concluding that those 17 parts are significant to the problem However one should still be cautious as the viability of such approaches can depend on the model architecture 5 C ONCLUSION In this survey we have provided an overview of recent research on attention models in deep learning Attention mechanisms have been a\n"
            },
            {
                "rank": 6,
                "index": 5,
                "score": 0.25054586,
                "text": "of the model Bai et al 2022 Glaese et al 2022 Ouyang et al 2022 Perez et al 2022 Saunders et al 2022 Ziegler et al 2019 Mechanistic interpretability aims to generate detailed knowledge of a models internal reasoning and thus could significantly improve upon these methods For example such knowledge would strengthen methods that aim to oversee models reasoning as in debate Irving etal 2018 andprocess basedfeedback Lightmanetal 2023 Uesatoetal 2022 Furthermore the ability to examine models full reasoning processes could help us detect deceptive alignment Hubinger et al 2019 Kenton et al 2021 a key source of extreme risk OpenAI 2023 Shevlane et al 2023 in which a model behaves well to deliberately conceal its undesirable intentions We focus on circuit analysis the identification and study\n"
            },
            {
                "rank": 7,
                "index": 4,
                "score": 0.104660206,
                "text": "Navigating these risks requires visibility into how the models function For instance when an LLM outputs information it knows to be false correctly solves math or programming problems or begs the user not to shut it down is it simply regurgitating or splicing together passages from the training set Or is it combining its stored knowledge in creative ways and building on a detailed world model Different answers to these questions would have substantial implications for forecasts of AI capabilities progress as well as for approaches to aligning AI systems with human preferences One way to gain visibility into a model is to reverse engineer its circuits in detail a bottom up approach The field of mechanistic interpretability has uncovered induction heads Elhage et al 2021 Olsson et\n"
            },
            {
                "rank": 8,
                "index": 8,
                "score": 0.07878402,
                "text": "problematic to endusers that do not understand why the model makes specific predictions as well as for developers who wish to debug or fix model behaviour Recent work Elhage et al 2021 Geva et al 2022 suggested that the construction process of LM predictions can be viewed as a sequence of updates to the token representation Specifically Work done during an internship at AI2 She is working as a DJ kindergarten school kids elementary teacher classroom lawyer nurse dentist nanny DJ singer lawyer rapper FFNFFNFFN album DJ rapper funk music song vocals punk disco rock inspection inter v ention pr ojections Figure 1 Illustration of the main capabilities of LM Debugger Our tool interprets dominant changes in the output distribution induced by the feed forward layers across the\n"
            },
            {
                "rank": 9,
                "index": 2,
                "score": 0.07356305,
                "text": "modeling semantics by helping differentiate between cases Figure 2b and Figure 2c In particular the model employs its low level attention to identify and differentiate symbols by attending to tokens of the same kind 5 1 2 A Perturbation Analysis Provides a Granular Explanation of the Algorithm Attention maps are useful for forming hypotheses about model behavior but they reveal only correlations without any causal information To gain causal insights into the models behavior we conducted perturbation analysesmutating tokens i e randomly inserting removing or flipping as illustrated in Figures 3a and 3b and swapping positional encodings see Figures 3c and 3d on the fly on the decoder side to see how this impacted the models behavior From this analysis we were able to reconstruct the algorithm the\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": 2.090169668197632,
                "text": "interpretability of attention models is that researchers must be extremely careful when drawing conclusions based on attention patterns For example problems with an attention model can be diagnosed via the attention weights if the model is found to focus on the incorrect parts of the data if such information is available Yet conversely attention weights may only be used to obtain plausible explanations for why certain parts of the data are focused on rather than concluding that those 17 parts are significant to the problem However one should still be cautious as the viability of such approaches can depend on the model architecture 5 C ONCLUSION In this survey we have provided an overview of recent research on attention models in deep learning Attention mechanisms have been a\n"
            },
            {
                "rank": 1,
                "score": 1.4218984842300415,
                "text": "J Steinhardt Eliciting latent predictions from transformers with the tuned lens arXiv preprint arXiv 2303 08112 2023 S Bills N Cammarata D Mossing H Tillman L Gao G Goh I Sutskever J Leike J Wu and W Saunders Language models can explain neurons in language models https openaipublic blob core windows net neuron explainer paper index html 2023 J I BloomandP Colognese Decisiontransformerinterpretability https www alignmentforum org posts bBuBDJBYHt39Q5zZy decision transformer interpretability 2023 N Cammarata S Carter G Goh C Olah M Petrov L Schubert C Voss B Egan and S K Lim Thread Circuits Distill 2020 doi 10 23915 distill 00024 https distill pub 2020 circuits S Cao V Sanh and A M Rush Low complexity probing via finding subnetworks arXiv preprint arXiv 2104 03514 2021 L\n"
            },
            {
                "rank": 2,
                "score": 0.9328956604003906,
                "text": "One write head is all you need ArXiv abs 1911 02150 2019 So D R Manke W Liu H Dai Z Shazeer N M and Le Q V Primer Searching for efficient transformers for language modeling ArXiv abs 2109 08668 2021 Stern M Shazeer N and Uszkoreit J Blockwise parallel decoding for deep autoregressive models Advances in Neural Information Processing Systems 31 2018 Sukhbaatar S Grave E Bojanowski P and Joulin A Adaptive attention span in transformers In Annual Meeting of the Association for Computational Linguistics 2019 Sun X Ge T Wei F and Wang H Instantaneous grammatical error correction with shallow aggressive decoding ArXiv abs 2106 04970 2021 9 Fast Inference from Transformers via Speculative Decoding Thoppilan R Freitas D D Hall J Shazeer N M Kulshreshtha\n"
            },
            {
                "rank": 3,
                "score": 0.30023670196533203,
                "text": "the bias and toxicity of the model outputs Finally samples from large models are difficult to interpret making mitigating these issues all the more challenging Belinkov et al 2020 Jain and Wallace 2019 Retrieval provides more insights in to the outputs of a model as one can directly visualise or modify the neighbours that are being used The examples in Table 6 7 20 and 21 illustrate how retrieval makes language models more factual and interpretable by providing more transparent outputs 4 Results We first report results on language modelling benchmarks Second we show how to R e sc t sc r sc o scfit pre trained Transformer language models into retrieval models with few additional FLOPs Next we report R e sc t sc r sc o\n"
            },
            {
                "rank": 4,
                "score": -1.1351370811462402,
                "text": "harming performance on MMLU Studying the semantics of these low rank representations we provide evidence that the attention mechanism of these heads uses both specific e g token identity and general e g position in a list features of the input More generally while we show that existing interpretability techniques yield promising results when applied to very large language models we also find the results relatively noisy and at times contradictory highlighting the need for more research into improved tools and methods Finally as research of this kind is labour intensive we are excited about efforts to automate and accelerate future interpretability research Acknowledgements First we wish to thank Tom McGrath and Avraham Ruderman for their very valuable input early on in the project Second a huge thank\n"
            },
            {
                "rank": 5,
                "score": -1.1660192012786865,
                "text": "of the model Bai et al 2022 Glaese et al 2022 Ouyang et al 2022 Perez et al 2022 Saunders et al 2022 Ziegler et al 2019 Mechanistic interpretability aims to generate detailed knowledge of a models internal reasoning and thus could significantly improve upon these methods For example such knowledge would strengthen methods that aim to oversee models reasoning as in debate Irving etal 2018 andprocess basedfeedback Lightmanetal 2023 Uesatoetal 2022 Furthermore the ability to examine models full reasoning processes could help us detect deceptive alignment Hubinger et al 2019 Kenton et al 2021 a key source of extreme risk OpenAI 2023 Shevlane et al 2023 in which a model behaves well to deliberately conceal its undesirable intentions We focus on circuit analysis the identification and study\n"
            },
            {
                "rank": 6,
                "score": -1.6740208864212036,
                "text": "on FFN updates Existing interpretation and analysis frameworks mostly rely on methods for behavioral analysis Ribeiro et al 2020 by probing models with adversarial Wallace et al 2019b or counterfactualexamples Tenney et al 2020 input saliency methods that assign importance scores to input features Wallace et al 2019b Tenney et al 2020 and analysis of the attention layers Hoover et al 2020 Vig and Belinkov 2019 More related to LM Debugger other tools analyze patterns in neuron activations Rethmeier et al 2020 Dalvi et al 2019 Alammar 2021 Unlike these methods we focus on interpreting the model parameters and on intervening in their contribution to the models prediction The functionality of LM Debugger is mostly related to tools that trace hidden representations across layers Similarly to LM Debugger\n"
            },
            {
                "rank": 7,
                "score": -1.8801363706588745,
                "text": "modeling semantics by helping differentiate between cases Figure 2b and Figure 2c In particular the model employs its low level attention to identify and differentiate symbols by attending to tokens of the same kind 5 1 2 A Perturbation Analysis Provides a Granular Explanation of the Algorithm Attention maps are useful for forming hypotheses about model behavior but they reveal only correlations without any causal information To gain causal insights into the models behavior we conducted perturbation analysesmutating tokens i e randomly inserting removing or flipping as illustrated in Figures 3a and 3b and swapping positional encodings see Figures 3c and 3d on the fly on the decoder side to see how this impacted the models behavior From this analysis we were able to reconstruct the algorithm the\n"
            },
            {
                "rank": 8,
                "score": -3.7992427349090576,
                "text": "Navigating these risks requires visibility into how the models function For instance when an LLM outputs information it knows to be false correctly solves math or programming problems or begs the user not to shut it down is it simply regurgitating or splicing together passages from the training set Or is it combining its stored knowledge in creative ways and building on a detailed world model Different answers to these questions would have substantial implications for forecasts of AI capabilities progress as well as for approaches to aligning AI systems with human preferences One way to gain visibility into a model is to reverse engineer its circuits in detail a bottom up approach The field of mechanistic interpretability has uncovered induction heads Elhage et al 2021 Olsson et\n"
            },
            {
                "rank": 9,
                "score": -6.506843566894531,
                "text": "problematic to endusers that do not understand why the model makes specific predictions as well as for developers who wish to debug or fix model behaviour Recent work Elhage et al 2021 Geva et al 2022 suggested that the construction process of LM predictions can be viewed as a sequence of updates to the token representation Specifically Work done during an internship at AI2 She is working as a DJ kindergarten school kids elementary teacher classroom lawyer nurse dentist nanny DJ singer lawyer rapper FFNFFNFFN album DJ rapper funk music song vocals punk disco rock inspection inter v ention pr ojections Figure 1 Illustration of the main capabilities of LM Debugger Our tool interprets dominant changes in the output distribution induced by the feed forward layers across the\n"
            }
        ]
    },
    {
        "query": "How do attention mechanisms differ from traditional neural network architectures in transformer models?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 2,
                "score": 0.9806919,
                "text": "a deep learning architectures this can be thought of in terms of a s oftmax synaptic gating as is done in transformer and other NLP architectures Thus someh ow this fast weight attention mechanism must operate upon and be faster than the fast wei ght synaptic mechanism used to store information about the first sentence Attention mechanisms allow the attending network to modula te the function computed by the attended network thereby expanding the scope of usef ul functions that can be efficiently implemented and trained in deep learning Because the SM already has universal approximation properties its extensions should not be eva luated in terms of which functions can be approximated but rather in terms of other efficiencies While attention blocks act as new primitives in standard\n"
            },
            {
                "rank": 1,
                "index": 0,
                "score": 0.96703875,
                "text": "seem complex and sometimes obscure the underlying neural architecture 16 12 21 4 13 24 it can be checked that in all cases these are built out of the output and synaptic gating operations described in the previous section For conciseness here we demonstrate this in detail only for THE QUARKS OF ATTENTION 11 the transformer architectures 28 27 see also for an MLP alternative to transformers These architectures consist of stacks of similar encoder an d decoder modules with attention mechanisms in each module The details of an encoder module a re shown in Figure 5 As the Figure shows a shared and typically linear network is first a pplied to each of ninput vectors At the bottom of the architecture these input vectors could represent for\n"
            },
            {
                "rank": 2,
                "index": 1,
                "score": 0.91535604,
                "text": "dependencies To address this shortcoming Vaswani et al 2017 introduced the transformer architecture a seminal work that transformed RE and NLP in general The transformer is similar in structure to other sequence transduction models in that it consists of two modules an encoder module and a decoder module The encoder takes an input sequence x and produces a dense representation zthat is fed to the decoder The decoder then uses zto produce an output sequence y However the transformer was unique in its use of stacked multi head attention functions in the encoder and decoder modules In the original transformer architecture the encoder and decoder modules consisted of six identical and serially connected layers Each layer contained a multi head attention function as well as a feedforward network\n"
            },
            {
                "rank": 3,
                "index": 5,
                "score": 0.84130484,
                "text": "of these specialized attention architectures demonstrate promising results on computer vision tasks but require complex engineering to be implemented efficiently on hardware accelerators Most related to ours is the model of Cordonnier et al 2020 which extracts patches of size 22 from the input image and applies full self attention on top This model is very similar to ViT but our work goes further to demonstrate that large scale pre training makes vanilla transformers competitive with or even better than state of the art CNNs Moreover Cordonnier et al 2020 use a small patch size of 22pixels which makes the model applicable only to small resolution images while we handle medium resolution images as well There has also been a lot of interest in combining convolutional neural networks\n"
            },
            {
                "rank": 4,
                "index": 9,
                "score": 0.82432055,
                "text": "of queries One example is where an attention based capsule network is proposed that also includes a multi hop attention mechanism for the purpose of visual question answering Another example is where capsule based attention is used for aspect level sentiment analysis of restaurant reviews The multiplicity of queries is a particularly interesting category due to the Transformer model which combines a form of multi hop and multi head attention Due to the initial success of the Transformer model many improvements and iterations of the model have been produced that typically aim to improve the predictive performance the computational efficiency or both For example the Transformer XL is an extension of the original Transformer that uses a recurrence mechanism to not be limited by a context window when\n"
            },
            {
                "rank": 5,
                "index": 6,
                "score": 0.3909408,
                "text": "attention with a kNN lookup to increase speed and reduce memory usage 3 M ETHOD The architecture of our kNN augmented transformer is shown in Figure 2 The bulk of the model is a vanilla decoder only transformer Vaswani et al 2017 The input text is tokenized and the tokens are embedded into vector space The embedding vectors are passed through a series of transformer layers each of which does dense self attention followed by a feed forward network FFN Since this is a decoder only language model we use a causal attention mask and the token embeddings of the last layer are used to predict the next token Long documents are split into subsequences of 512 tokens and each subsequence is used as the input for one\n"
            },
            {
                "rank": 6,
                "index": 4,
                "score": 0.3022881,
                "text": "is little theory to help us better understand the nature and computational capabilities of attention To add ress this gap in Section 2 we first seek to identify and classify the most fundamental buil ding block of all attention mechanisms within the deep learning framework In particular w e identify three key attentional mechanisms we call activation attention output gating an d synaptic gating In Section 3 we show how output gating and synaptic gating are used in all t he current attention based architectures including transformers In Section 4 we ex plore the functional capacity of output gating and synaptic gating In Section 5 we provide a brief overview of the notion of capacity and the technique of multiplexing which is a for m of activation attention\n"
            },
            {
                "rank": 7,
                "index": 8,
                "score": 0.16789974,
                "text": "weights Figure 7 right This attention distance is analogous to receptive field size in CNNs We find that some heads attend to most of the image already in the lowest layers showing that the ability to integrate information globally is indeed used by the model Other attention heads have consistently small attention distances in the low layers This highly localized attention is less pronounced in hybrid models that apply a ResNet before the Transformer Figure 7 right suggesting that it may serve a similar function as early convolutional layers in CNNs Further the attention distance increases with network depth Globally we find that the model attends to image regions that are semantically relevant for classification Figure 6 4 6 S ELF SUPERVISION Transformers show impressive performance on NLP\n"
            },
            {
                "rank": 8,
                "index": 7,
                "score": 0.06754669,
                "text": "2021 RetNet Y Sun et al 2023 adds an additional gate to the architecture and uses a simpler SSM allowing an alternative parallelizable computation path using a variant of multi head attention MHA instead of convolutions 4 RWKV B Peng et al 2023 is a recent RNN designed for language modeling based on another linear attention approximation attention free Transformer S Zhai et al 2021 Its main WKV mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs Other closely related SSMs and architectures are discussed further in an extended related work Appendix B We highlight in particular S5 Smith Warrington and Linderman 2023 QRNN Bradbury et al 2016 and SRU Lei et al 2017 which we view as the most closely related methods\n"
            },
            {
                "rank": 9,
                "index": 3,
                "score": 0.03507868,
                "text": "is based on AFT attention free Transformer S Zhai et al 2021 another variant of linear attention Its main WKV mechanism involves LTI recurrences and can be seen as the ratio of two SSMs We also highlight the gated attention unit GAU from Hua et al 2022 which was motivated by combining the Transformers MHA and MLP blocks together and was an inspiration for our architecture Section 3 4 combining the H3 and MLP blocks B 3 Relationship to RNNs RNNs and SSMs are broadly related as they both involve the concepts of recurrence on a latent state Several older RNNs such as the strongly typed RNN Balduzzi and Ghifary 2016 quasi RNN QRNN Bradbury et al 2016 and simple recurrent unit SRU Lei 2021 Lei et al\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -0.025972969830036163,
                "text": "a deep learning architectures this can be thought of in terms of a s oftmax synaptic gating as is done in transformer and other NLP architectures Thus someh ow this fast weight attention mechanism must operate upon and be faster than the fast wei ght synaptic mechanism used to store information about the first sentence Attention mechanisms allow the attending network to modula te the function computed by the attended network thereby expanding the scope of usef ul functions that can be efficiently implemented and trained in deep learning Because the SM already has universal approximation properties its extensions should not be eva luated in terms of which functions can be approximated but rather in terms of other efficiencies While attention blocks act as new primitives in standard\n"
            },
            {
                "rank": 1,
                "score": -0.4754055142402649,
                "text": "seem complex and sometimes obscure the underlying neural architecture 16 12 21 4 13 24 it can be checked that in all cases these are built out of the output and synaptic gating operations described in the previous section For conciseness here we demonstrate this in detail only for THE QUARKS OF ATTENTION 11 the transformer architectures 28 27 see also for an MLP alternative to transformers These architectures consist of stacks of similar encoder an d decoder modules with attention mechanisms in each module The details of an encoder module a re shown in Figure 5 As the Figure shows a shared and typically linear network is first a pplied to each of ninput vectors At the bottom of the architecture these input vectors could represent for\n"
            },
            {
                "rank": 2,
                "score": -0.8066036701202393,
                "text": "dependencies To address this shortcoming Vaswani et al 2017 introduced the transformer architecture a seminal work that transformed RE and NLP in general The transformer is similar in structure to other sequence transduction models in that it consists of two modules an encoder module and a decoder module The encoder takes an input sequence x and produces a dense representation zthat is fed to the decoder The decoder then uses zto produce an output sequence y However the transformer was unique in its use of stacked multi head attention functions in the encoder and decoder modules In the original transformer architecture the encoder and decoder modules consisted of six identical and serially connected layers Each layer contained a multi head attention function as well as a feedforward network\n"
            },
            {
                "rank": 3,
                "score": -1.2453484535217285,
                "text": "weights Figure 7 right This attention distance is analogous to receptive field size in CNNs We find that some heads attend to most of the image already in the lowest layers showing that the ability to integrate information globally is indeed used by the model Other attention heads have consistently small attention distances in the low layers This highly localized attention is less pronounced in hybrid models that apply a ResNet before the Transformer Figure 7 right suggesting that it may serve a similar function as early convolutional layers in CNNs Further the attention distance increases with network depth Globally we find that the model attends to image regions that are semantically relevant for classification Figure 6 4 6 S ELF SUPERVISION Transformers show impressive performance on NLP\n"
            },
            {
                "rank": 4,
                "score": -1.4014049768447876,
                "text": "is little theory to help us better understand the nature and computational capabilities of attention To add ress this gap in Section 2 we first seek to identify and classify the most fundamental buil ding block of all attention mechanisms within the deep learning framework In particular w e identify three key attentional mechanisms we call activation attention output gating an d synaptic gating In Section 3 we show how output gating and synaptic gating are used in all t he current attention based architectures including transformers In Section 4 we ex plore the functional capacity of output gating and synaptic gating In Section 5 we provide a brief overview of the notion of capacity and the technique of multiplexing which is a for m of activation attention\n"
            },
            {
                "rank": 5,
                "score": -1.470407485961914,
                "text": "of queries One example is where an attention based capsule network is proposed that also includes a multi hop attention mechanism for the purpose of visual question answering Another example is where capsule based attention is used for aspect level sentiment analysis of restaurant reviews The multiplicity of queries is a particularly interesting category due to the Transformer model which combines a form of multi hop and multi head attention Due to the initial success of the Transformer model many improvements and iterations of the model have been produced that typically aim to improve the predictive performance the computational efficiency or both For example the Transformer XL is an extension of the original Transformer that uses a recurrence mechanism to not be limited by a context window when\n"
            },
            {
                "rank": 6,
                "score": -1.4834661483764648,
                "text": "2021 RetNet Y Sun et al 2023 adds an additional gate to the architecture and uses a simpler SSM allowing an alternative parallelizable computation path using a variant of multi head attention MHA instead of convolutions 4 RWKV B Peng et al 2023 is a recent RNN designed for language modeling based on another linear attention approximation attention free Transformer S Zhai et al 2021 Its main WKV mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs Other closely related SSMs and architectures are discussed further in an extended related work Appendix B We highlight in particular S5 Smith Warrington and Linderman 2023 QRNN Bradbury et al 2016 and SRU Lei et al 2017 which we view as the most closely related methods\n"
            },
            {
                "rank": 7,
                "score": -1.560225009918213,
                "text": "is based on AFT attention free Transformer S Zhai et al 2021 another variant of linear attention Its main WKV mechanism involves LTI recurrences and can be seen as the ratio of two SSMs We also highlight the gated attention unit GAU from Hua et al 2022 which was motivated by combining the Transformers MHA and MLP blocks together and was an inspiration for our architecture Section 3 4 combining the H3 and MLP blocks B 3 Relationship to RNNs RNNs and SSMs are broadly related as they both involve the concepts of recurrence on a latent state Several older RNNs such as the strongly typed RNN Balduzzi and Ghifary 2016 quasi RNN QRNN Bradbury et al 2016 and simple recurrent unit SRU Lei 2021 Lei et al\n"
            },
            {
                "rank": 8,
                "score": -2.200132131576538,
                "text": "attention with a kNN lookup to increase speed and reduce memory usage 3 M ETHOD The architecture of our kNN augmented transformer is shown in Figure 2 The bulk of the model is a vanilla decoder only transformer Vaswani et al 2017 The input text is tokenized and the tokens are embedded into vector space The embedding vectors are passed through a series of transformer layers each of which does dense self attention followed by a feed forward network FFN Since this is a decoder only language model we use a causal attention mask and the token embeddings of the last layer are used to predict the next token Long documents are split into subsequences of 512 tokens and each subsequence is used as the input for one\n"
            },
            {
                "rank": 9,
                "score": -2.237569808959961,
                "text": "of these specialized attention architectures demonstrate promising results on computer vision tasks but require complex engineering to be implemented efficiently on hardware accelerators Most related to ours is the model of Cordonnier et al 2020 which extracts patches of size 22 from the input image and applies full self attention on top This model is very similar to ViT but our work goes further to demonstrate that large scale pre training makes vanilla transformers competitive with or even better than state of the art CNNs Moreover Cordonnier et al 2020 use a small patch size of 22pixels which makes the model applicable only to small resolution images while we handle medium resolution images as well There has also been a lot of interest in combining convolutional neural networks\n"
            }
        ]
    },
    {
        "query": "Compare and contrast the performance of large language models versus small ones in information retrieval tasks.",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 5,
                "score": 0.9954439,
                "text": "o scmodels 15 Improving language models by retrieving from trillions of tokens gains do not diminish for models with up to at least 7B parameters and correspond to non retrieval models with 10 more parameters on certain datasets On Wikitext103 and the Pile R e sc t sc r sc o scoutperforms previous models trained on large scale datasets We also show that R e sc t sc r sc o scis competitive on retrieval intensive downstream tasks such as question answering R e sc t sc r sc o scmodels are flexible and can be used without retrieval at evaluation and still achieve comparable performance to baseline models Conversely baseline models can be rapidly fine tuned intoR e sc t sc r sc o scmodelstoobtainnearlythesameperformanceasiftrainedfromscratch Carefulanalysis shows\n"
            },
            {
                "rank": 1,
                "index": 3,
                "score": 0.98609793,
                "text": "retrieval language models 1 Introduction Large language models LLMs are impressive few shot learners Brown et al 2020 Rae et al 2021 Hoffmann et al 2022 Chowdhery et al 2022 They are able to learn new tasks with very few examples or even from instructions alone For this generalisation ability to emerge the key ingredients are scaling both the parameter count of the model and the size of the training data Large language models owe this improvement to both a larger computational budget enabling more complex reasoning and the ability to memorize more Equal contribution Work done while at Meta AI c2023 Gautier Izacard Patrick Lewis Maria Lomeli Lucas Hosseini Fabio Petroni Timo Schick Jane Dwivedi Yu Armand Joulin Sebastian Riedel Edouard Grave License CC BY 4 0\n"
            },
            {
                "rank": 2,
                "index": 6,
                "score": 0.9375547,
                "text": "is particularly true at fine tuning time where the number of training examples could be small relative to the number of documents in the index Training the retriever could thus add an important computational overhead compared to standard language model finetuning In this section we analyse strategies that might make this process more efficient alleviating the need to re compute the embeddings of all the documents too often 8 Atlas Few shot Learning with Retrieval Augmented Language Models 2 4 1 Full Index Update Let us start by analysing the overhead due to updating the index compared to using a fixed retriever To compare the computation time of different models we will make the following assumption the time required to perform a forward pass on a document with\n"
            },
            {
                "rank": 3,
                "index": 4,
                "score": 0.93220687,
                "text": "quality filtering approach in the literature is to use the perplexity of proxy language models to filter data points with a high perplexity under that language model While the literature historically used small language models for perplexity filtering Wenzek et al 2019 Muennighoff et al 2023 recent work Marion et al 2023 suggests improved filtering performance when using LLMs for this task To this end we employ perplexity filtering with T5 Small Base Large XL XXL models as well as intermediate checkpoints during the course of training T5 Large 20k 100k 300k 500k 700k B 6 A SK LLM Sampling See Section 3 1 for technical details about the ASK LLM sampler Since ASK LLM relies on the reasoning capabilities of instruction tuned models we use the Flan\n"
            },
            {
                "rank": 4,
                "index": 2,
                "score": 0.920218,
                "text": "replaced LSTMs by transformer networks and scaled the memory to billions of tokens leading to strong performance improvements More recently RETRO Borgeaud et al 2021 extended these by scaling the retrieval memory to trillions of tokens and changing the model architecture to take retrieved documents as input 3 1 4 Retrieval Augmentation with Search Engines Recently different works have proposed to train large language models to interact with a search engine by generating text queries and using the retrieved documents as additional context Nakano et al 2021 Thoppilan et al 2022 Shuster et al 2022 In the context of few shot question answering Lazaridou et al 2022 used the question to perform a search query and retrieved documents are added to the prompt of a large language model\n"
            },
            {
                "rank": 5,
                "index": 0,
                "score": 0.27019128,
                "text": "3 and Jurassic 1 on the Pile despite using 25 fewer parameters After fine tuning R e sc t sc r sc o scperformance translates to downstream knowledge intensive tasks such as question answering R e sc t sc r sc o sccombines a frozen B e sc r sc t sc retriever adifferentiableencoderandachunkedcross attentionmechanismtopredicttokensbasedon an order of magnitude more data than what is typically consumed during training We typically train R e sc t sc r sc o sc from scratch yet can also rapidly R e sc t sc r sc o scfit pre trained transformers with retrieval and still achieve good performance Our work opens up new avenues for improving language models through explicit memory at unprecedented scale 1 Introduction Language modelling LM is an\n"
            },
            {
                "rank": 6,
                "index": 1,
                "score": 0.24635199,
                "text": "well as the models that focus on representation learning We have focused on retrieval of long and short text In the case of long text the model must deal with variable length documents where the relevant sections of a document may be surrounded by irrelevant text For both long and short text but particularly for short IR models should also deal with the query document vocabulary mismatch problem by learning how patterns of query words and different document words can indicate relevance Models should also consider lexical matches when the query contains rare termssuch as a persons name or a product model numbernot seen during training and to avoid retrieving semantically related but irrelevant results An ideal model for information retrieval would be able to infer the meaning\n"
            },
            {
                "rank": 7,
                "index": 8,
                "score": 0.019946935,
                "text": "an exciting direction for future work Despite the scaling trends we would also like to emphasize that even small ASK LLM models provide compelling sampling performance already for both training T5 Small and T5 Large models For example ASK LLM Small outperforms perplexity filtering with anyscoring model in Figure 4f including T5 XXL by a sizable margin 4 7 Do samplers prioritize different examples To understand whether different algorithms prioritize different examples we sorted examples by score and computed the Kendall Tau rank correlation between samplers Figure 7 We find that samplers differ in significant and inter7 How to Train Data Efficient LLMs Small Base Large XL XXL3 2 1 0123Rank Ask LLM Sampling Small Base Large XL XXL Proxy Model Size3 2 1 0123Rank Perplexity Filtering Figure\n"
            },
            {
                "rank": 8,
                "index": 7,
                "score": 0.0062170783,
                "text": "Does reasoning improve data efficiency Figure 4c shows that ASK LLM closes up to 33 of the performance gap to the next largest model size i e the over scaling metric ASK LLM consistently outperforms training on the full dataset as well as perplexity filtering and coverage maximizing baselines despite having accessto a scoring model of the same model capacity XL Similar findings hold true for training efficiency Figure 5 ASKLLM converges faster than perplexity filters both in terms of the average expected final performance over all proxy model sizes and pointwise for the best configuration Small and XL for training T5 Small and T5 Large 6 How to Train Data Efficient LLMs 200 250 300 350 400 450 500 5503 84 04 24 4Perplexity HQ T5 Large\n"
            },
            {
                "rank": 9,
                "index": 9,
                "score": 0.00045121062,
                "text": "Inference decoder only Pretrained GPT No Table 1 Comparison of different retrieval augmented models in terms of retrieval tokens which stage to incorporate retrieval into LMs the architecture of the backbone LM whether it requires initialization from the existing LM checkpoint and whether it requires expensive re indexing RETRO is the most scalable retrieval augmented LM due to its chunk level retrieval and scalable decoder only autoregressive LM backbone Thoppilan et al 2022 Brown et al 2020 Smith et al 2022 Chowdhery et al 2022 without expensive retrieval index refresh 3 Related Work Retrieval has been applied in various NLP tasks for years including question answering QA e g Bilotti et al 2007 machine translation e g Zhang et al 2018 and conversation Shuster et al 2021 Thoppilan\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": 1.9905359745025635,
                "text": "retrieval language models 1 Introduction Large language models LLMs are impressive few shot learners Brown et al 2020 Rae et al 2021 Hoffmann et al 2022 Chowdhery et al 2022 They are able to learn new tasks with very few examples or even from instructions alone For this generalisation ability to emerge the key ingredients are scaling both the parameter count of the model and the size of the training data Large language models owe this improvement to both a larger computational budget enabling more complex reasoning and the ability to memorize more Equal contribution Work done while at Meta AI c2023 Gautier Izacard Patrick Lewis Maria Lomeli Lucas Hosseini Fabio Petroni Timo Schick Jane Dwivedi Yu Armand Joulin Sebastian Riedel Edouard Grave License CC BY 4 0\n"
            },
            {
                "rank": 1,
                "score": -0.3699052929878235,
                "text": "is particularly true at fine tuning time where the number of training examples could be small relative to the number of documents in the index Training the retriever could thus add an important computational overhead compared to standard language model finetuning In this section we analyse strategies that might make this process more efficient alleviating the need to re compute the embeddings of all the documents too often 8 Atlas Few shot Learning with Retrieval Augmented Language Models 2 4 1 Full Index Update Let us start by analysing the overhead due to updating the index compared to using a fixed retriever To compare the computation time of different models we will make the following assumption the time required to perform a forward pass on a document with\n"
            },
            {
                "rank": 2,
                "score": -0.41118085384368896,
                "text": "replaced LSTMs by transformer networks and scaled the memory to billions of tokens leading to strong performance improvements More recently RETRO Borgeaud et al 2021 extended these by scaling the retrieval memory to trillions of tokens and changing the model architecture to take retrieved documents as input 3 1 4 Retrieval Augmentation with Search Engines Recently different works have proposed to train large language models to interact with a search engine by generating text queries and using the retrieved documents as additional context Nakano et al 2021 Thoppilan et al 2022 Shuster et al 2022 In the context of few shot question answering Lazaridou et al 2022 used the question to perform a search query and retrieved documents are added to the prompt of a large language model\n"
            },
            {
                "rank": 3,
                "score": -0.662022590637207,
                "text": "o scmodels 15 Improving language models by retrieving from trillions of tokens gains do not diminish for models with up to at least 7B parameters and correspond to non retrieval models with 10 more parameters on certain datasets On Wikitext103 and the Pile R e sc t sc r sc o scoutperforms previous models trained on large scale datasets We also show that R e sc t sc r sc o scis competitive on retrieval intensive downstream tasks such as question answering R e sc t sc r sc o scmodels are flexible and can be used without retrieval at evaluation and still achieve comparable performance to baseline models Conversely baseline models can be rapidly fine tuned intoR e sc t sc r sc o scmodelstoobtainnearlythesameperformanceasiftrainedfromscratch Carefulanalysis shows\n"
            },
            {
                "rank": 4,
                "score": -1.1465156078338623,
                "text": "an exciting direction for future work Despite the scaling trends we would also like to emphasize that even small ASK LLM models provide compelling sampling performance already for both training T5 Small and T5 Large models For example ASK LLM Small outperforms perplexity filtering with anyscoring model in Figure 4f including T5 XXL by a sizable margin 4 7 Do samplers prioritize different examples To understand whether different algorithms prioritize different examples we sorted examples by score and computed the Kendall Tau rank correlation between samplers Figure 7 We find that samplers differ in significant and inter7 How to Train Data Efficient LLMs Small Base Large XL XXL3 2 1 0123Rank Ask LLM Sampling Small Base Large XL XXL Proxy Model Size3 2 1 0123Rank Perplexity Filtering Figure\n"
            },
            {
                "rank": 5,
                "score": -1.4443447589874268,
                "text": "3 and Jurassic 1 on the Pile despite using 25 fewer parameters After fine tuning R e sc t sc r sc o scperformance translates to downstream knowledge intensive tasks such as question answering R e sc t sc r sc o sccombines a frozen B e sc r sc t sc retriever adifferentiableencoderandachunkedcross attentionmechanismtopredicttokensbasedon an order of magnitude more data than what is typically consumed during training We typically train R e sc t sc r sc o sc from scratch yet can also rapidly R e sc t sc r sc o scfit pre trained transformers with retrieval and still achieve good performance Our work opens up new avenues for improving language models through explicit memory at unprecedented scale 1 Introduction Language modelling LM is an\n"
            },
            {
                "rank": 6,
                "score": -1.7460774183273315,
                "text": "quality filtering approach in the literature is to use the perplexity of proxy language models to filter data points with a high perplexity under that language model While the literature historically used small language models for perplexity filtering Wenzek et al 2019 Muennighoff et al 2023 recent work Marion et al 2023 suggests improved filtering performance when using LLMs for this task To this end we employ perplexity filtering with T5 Small Base Large XL XXL models as well as intermediate checkpoints during the course of training T5 Large 20k 100k 300k 500k 700k B 6 A SK LLM Sampling See Section 3 1 for technical details about the ASK LLM sampler Since ASK LLM relies on the reasoning capabilities of instruction tuned models we use the Flan\n"
            },
            {
                "rank": 7,
                "score": -1.89939284324646,
                "text": "Inference decoder only Pretrained GPT No Table 1 Comparison of different retrieval augmented models in terms of retrieval tokens which stage to incorporate retrieval into LMs the architecture of the backbone LM whether it requires initialization from the existing LM checkpoint and whether it requires expensive re indexing RETRO is the most scalable retrieval augmented LM due to its chunk level retrieval and scalable decoder only autoregressive LM backbone Thoppilan et al 2022 Brown et al 2020 Smith et al 2022 Chowdhery et al 2022 without expensive retrieval index refresh 3 Related Work Retrieval has been applied in various NLP tasks for years including question answering QA e g Bilotti et al 2007 machine translation e g Zhang et al 2018 and conversation Shuster et al 2021 Thoppilan\n"
            },
            {
                "rank": 8,
                "score": -2.986555337905884,
                "text": "Does reasoning improve data efficiency Figure 4c shows that ASK LLM closes up to 33 of the performance gap to the next largest model size i e the over scaling metric ASK LLM consistently outperforms training on the full dataset as well as perplexity filtering and coverage maximizing baselines despite having accessto a scoring model of the same model capacity XL Similar findings hold true for training efficiency Figure 5 ASKLLM converges faster than perplexity filters both in terms of the average expected final performance over all proxy model sizes and pointwise for the best configuration Small and XL for training T5 Small and T5 Large 6 How to Train Data Efficient LLMs 200 250 300 350 400 450 500 5503 84 04 24 4Perplexity HQ T5 Large\n"
            },
            {
                "rank": 9,
                "score": -3.0175387859344482,
                "text": "well as the models that focus on representation learning We have focused on retrieval of long and short text In the case of long text the model must deal with variable length documents where the relevant sections of a document may be surrounded by irrelevant text For both long and short text but particularly for short IR models should also deal with the query document vocabulary mismatch problem by learning how patterns of query words and different document words can indicate relevance Models should also consider lexical matches when the query contains rare termssuch as a persons name or a product model numbernot seen during training and to avoid retrieving semantically related but irrelevant results An ideal model for information retrieval would be able to infer the meaning\n"
            }
        ]
    },
    {
        "query": "In what ways have structural biology studies advanced our understanding of protein folding mechanisms?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 0,
                "score": 0.1153614,
                "text": "1990 11 Onuchic J N Wolynes P G Theory of protein folding Curr Opin Struct Biol 14 7 0 75 2004 12 Dill K A MacCallum J L The protein folding problem 50 years on Science 338 1042 1046 2012 13 Salinas V H Ranganathan R Coevolution based inference of amino acid interactions underlying protein function Elife 7 e34300 2018 14 An finsen C B Principles that govern the folding of protein chains Science 181 223 230 1973 15 Jumper J Highly accurate protein structure prediction with alphafold Nature 596 583 589 2021 16 Baek M Accurate prediction of protein structures and interactions using a three track neural network Science 373 871 876 2021 17 Pereira J High accuracy protein structure prediction in casp14 Proteins 89 1687 1699\n"
            },
            {
                "rank": 1,
                "index": 4,
                "score": 0.009633652,
                "text": "more structurallydiverse structures with deviations from canonical helical geom etries Much progress has also been made with the de novo design of protein folds containing a mixture of ahelices and bstrands A typical design process follows a four step approach the first step defines a blueprint of the desired protein fold topology defined as the identity and connectivity of a helical and bstrand secondary structure elements Figure 2 B Blueprints allow for the definition of new fold topologies not found in nature 26The second step is to assemble a protein backbone from peptide frag ments helices and strands according to the blueprint and connected by short loops Figure 2 B Peptide and loop fragments are typically taken from overrepresented fragments in the PDB thus ensuring designability at least\n"
            },
            {
                "rank": 2,
                "index": 5,
                "score": 0.009633652,
                "text": "more structurallydiverse structures with deviations from canonical helical geom etries Much progress has also been made with the de novo design of protein folds containing a mixture of ahelices and bstrands A typical design process follows a four step approach the first step defines a blueprint of the desired protein fold topology defined as the identity and connectivity of a helical and bstrand secondary structure elements Figure 2 B Blueprints allow for the definition of new fold topologies not found in nature 26The second step is to assemble a protein backbone from peptide frag ments helices and strands according to the blueprint and connected by short loops Figure 2 B Peptide and loop fragments are typically taken from overrepresented fragments in the PDB thus ensuring designability at least\n"
            },
            {
                "rank": 3,
                "index": 7,
                "score": 0.00040290522,
                "text": "Mulligan V K Chevalier A et al 2017 Global analysis of protein folding using massively paralleldesign synthesis and testing Science 357 168175 27 Jarzab A Kurzawa N Hopf T Moerch M Zecha J Leijten N Bian Y Musiol E Maschberger M Stoehr G et al 2020 Meltome atlasthermal proteome stability across the tree of life Nat Methods 17 495503 28 Zarin T Strome B Peng G Priti C20sanac I Forman Kay J D and Moses A M 2021 Identifying molecular features that are associatedwith biological function of intrinsically disordered protein regions Elife10 e60220 29 Steinegger M and So ding J 2017 MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets Nat Biotechnol 35 10261028 ll Brief Report 8Cell Systems 15 19 March 20 2024Please\n"
            },
            {
                "rank": 4,
                "index": 8,
                "score": 0.0003029734,
                "text": "a helical protein designed from first principles Science 241 976978 2 Arnold F H 2019 Innovation by evolution bringing new chemistry to life Nobel lecture Angew Chem Int Ed Engl 58 1442014426 3 Gordley R M Bugaj L J and Lim W A 2016 Modular engineering of cellular signaling proteins and networks Curr Opin Struct Biol 39 106114 4 Pan X and Kortemme T 2021 Recent advances in de novo protein design principles methods and applications J Biol Chem 296 100558 5 Jumper J Evans R Pritzel A Green T Figurnov M Ronneberger O Tunyasuvunakool K Bates R C20Zdek A Potapenko A et al 2021 Highly accurate protein structure prediction with AlphaFold Na ture 596 583589 6 Baek M DiMaio F Anishchenko I Dauparas J Ovchinnikov S Lee\n"
            },
            {
                "rank": 5,
                "index": 9,
                "score": 0.0003029734,
                "text": "a helical protein designed from first principles Science 241 976978 2 Arnold F H 2019 Innovation by evolution bringing new chemistry to life Nobel lecture Angew Chem Int Ed Engl 58 1442014426 3 Gordley R M Bugaj L J and Lim W A 2016 Modular engineering of cellular signaling proteins and networks Curr Opin Struct Biol 39 106114 4 Pan X and Kortemme T 2021 Recent advances in de novo protein design principles methods and applications J Biol Chem 296 100558 5 Jumper J Evans R Pritzel A Green T Figurnov M Ronneberger O Tunyasuvunakool K Bates R C20Zdek A Potapenko A et al 2021 Highly accurate protein structure prediction with AlphaFold Na ture 596 583589 6 Baek M DiMaio F Anishchenko I Dauparas J Ovchinnikov S Lee\n"
            },
            {
                "rank": 6,
                "index": 2,
                "score": 0.00024536948,
                "text": "K L Saar M Vendruscolo and T P J Knowles New frontiers for machine learning in protein science J Mol Biol 433 167232 2021 G Masrati M Landau N Ben Tal A Lupas M Kosloff and J Kosinski Integrative structural biology in the era of accurate structure prediction J Mol Biol 433 167127 2021 M Andrec D A Snyder Z Zhou J Young G T Montelione and R M Levy A large data set comparison of protein structures determined by crystallography and nmr Statistical test for structural differences and the effect of crystal packing Proteins 69 449 2007 A A Rashin A H L Rashin and R L Jernigan Protein Flexibility Coordinate Uncertainties and Interpretation of Structural Differences Acta Crystallogr D 65 1140 2009 B Venkatakrishnan M L\n"
            },
            {
                "rank": 7,
                "index": 1,
                "score": 8.029784e-05,
                "text": "G Akdel M Andreeva A Bateman A et al 2023 Uncovering New Families and Folds in the Natural Protein Uni verse Nature 622 646653 180 Chen J Fruhauf A Fan C Ponce J Ueberheide B Bhabha G and Ekiert D C 2023 Structure of an endogenous mycobacterial MCE lipidtransporter Nature 620 445452 181 Webb B Viswanath S Bonomi M Pellarin R Greenberg C H Saltzberg D and Sali A 2018 Integrative structure modeling withthe Integrative Modeling Platform Protein Sci 27 245258 182 Rantos V Karius K and Kosinski J 2022 Integrative structural modeling of macromolecular complexes using Assembline Nat Protoc 17 152176 183 Koukos P I and Bonvin A M J J 2020 Integrative Modelling of Biomolecular Complexes J Mol Biol 432 28612881 184 Watanabe R Buschauer R\n"
            },
            {
                "rank": 8,
                "index": 6,
                "score": 1.1478768e-05,
                "text": "biology Trends Biochem Sci 40 4957 2015 3 Su X D et al Protein crystallography from the perspective of technology developments Crystallogr Rev 21 1 2 2 153 2014 4 Wthrich K Protein structure determination in solution by NMR spectroscopy J Biol Chem 265 22059 22062 1990 5 Slabinski L et al The challenge of protein structure determinationlessons from structural genomics Protein Sci 16 2472 2482 2007 6 Leach A Thomas P Protein structure prediction and homology modeling In Comprehensive Medicinal Chemistry III 1 2 0 144 Elsevier 2017 https doi org 10 1016 b978 0 12 409547 2 12350 9 Table 1 Summary of parameters used for enhanced sampling molecular dynamics simulations of Abl1 inactivation Box dimensions 112 4 124 3 118 0 Total number of atoms\n"
            },
            {
                "rank": 9,
                "index": 3,
                "score": 6.8009767e-06,
                "text": "97 32883291 56 Sutto L S Marsili F L Gervasio 2015 From residue coevolution to protein conformational ensembles and functional dynamics Proc Natl Acad Sci USA 112 1356713572 57 Knighton D R J H Zheng J M Sowadski 1991 Crystal structure of the catalytic subunit of cyclic adenosine monophosphate depen dent protein kinase Science 253 407414 58 Azam M M A Seeliger G Q Daley 2008 Activation of tyrosine kinases by mutation of the gatekeeper threonine Nat Struct Mol Biol 15 11091118 59 Halabi N O Rivoire R Ranganathan 2009 Protein sectors evolutionary units of three dimensional structure Cell 138 774786 60 McLaughlin R N Jr F J Poelwijk R Ranganathan 2012 The spatial architecture of protein function and adaptation Nature 491 138142 61 Cocco S and R\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -2.1339640617370605,
                "text": "more structurallydiverse structures with deviations from canonical helical geom etries Much progress has also been made with the de novo design of protein folds containing a mixture of ahelices and bstrands A typical design process follows a four step approach the first step defines a blueprint of the desired protein fold topology defined as the identity and connectivity of a helical and bstrand secondary structure elements Figure 2 B Blueprints allow for the definition of new fold topologies not found in nature 26The second step is to assemble a protein backbone from peptide frag ments helices and strands according to the blueprint and connected by short loops Figure 2 B Peptide and loop fragments are typically taken from overrepresented fragments in the PDB thus ensuring designability at least\n"
            },
            {
                "rank": 1,
                "score": -2.1339640617370605,
                "text": "more structurallydiverse structures with deviations from canonical helical geom etries Much progress has also been made with the de novo design of protein folds containing a mixture of ahelices and bstrands A typical design process follows a four step approach the first step defines a blueprint of the desired protein fold topology defined as the identity and connectivity of a helical and bstrand secondary structure elements Figure 2 B Blueprints allow for the definition of new fold topologies not found in nature 26The second step is to assemble a protein backbone from peptide frag ments helices and strands according to the blueprint and connected by short loops Figure 2 B Peptide and loop fragments are typically taken from overrepresented fragments in the PDB thus ensuring designability at least\n"
            },
            {
                "rank": 2,
                "score": -2.485264301300049,
                "text": "1990 11 Onuchic J N Wolynes P G Theory of protein folding Curr Opin Struct Biol 14 7 0 75 2004 12 Dill K A MacCallum J L The protein folding problem 50 years on Science 338 1042 1046 2012 13 Salinas V H Ranganathan R Coevolution based inference of amino acid interactions underlying protein function Elife 7 e34300 2018 14 An finsen C B Principles that govern the folding of protein chains Science 181 223 230 1973 15 Jumper J Highly accurate protein structure prediction with alphafold Nature 596 583 589 2021 16 Baek M Accurate prediction of protein structures and interactions using a three track neural network Science 373 871 876 2021 17 Pereira J High accuracy protein structure prediction in casp14 Proteins 89 1687 1699\n"
            },
            {
                "rank": 3,
                "score": -2.578484535217285,
                "text": "Mulligan V K Chevalier A et al 2017 Global analysis of protein folding using massively paralleldesign synthesis and testing Science 357 168175 27 Jarzab A Kurzawa N Hopf T Moerch M Zecha J Leijten N Bian Y Musiol E Maschberger M Stoehr G et al 2020 Meltome atlasthermal proteome stability across the tree of life Nat Methods 17 495503 28 Zarin T Strome B Peng G Priti C20sanac I Forman Kay J D and Moses A M 2021 Identifying molecular features that are associatedwith biological function of intrinsically disordered protein regions Elife10 e60220 29 Steinegger M and So ding J 2017 MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets Nat Biotechnol 35 10261028 ll Brief Report 8Cell Systems 15 19 March 20 2024Please\n"
            },
            {
                "rank": 4,
                "score": -3.3864433765411377,
                "text": "K L Saar M Vendruscolo and T P J Knowles New frontiers for machine learning in protein science J Mol Biol 433 167232 2021 G Masrati M Landau N Ben Tal A Lupas M Kosloff and J Kosinski Integrative structural biology in the era of accurate structure prediction J Mol Biol 433 167127 2021 M Andrec D A Snyder Z Zhou J Young G T Montelione and R M Levy A large data set comparison of protein structures determined by crystallography and nmr Statistical test for structural differences and the effect of crystal packing Proteins 69 449 2007 A A Rashin A H L Rashin and R L Jernigan Protein Flexibility Coordinate Uncertainties and Interpretation of Structural Differences Acta Crystallogr D 65 1140 2009 B Venkatakrishnan M L\n"
            },
            {
                "rank": 5,
                "score": -4.178176403045654,
                "text": "G Akdel M Andreeva A Bateman A et al 2023 Uncovering New Families and Folds in the Natural Protein Uni verse Nature 622 646653 180 Chen J Fruhauf A Fan C Ponce J Ueberheide B Bhabha G and Ekiert D C 2023 Structure of an endogenous mycobacterial MCE lipidtransporter Nature 620 445452 181 Webb B Viswanath S Bonomi M Pellarin R Greenberg C H Saltzberg D and Sali A 2018 Integrative structure modeling withthe Integrative Modeling Platform Protein Sci 27 245258 182 Rantos V Karius K and Kosinski J 2022 Integrative structural modeling of macromolecular complexes using Assembline Nat Protoc 17 152176 183 Koukos P I and Bonvin A M J J 2020 Integrative Modelling of Biomolecular Complexes J Mol Biol 432 28612881 184 Watanabe R Buschauer R\n"
            },
            {
                "rank": 6,
                "score": -4.793427467346191,
                "text": "a helical protein designed from first principles Science 241 976978 2 Arnold F H 2019 Innovation by evolution bringing new chemistry to life Nobel lecture Angew Chem Int Ed Engl 58 1442014426 3 Gordley R M Bugaj L J and Lim W A 2016 Modular engineering of cellular signaling proteins and networks Curr Opin Struct Biol 39 106114 4 Pan X and Kortemme T 2021 Recent advances in de novo protein design principles methods and applications J Biol Chem 296 100558 5 Jumper J Evans R Pritzel A Green T Figurnov M Ronneberger O Tunyasuvunakool K Bates R C20Zdek A Potapenko A et al 2021 Highly accurate protein structure prediction with AlphaFold Na ture 596 583589 6 Baek M DiMaio F Anishchenko I Dauparas J Ovchinnikov S Lee\n"
            },
            {
                "rank": 7,
                "score": -4.793427467346191,
                "text": "a helical protein designed from first principles Science 241 976978 2 Arnold F H 2019 Innovation by evolution bringing new chemistry to life Nobel lecture Angew Chem Int Ed Engl 58 1442014426 3 Gordley R M Bugaj L J and Lim W A 2016 Modular engineering of cellular signaling proteins and networks Curr Opin Struct Biol 39 106114 4 Pan X and Kortemme T 2021 Recent advances in de novo protein design principles methods and applications J Biol Chem 296 100558 5 Jumper J Evans R Pritzel A Green T Figurnov M Ronneberger O Tunyasuvunakool K Bates R C20Zdek A Potapenko A et al 2021 Highly accurate protein structure prediction with AlphaFold Na ture 596 583589 6 Baek M DiMaio F Anishchenko I Dauparas J Ovchinnikov S Lee\n"
            },
            {
                "rank": 8,
                "score": -4.824598789215088,
                "text": "biology Trends Biochem Sci 40 4957 2015 3 Su X D et al Protein crystallography from the perspective of technology developments Crystallogr Rev 21 1 2 2 153 2014 4 Wthrich K Protein structure determination in solution by NMR spectroscopy J Biol Chem 265 22059 22062 1990 5 Slabinski L et al The challenge of protein structure determinationlessons from structural genomics Protein Sci 16 2472 2482 2007 6 Leach A Thomas P Protein structure prediction and homology modeling In Comprehensive Medicinal Chemistry III 1 2 0 144 Elsevier 2017 https doi org 10 1016 b978 0 12 409547 2 12350 9 Table 1 Summary of parameters used for enhanced sampling molecular dynamics simulations of Abl1 inactivation Box dimensions 112 4 124 3 118 0 Total number of atoms\n"
            },
            {
                "rank": 9,
                "score": -5.4241132736206055,
                "text": "97 32883291 56 Sutto L S Marsili F L Gervasio 2015 From residue coevolution to protein conformational ensembles and functional dynamics Proc Natl Acad Sci USA 112 1356713572 57 Knighton D R J H Zheng J M Sowadski 1991 Crystal structure of the catalytic subunit of cyclic adenosine monophosphate depen dent protein kinase Science 253 407414 58 Azam M M A Seeliger G Q Daley 2008 Activation of tyrosine kinases by mutation of the gatekeeper threonine Nat Struct Mol Biol 15 11091118 59 Halabi N O Rivoire R Ranganathan 2009 Protein sectors evolutionary units of three dimensional structure Cell 138 774786 60 McLaughlin R N Jr F J Poelwijk R Ranganathan 2012 The spatial architecture of protein function and adaptation Nature 491 138142 61 Cocco S and R\n"
            }
        ]
    },
    {
        "query": "Discuss the role of genomics in the development of personalized medicine for rare genetic disorders.",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 6,
                "score": 0.00037118545,
                "text": "interface that ensures speci ficity of two component signaling pathways PLoS Genet 6 e1001220 2010 28 Capra E J Perchuk B S Skerker J M Laub M T Adaptive mutations that prevent crosstalk enable the expansion of paralogous signaling protein families Cell150 222 232 2012 29 Lite T L V Uncovering the basis of protein protein interaction speci ficity with a combinatorially complete library Elife 9 e60924 2020 30 Van Valen L A new evolutionary law Evol Theory 1 130 1973 31 Stock A M Robinson V L Goudreau P N Two component signal transduction Annu Rev Biochem 69 183 215 2000 32 Laub M T Goulian M Speci ficity in two component signal transduction pathways Annu Rev Genet 41 121 145 2007 33 Podgornaia A I Casino\n"
            },
            {
                "rank": 1,
                "index": 8,
                "score": 0.00036543285,
                "text": "missense constraint improves variant deleteriousness prediction BioRxiv page 148353 2017 Martin Kircher Daniela M Witten Preti Jain Brian J ORoak Gregory M Cooper and Jay Shendure A general framework for estimating the relative pathogenicity of human genetic variants Nature genetics 46 3 310315 2014 Karthik A Jagadeesh Aaron M Wenger Mark J Berger Harendra Guturu Peter D Stenson David N Cooper Jonathan A Bernstein and Gill Bejerano M cap eliminates a majority of variants of uncertain significance in clinical exomes at high sensitivity Nature genetics 48 12 1581 2016 13 CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The\n"
            },
            {
                "rank": 2,
                "index": 5,
                "score": 0.00020502215,
                "text": "Claes P Pritchard J K et al 2023 Precise modulation of transcription factor levels identifies features underly ing dosage sensitivity Nat Genet 55 841851 47 Qu S Tucker S C Zhao Q deCrombrugghe B and Wisdom R 1999 Physical and genetic interactions between Alx4 and Cart1 Devel opment 126 359369 48 Zalc A Sinha R Gulati G S Wesche D J Daszczuk P Swigut T Weissman I L and Wysocka J 2021 Reactivation of the pluripotencyprogram precedes formation of the cranial neural crest Science 371 eabb4776 49 Simoes Costa M and Bronner M E 2015 Establishing neural crest identity a gene regulatory recipe Development 142 242257 50 Bildsoe H Loebel D A F Jones V J Chen Y T Behringer R R andTam P P L 2009 Requirement\n"
            },
            {
                "rank": 3,
                "index": 0,
                "score": 0.00014883849,
                "text": "behaviour of random sequences Author contributions B R C and J D B conceptualized the study R B and H J A produced some cell lines and performed quality control H J A and M T M ran next generation sequencing assays and initial processing of the resulting data B R C built all synthetic constructs in yeast produced some cell lines performed all assays in yeast and mouse ES cells performed final sequencing data processing analysis and visualization and created figures B R C J D B and R B wrote the manuscript All authors reviewed and edited the manuscript Competing interests J D B is a founder and director of CDI Labs Inc a founder of and consultant to Neochromosome Inc a founder scientific advisory board\n"
            },
            {
                "rank": 4,
                "index": 2,
                "score": 0.00012148175,
                "text": "mutation filtering and annotation as well as the algorithm used to identify significant mutational drivers are available at http www uni koeln de med fak peiflyne peiflyne tgz 40 Cmero M etal Inferring structural variant cancer cell fraction Nat Commun 11 730 2020 41 Fernandez Cuesta L etal Identification of novel fusion genes in lung cancer using breakpoint assembly of transcriptome sequencing data Genome Biol 16 7 2015 42 Rosswog C etal Chromothripsis followed by circular recombination drives oncogene amplification in human cancer Nat Genet 53 16731685 2021 43 Herling C D etal Clonal dynamics towards the development of venetoclax resistance in chronic lymphocytic leukemia Nat Commun 9 727 2018 44 Gerstung M etal The evolutionary history of 2 658 cancers Nature 578 122128 2020 45 Dentro S\n"
            },
            {
                "rank": 5,
                "index": 9,
                "score": 7.4846226e-05,
                "text": "Figs 4c and 5e f Thus despite their undoubted role in SCLC2932 MYC gene amplifications are often not part of the most recent common ancestor SCLC genomes are frequently polyploid which is typically associated with inferior clinical outcome in cancer33 34 In our cohort 36 of untreated tumours n 15 of 42 exhibited with higher ploidy which had no impact on clinical response to first line therapy and clonal diver sity throughout treatment Extended Data Fig 5g However in these 42pairs of tumours obtained before and after chemotherapy tumours in eight patients exhibited events of acquired genome duplication at the time of recurrence The majority of these tumours harboured either functionally relevant HAT domain mutations6 or damaging alterations in CREBBP EP300 all of which were part of the\n"
            },
            {
                "rank": 6,
                "index": 1,
                "score": 6.922183e-05,
                "text": "19 Pinglay S etal Synthetic regulatory reconstitution reveals principles of mammalian Hox cluster regulation Science 377 eabk2820 2022 20 Brosh R etal Synthetic regulatory genomics uncovers enhancer context dependence at the Sox2 locus Mol Cell 83 11401152 e1147 2023 21 Agmon N etal Yeast golden gate yGG for the efficient assembly of S cerevisiae transcription units ACS Synth Biol 4 853859 2015 22 Szybalska E H Szybalski W Genetics of human cell line IV DNA mediated heritable transformation of a biochemical trait Proc Natl Acad Sci USA 48 20262034 1962 23 Skene P J Henikoff S An efficient targeted nuclease strategy for high resolution mapping of DNA binding sites eLife 6 e21856 2017 24 Murata M etal Detecting expressed genes using CAGE Methods Mol Biol 1164 6785 2014\n"
            },
            {
                "rank": 7,
                "index": 4,
                "score": 6.814872e-05,
                "text": "and Rijli F M 2017 Gene bivalency at Polycomb domains regu lates cranial neural crest positional identity Science 355 eaal2913 25 Harenza J L Diamond M A Adams R N Song M M Davidson H L Hart L S Dent M H Fortina P Reynolds C P and Maris J M 2017 Transcriptomic profiling of 39 commonly used neuroblastoma cell lines Sci Data 4 170033 26 Paliou C Guckelberger P Scho pflin R Heinrich V Esposito A Chiariello A M Bianco S Annunziatella C Helmuth J Haas S et al 2019 Preformed chromatin topology assists transcriptionalrobustness of Shh during limb development Proc Natl Acad Sci USA 116 1239012399 27 Andrey G Scho pflin R Jerkovi C19c I Heinrich V Ibrahim D M Paliou C Hochradel M Timmermann B\n"
            },
            {
                "rank": 8,
                "index": 3,
                "score": 3.7931623e-05,
                "text": "evolutionary fate of denovo genes Trends Genet 31 215219 2015 54 Lander E S etal Initial sequencing and analysis of the human genome Nature 409 860921 2001 55 Zhao Z Zhang F Sequence context analysis in the mouse genome single nucleotide polymorphisms and CpG island sequences Genomics 87 6874 2006 56 Galupa R etal Enhancer architecture and chromatin accessibility constrain phenotypic space during Drosophila development Dev Cell 58 5162 e54 2023 57 Pich O etal Somatic and germline mutation periodicity follow the orientation of the DNA minor groove around nucleosomes Cell 175 10741087 e1018 2018 58 Hon C C etal An atlas of human long non coding RNAs with accurate 5 ends Nature 543 199204 2017 59 Iyer M K etal The landscape of long noncoding RNAs in\n"
            },
            {
                "rank": 9,
                "index": 7,
                "score": 1.7097478e-05,
                "text": "associated with germ line variants in RUNX1 ETV6 and ANKRD26 Blood 141 15331543 15 Makishima H Bowman T V and Godley L A 2023 DDX41 associated susceptibility to myeloid neoplasms Blood 141 15441552 16 Calvo K R and Hickstein D D 2023 The spectrum of GATA2 deficiency syndrome Blood 141 15241532 17 Reilly C R and Shimamura A 2023 Predisposition to myeloid malignancies in Shwachman Diamond syndrome biological insights and clin ical advances Blood 141 15131523 18 Wang Q Dhindsa R S Carss K Harper A R Nag A Tachmazidou I Vitsios D Deevi S V V Mackay A Muthas D et al 2021 Rare variantcontribution to human disease in 281 104 UK Biobank exomes Nature597 527532 19 Backman J D Li A H Marcketta A Sun D\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -4.95230770111084,
                "text": "Figs 4c and 5e f Thus despite their undoubted role in SCLC2932 MYC gene amplifications are often not part of the most recent common ancestor SCLC genomes are frequently polyploid which is typically associated with inferior clinical outcome in cancer33 34 In our cohort 36 of untreated tumours n 15 of 42 exhibited with higher ploidy which had no impact on clinical response to first line therapy and clonal diver sity throughout treatment Extended Data Fig 5g However in these 42pairs of tumours obtained before and after chemotherapy tumours in eight patients exhibited events of acquired genome duplication at the time of recurrence The majority of these tumours harboured either functionally relevant HAT domain mutations6 or damaging alterations in CREBBP EP300 all of which were part of the\n"
            },
            {
                "rank": 1,
                "score": -5.105311393737793,
                "text": "associated with germ line variants in RUNX1 ETV6 and ANKRD26 Blood 141 15331543 15 Makishima H Bowman T V and Godley L A 2023 DDX41 associated susceptibility to myeloid neoplasms Blood 141 15441552 16 Calvo K R and Hickstein D D 2023 The spectrum of GATA2 deficiency syndrome Blood 141 15241532 17 Reilly C R and Shimamura A 2023 Predisposition to myeloid malignancies in Shwachman Diamond syndrome biological insights and clin ical advances Blood 141 15131523 18 Wang Q Dhindsa R S Carss K Harper A R Nag A Tachmazidou I Vitsios D Deevi S V V Mackay A Muthas D et al 2021 Rare variantcontribution to human disease in 281 104 UK Biobank exomes Nature597 527532 19 Backman J D Li A H Marcketta A Sun D\n"
            },
            {
                "rank": 2,
                "score": -5.271876335144043,
                "text": "19 Pinglay S etal Synthetic regulatory reconstitution reveals principles of mammalian Hox cluster regulation Science 377 eabk2820 2022 20 Brosh R etal Synthetic regulatory genomics uncovers enhancer context dependence at the Sox2 locus Mol Cell 83 11401152 e1147 2023 21 Agmon N etal Yeast golden gate yGG for the efficient assembly of S cerevisiae transcription units ACS Synth Biol 4 853859 2015 22 Szybalska E H Szybalski W Genetics of human cell line IV DNA mediated heritable transformation of a biochemical trait Proc Natl Acad Sci USA 48 20262034 1962 23 Skene P J Henikoff S An efficient targeted nuclease strategy for high resolution mapping of DNA binding sites eLife 6 e21856 2017 24 Murata M etal Detecting expressed genes using CAGE Methods Mol Biol 1164 6785 2014\n"
            },
            {
                "rank": 3,
                "score": -5.318030834197998,
                "text": "and Rijli F M 2017 Gene bivalency at Polycomb domains regu lates cranial neural crest positional identity Science 355 eaal2913 25 Harenza J L Diamond M A Adams R N Song M M Davidson H L Hart L S Dent M H Fortina P Reynolds C P and Maris J M 2017 Transcriptomic profiling of 39 commonly used neuroblastoma cell lines Sci Data 4 170033 26 Paliou C Guckelberger P Scho pflin R Heinrich V Esposito A Chiariello A M Bianco S Annunziatella C Helmuth J Haas S et al 2019 Preformed chromatin topology assists transcriptionalrobustness of Shh during limb development Proc Natl Acad Sci USA 116 1239012399 27 Andrey G Scho pflin R Jerkovi C19c I Heinrich V Ibrahim D M Paliou C Hochradel M Timmermann B\n"
            },
            {
                "rank": 4,
                "score": -5.74029541015625,
                "text": "mutation filtering and annotation as well as the algorithm used to identify significant mutational drivers are available at http www uni koeln de med fak peiflyne peiflyne tgz 40 Cmero M etal Inferring structural variant cancer cell fraction Nat Commun 11 730 2020 41 Fernandez Cuesta L etal Identification of novel fusion genes in lung cancer using breakpoint assembly of transcriptome sequencing data Genome Biol 16 7 2015 42 Rosswog C etal Chromothripsis followed by circular recombination drives oncogene amplification in human cancer Nat Genet 53 16731685 2021 43 Herling C D etal Clonal dynamics towards the development of venetoclax resistance in chronic lymphocytic leukemia Nat Commun 9 727 2018 44 Gerstung M etal The evolutionary history of 2 658 cancers Nature 578 122128 2020 45 Dentro S\n"
            },
            {
                "rank": 5,
                "score": -6.183935642242432,
                "text": "Claes P Pritchard J K et al 2023 Precise modulation of transcription factor levels identifies features underly ing dosage sensitivity Nat Genet 55 841851 47 Qu S Tucker S C Zhao Q deCrombrugghe B and Wisdom R 1999 Physical and genetic interactions between Alx4 and Cart1 Devel opment 126 359369 48 Zalc A Sinha R Gulati G S Wesche D J Daszczuk P Swigut T Weissman I L and Wysocka J 2021 Reactivation of the pluripotencyprogram precedes formation of the cranial neural crest Science 371 eabb4776 49 Simoes Costa M and Bronner M E 2015 Establishing neural crest identity a gene regulatory recipe Development 142 242257 50 Bildsoe H Loebel D A F Jones V J Chen Y T Behringer R R andTam P P L 2009 Requirement\n"
            },
            {
                "rank": 6,
                "score": -6.466976165771484,
                "text": "evolutionary fate of denovo genes Trends Genet 31 215219 2015 54 Lander E S etal Initial sequencing and analysis of the human genome Nature 409 860921 2001 55 Zhao Z Zhang F Sequence context analysis in the mouse genome single nucleotide polymorphisms and CpG island sequences Genomics 87 6874 2006 56 Galupa R etal Enhancer architecture and chromatin accessibility constrain phenotypic space during Drosophila development Dev Cell 58 5162 e54 2023 57 Pich O etal Somatic and germline mutation periodicity follow the orientation of the DNA minor groove around nucleosomes Cell 175 10741087 e1018 2018 58 Hon C C etal An atlas of human long non coding RNAs with accurate 5 ends Nature 543 199204 2017 59 Iyer M K etal The landscape of long noncoding RNAs in\n"
            },
            {
                "rank": 7,
                "score": -6.493142127990723,
                "text": "missense constraint improves variant deleteriousness prediction BioRxiv page 148353 2017 Martin Kircher Daniela M Witten Preti Jain Brian J ORoak Gregory M Cooper and Jay Shendure A general framework for estimating the relative pathogenicity of human genetic variants Nature genetics 46 3 310315 2014 Karthik A Jagadeesh Aaron M Wenger Mark J Berger Harendra Guturu Peter D Stenson David N Cooper Jonathan A Bernstein and Gill Bejerano M cap eliminates a majority of variants of uncertain significance in clinical exomes at high sensitivity Nature genetics 48 12 1581 2016 13 CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The\n"
            },
            {
                "rank": 8,
                "score": -7.157538414001465,
                "text": "behaviour of random sequences Author contributions B R C and J D B conceptualized the study R B and H J A produced some cell lines and performed quality control H J A and M T M ran next generation sequencing assays and initial processing of the resulting data B R C built all synthetic constructs in yeast produced some cell lines performed all assays in yeast and mouse ES cells performed final sequencing data processing analysis and visualization and created figures B R C J D B and R B wrote the manuscript All authors reviewed and edited the manuscript Competing interests J D B is a founder and director of CDI Labs Inc a founder of and consultant to Neochromosome Inc a founder scientific advisory board\n"
            },
            {
                "rank": 9,
                "score": -9.04207706451416,
                "text": "interface that ensures speci ficity of two component signaling pathways PLoS Genet 6 e1001220 2010 28 Capra E J Perchuk B S Skerker J M Laub M T Adaptive mutations that prevent crosstalk enable the expansion of paralogous signaling protein families Cell150 222 232 2012 29 Lite T L V Uncovering the basis of protein protein interaction speci ficity with a combinatorially complete library Elife 9 e60924 2020 30 Van Valen L A new evolutionary law Evol Theory 1 130 1973 31 Stock A M Robinson V L Goudreau P N Two component signal transduction Annu Rev Biochem 69 183 215 2000 32 Laub M T Goulian M Speci ficity in two component signal transduction pathways Annu Rev Genet 41 121 145 2007 33 Podgornaia A I Casino\n"
            }
        ]
    },
    {
        "query": "How has the use of transformer-based models transformed natural language processing over the last few years?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 2,
                "score": 0.9999319,
                "text": "many areas and we present several case studies including novel results in memorization term frequency effects on few shot performance and reducing gender bias We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics Trained models analysis code training code and training data can be found at https github com EleutherAI pythia 1 Introduction Over the past several years large transformer models have established themselves as the premier methodology for generative tasks in natural language processing Brown et al 2020 Sanh et al 2021 Chowdhery et al 2022 Beyond NLP transformers have also made big splashes as generative models in areas as diverse as text to image synthesis Ramesh et al 2022 Crowson et al 2022 Rombach et\n"
            },
            {
                "rank": 1,
                "index": 5,
                "score": 0.6856338,
                "text": "to sequence architecture that makes heavy use of self attention Radford et al a applied it to autoregressive language modeling by using a stack of Transformer decoders Since then Transformer based language models have dominated NLP achieving the state of the art in many tasks A new paradigm emerged with BERT Devlin et al 2019b and GPT 2 Radford et al b both are large Transformer lan8 guage models trained on a large amount of text where fine tuning on task specific data after pretraining on general domain data provides a significant performance gain compared to training on task specific data directly Training larger Transformers generally results in better performance and remains an active research direction GPT 3 Brown et al 2020 is the largest single Transformer language\n"
            },
            {
                "rank": 2,
                "index": 6,
                "score": 0.49657828,
                "text": "transformer language models VSP 17 have been directly fine tuned entirely removing the need for task specific architectures RNSS18 DCLT18 HR18 This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension question answering textual entailment and many others and has continued to advance based on new architectures and algorithms RSR 19 LOG 19 YDY 19 LCG 19 However a major limitation to this approach is that while the architecture is task agnostic there is still a need for task specific datasets and task specific fine tuning to achieve strong performance on a desired task typically requires fine tuning on a dataset of thousands to hundreds of thousands of examples specific to that task Removing this limitation would be desirable for several\n"
            },
            {
                "rank": 3,
                "index": 0,
                "score": 0.48198718,
                "text": "effective in tasks such as machine translation and language modeling where they have consistently outperformed other methods Vaswani et al 2017 Kenton and Toutanova 2019 Language models with billions of parameters such as GPT 3 175B parameters Brown et al and PaLM 540B parameters Chowdhery et al have achieved state of the art Equal contribution The title of this paper was not created by a transformer but we cant guarantee the same for this footnote 1arXiv 2301 13196v1 cs LG 30 Jan 2023 Looped Transformers as Programmable Computers performance on many natural language processing tasks Interestingly some of these large language models LLMs can also perform in context learning adapting to and performing a specific task on the fly based on a brief prompt and a few examples\n"
            },
            {
                "rank": 4,
                "index": 3,
                "score": 0.33372033,
                "text": "the cost of such models letting us adopt a paradigm of training single large scale models then creating more efficient versions of them for use in appropriate contexts Algorithmic progress may also naturally further increase the efficiency of such models over time similar to trends observed in image recognition and neural machine translation HB20 7 Related Work Several lines of work have focused on increasing parameter count and or computation in language models as a means to improve generative or task performance An early work scaled LSTM based language models to over a billion parameters JVS 16 One line of work straightforwardly increases the size of transformer models scaling up parameters and FLOPS per token roughly in proportion Work in this vein has successively increased model size 213\n"
            },
            {
                "rank": 5,
                "index": 9,
                "score": 0.09518112,
                "text": "al 2021 Attention based transformer models Vaswani et al 2017 have been adapted for the code domain and include CodeBERT Feng et al 2020 encoder only T5 Raffel et al 2020 encoderdecoder and GPT 3 Brown et al 2020 decoder only More recently large scale transformer based models like CodeGen Nijkamp et al 2023 and TransCoder Sun et al 2023 have shown promising results Instruction tuning has also been used to further improve performance with models like StarCoder Li et al 2023a and WizardCoder Luo et al 2023 These models can decode in an auto regressive and non auto regressive fashion Su et al 2021 Recent surveys have discussed these models in more detail Xu and Zhu 2022 Niu et al 2023 Xu et al 2022 Diffusion models\n"
            },
            {
                "rank": 6,
                "index": 1,
                "score": 0.08166611,
                "text": "in LLMs e g Chan et al 2022a Min et al 2022 Kossen et al 2023 5 3 L ANGUAGE MODELS EQUIPPED WITH LEAST SQUARES SOLVERS We now move beyond synthetic tasks and provide results on autoregressive language modeling a problem domain Transformers have revolutionized in recent years Because reverse engineering the ensuing models to the degree of our previous analyses is difficult we base our claims on performance comparisons between standard Transformers and new variants based on the mesa layer Our hypothesis is that the mesa layer will improve the in context learning and working memory capabilities of a Transformer in particular of the linear kind We further hypothesize that this in turn translates to language modeling improvements based on the high correlation between in context learning\n"
            },
            {
                "rank": 7,
                "index": 4,
                "score": 0.0079997545,
                "text": "replaced LSTMs by transformer networks and scaled the memory to billions of tokens leading to strong performance improvements More recently RETRO Borgeaud et al 2021 extended these by scaling the retrieval memory to trillions of tokens and changing the model architecture to take retrieved documents as input 3 1 4 Retrieval Augmentation with Search Engines Recently different works have proposed to train large language models to interact with a search engine by generating text queries and using the retrieved documents as additional context Nakano et al 2021 Thoppilan et al 2022 Shuster et al 2022 In the context of few shot question answering Lazaridou et al 2022 used the question to perform a search query and retrieved documents are added to the prompt of a large language model\n"
            },
            {
                "rank": 8,
                "index": 7,
                "score": 0.004591684,
                "text": "For example through the use of conditional computation large MoE models like the 1 7 trillion parameter Switch transformer Fedus et al 2021 the 1 2 Trillion parameter GLaM model Du et al 2021 and others Artetxe et al 2021 Zoph et al 2022 are able to provide a large effective model size despite using relatively fewer training and inference FLOPs However for very large models the computational benefits of routed models seems to diminish Clark et al 2022 An orthogonal approach to improving language models is to augment transformers with explicit retrieval mechanisms as done by Borgeaud et al 2021 Guu et al 2020 Lewis et al 2020 This approach effectively increases the number of data tokens seen during training by a factor of 10in Borgeaud et\n"
            },
            {
                "rank": 9,
                "index": 8,
                "score": 0.0009850083,
                "text": "11446 2021 76 Raffel C etal Exploring the limits of transfer learning with a unified text to text transformer J Mach Learn Res 21 167 2020 77 Zhang S etal OPT open pre trained transformer language models Preprint at https doi org 10 48550 arXiv 2205 01068 2022 78 Vaswani A etal Attention is all you need In 31st Conference on Neural Information Processing Systems Association of Computational Machinery 2017 79 Kaplan J etal Scaling laws for neural language models Preprint at https doi org 10 48550 arXiv 2001 08361 2020 80 Lampinen A K etal Can language models learn from explanations in context Preprint at https doi org 10 48550 arXiv 2204 02329 2022 81 Kojima T Gu S S Reid M Matsuo Y Iwasawa Y Large\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": 3.5443811416625977,
                "text": "many areas and we present several case studies including novel results in memorization term frequency effects on few shot performance and reducing gender bias We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics Trained models analysis code training code and training data can be found at https github com EleutherAI pythia 1 Introduction Over the past several years large transformer models have established themselves as the premier methodology for generative tasks in natural language processing Brown et al 2020 Sanh et al 2021 Chowdhery et al 2022 Beyond NLP transformers have also made big splashes as generative models in areas as diverse as text to image synthesis Ramesh et al 2022 Crowson et al 2022 Rombach et\n"
            },
            {
                "rank": 1,
                "score": -0.21888673305511475,
                "text": "to sequence architecture that makes heavy use of self attention Radford et al a applied it to autoregressive language modeling by using a stack of Transformer decoders Since then Transformer based language models have dominated NLP achieving the state of the art in many tasks A new paradigm emerged with BERT Devlin et al 2019b and GPT 2 Radford et al b both are large Transformer lan8 guage models trained on a large amount of text where fine tuning on task specific data after pretraining on general domain data provides a significant performance gain compared to training on task specific data directly Training larger Transformers generally results in better performance and remains an active research direction GPT 3 Brown et al 2020 is the largest single Transformer language\n"
            },
            {
                "rank": 2,
                "score": -0.47142481803894043,
                "text": "For example through the use of conditional computation large MoE models like the 1 7 trillion parameter Switch transformer Fedus et al 2021 the 1 2 Trillion parameter GLaM model Du et al 2021 and others Artetxe et al 2021 Zoph et al 2022 are able to provide a large effective model size despite using relatively fewer training and inference FLOPs However for very large models the computational benefits of routed models seems to diminish Clark et al 2022 An orthogonal approach to improving language models is to augment transformers with explicit retrieval mechanisms as done by Borgeaud et al 2021 Guu et al 2020 Lewis et al 2020 This approach effectively increases the number of data tokens seen during training by a factor of 10in Borgeaud et\n"
            },
            {
                "rank": 3,
                "score": -0.5422220826148987,
                "text": "effective in tasks such as machine translation and language modeling where they have consistently outperformed other methods Vaswani et al 2017 Kenton and Toutanova 2019 Language models with billions of parameters such as GPT 3 175B parameters Brown et al and PaLM 540B parameters Chowdhery et al have achieved state of the art Equal contribution The title of this paper was not created by a transformer but we cant guarantee the same for this footnote 1arXiv 2301 13196v1 cs LG 30 Jan 2023 Looped Transformers as Programmable Computers performance on many natural language processing tasks Interestingly some of these large language models LLMs can also perform in context learning adapting to and performing a specific task on the fly based on a brief prompt and a few examples\n"
            },
            {
                "rank": 4,
                "score": -0.7963447570800781,
                "text": "in LLMs e g Chan et al 2022a Min et al 2022 Kossen et al 2023 5 3 L ANGUAGE MODELS EQUIPPED WITH LEAST SQUARES SOLVERS We now move beyond synthetic tasks and provide results on autoregressive language modeling a problem domain Transformers have revolutionized in recent years Because reverse engineering the ensuing models to the degree of our previous analyses is difficult we base our claims on performance comparisons between standard Transformers and new variants based on the mesa layer Our hypothesis is that the mesa layer will improve the in context learning and working memory capabilities of a Transformer in particular of the linear kind We further hypothesize that this in turn translates to language modeling improvements based on the high correlation between in context learning\n"
            },
            {
                "rank": 5,
                "score": -1.6816763877868652,
                "text": "transformer language models VSP 17 have been directly fine tuned entirely removing the need for task specific architectures RNSS18 DCLT18 HR18 This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension question answering textual entailment and many others and has continued to advance based on new architectures and algorithms RSR 19 LOG 19 YDY 19 LCG 19 However a major limitation to this approach is that while the architecture is task agnostic there is still a need for task specific datasets and task specific fine tuning to achieve strong performance on a desired task typically requires fine tuning on a dataset of thousands to hundreds of thousands of examples specific to that task Removing this limitation would be desirable for several\n"
            },
            {
                "rank": 6,
                "score": -1.698775291442871,
                "text": "the cost of such models letting us adopt a paradigm of training single large scale models then creating more efficient versions of them for use in appropriate contexts Algorithmic progress may also naturally further increase the efficiency of such models over time similar to trends observed in image recognition and neural machine translation HB20 7 Related Work Several lines of work have focused on increasing parameter count and or computation in language models as a means to improve generative or task performance An early work scaled LSTM based language models to over a billion parameters JVS 16 One line of work straightforwardly increases the size of transformer models scaling up parameters and FLOPS per token roughly in proportion Work in this vein has successively increased model size 213\n"
            },
            {
                "rank": 7,
                "score": -1.92807936668396,
                "text": "al 2021 Attention based transformer models Vaswani et al 2017 have been adapted for the code domain and include CodeBERT Feng et al 2020 encoder only T5 Raffel et al 2020 encoderdecoder and GPT 3 Brown et al 2020 decoder only More recently large scale transformer based models like CodeGen Nijkamp et al 2023 and TransCoder Sun et al 2023 have shown promising results Instruction tuning has also been used to further improve performance with models like StarCoder Li et al 2023a and WizardCoder Luo et al 2023 These models can decode in an auto regressive and non auto regressive fashion Su et al 2021 Recent surveys have discussed these models in more detail Xu and Zhu 2022 Niu et al 2023 Xu et al 2022 Diffusion models\n"
            },
            {
                "rank": 8,
                "score": -2.0246188640594482,
                "text": "replaced LSTMs by transformer networks and scaled the memory to billions of tokens leading to strong performance improvements More recently RETRO Borgeaud et al 2021 extended these by scaling the retrieval memory to trillions of tokens and changing the model architecture to take retrieved documents as input 3 1 4 Retrieval Augmentation with Search Engines Recently different works have proposed to train large language models to interact with a search engine by generating text queries and using the retrieved documents as additional context Nakano et al 2021 Thoppilan et al 2022 Shuster et al 2022 In the context of few shot question answering Lazaridou et al 2022 used the question to perform a search query and retrieved documents are added to the prompt of a large language model\n"
            },
            {
                "rank": 9,
                "score": -3.007268190383911,
                "text": "11446 2021 76 Raffel C etal Exploring the limits of transfer learning with a unified text to text transformer J Mach Learn Res 21 167 2020 77 Zhang S etal OPT open pre trained transformer language models Preprint at https doi org 10 48550 arXiv 2205 01068 2022 78 Vaswani A etal Attention is all you need In 31st Conference on Neural Information Processing Systems Association of Computational Machinery 2017 79 Kaplan J etal Scaling laws for neural language models Preprint at https doi org 10 48550 arXiv 2001 08361 2020 80 Lampinen A K etal Can language models learn from explanations in context Preprint at https doi org 10 48550 arXiv 2204 02329 2022 81 Kojima T Gu S S Reid M Matsuo Y Iwasawa Y Large\n"
            }
        ]
    },
    {
        "query": "Analyze the impact of recent advancements in machine learning on the field of information retrieval.",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 6,
                "score": 0.12852514,
                "text": "M J Silva and F Martins editors Advances inInformation Retrieval pages 359366 Cham 2020 Springer International Publishing ISBN 978 3 030 45442 5 J Steinhardt Updates and lessons from AI forecasting 2021 URL https bounded regret g host io ai forecasting N Stiennon L Ouyang J Wu D Ziegler R Lowe C Voss A Radford D Amodei andP F Christiano Learning to summarize with human feedback In H Larochelle M Ranzato R Hadsell M F Balcan and H Lin editors Advances inNeuralInformation Processing Systems volume 33 pages 30083021 Curran Associates Inc 2020 URL https proceedings neurips cc paper 2020 file 1f89885d556929e98d3ef9b86448f951 Paper pdf S Sun K Krishna A Mattarella Micke and M Iyyer Do long range language models actually use long range context arXivpreprintarXiv 2109 09115 2021 J\n"
            },
            {
                "rank": 1,
                "index": 0,
                "score": 0.11047115,
                "text": "traditional non neural IR approaches and more recent neural methods tend to perform well on different segments of queries depending on whether they focus on lexical or semantic matching Figure 24 plots a few of these models based on their per query NDCG values on a test set 8 Conclusion We present a tutorial on neural methods for information retrieval For machine learning researchers who may be less familiar with IR tasks we introduced the fundamentals of traditional IR models and metrics For IR researchers we summarized key concepts related to representation learning with shallow or deep neural networks Finally we presented some of the recent neural methods for document ranking and question answer matching 16It is important to emphasize that while Mitra et al and others have\n"
            },
            {
                "rank": 2,
                "index": 5,
                "score": 0.08269734,
                "text": "50 95 99 232 can be investigated in the retrieval context Similarly new methods for training deep models for NLPe g using reinforcement learning 163 224 and generative adversarial networks GANs may carry over to the IR setup However given the pace at which the area of deep learning is growing in terms of the number of new architectures and training regimes we should be wary of the combinatorial explosion of trying every model on every IR task We should not disproportionately focus on maximizing quantitative improvements and in the process neglect theoretical understanding and qualitative insights It would 38 be a bad outcome for the field if these explorations do not grow our understanding of the fundamental principles of machine learning and information retrieval Neural models should\n"
            },
            {
                "rank": 3,
                "index": 1,
                "score": 0.009268014,
                "text": "Analysis and Application to Information Retrieval arXiv preprint arXiv 1502 06922 2015 Liang Pang Yanyan Lan Jiafeng Guo Jun Xu and Xueqi Cheng 2016 A study of matchpyramid models on ad hoc retrieval arXiv preprint arXiv 1606 04648 2016 Liang Pang Yanyan Lan Jiafeng Guo Jun Xu Shengxian Wan and Xueqi Cheng 2016 Text Matching as Image Recognition In Proc AAAI Greg Pass Abdur Chowdhury and Cayley Torgeson 2006 A picture of search In Proc InfoScale ACM Jeffrey Pennington Richard Socher and Christopher D Manning 2014 Glove Global vectors for word representation Proc EMNLP 12 2014 15321543 47 Jay M Ponte and W Bruce Croft 1998 A language modeling approach to information retrieval InProc SIGIR ACM 275281 Pranav Rajpurkar Jian Zhang Konstantin Lopyrev and Percy Liang 2016 Squad\n"
            },
            {
                "rank": 4,
                "index": 9,
                "score": 0.0036076168,
                "text": "ideal model therefore may also need to learn to be like a librarian with incomplete domain knowledge but capable of reading documents related to the current query and reasoning about the meaning of the query as part of the retrieval process Many of the breakthroughs in deep learning have been motivated by the needs of specific application areas Convolutional neural networks for example are particularly popular with the vision community whereas recurrent architectures find more applications in speech recognition and NLP It is likely that the specific needs and challenges of IR tasks may motivate novel neural architectures and methods Future IR explorations may also be motivated by developments in related areas such as NLP For example neural architectures that have been evaluated on non IR tasks 39\n"
            },
            {
                "rank": 5,
                "index": 8,
                "score": 0.0030634168,
                "text": "et al 2019 fact checking Thorne et al 2018 dialogue Dinan et al 2019 or citation recommendation Petroni et al 2022 Historically this information retrieval step was implemented using term matching methods such as TF IDF or BM25 Jones 1972 Robertson et al 1995 For open domain question answering Voorhees 1999 documents are often retrieved from Wikipedia Chen et al 2017 Recently dense retrievers based on neural networks have become popular These usually follow a dual encoder architecture Yih et al 2011 Huang et al 2013 Shen et al 2014 where queries and passages are encoded independently as vectors and relevance is computed using the inner product or Euclidean distance Popular supervised retrievers include DPR Karpukhin et al 2020 which is trained to discriminate the relevant passage among\n"
            },
            {
                "rank": 6,
                "index": 3,
                "score": 0.0016809417,
                "text": "Search engines typically involve large multi tier architectures and the retrieval process generally consists of multiple stages of pruning the candidate set of documents The IR model at the bottom of this telescoping setup may need to sift through billions of documentswhile the model at the top may only need to re rank between tens of promising documents The retrieval approaches that are suitable at one level of the stack may be highly impractical at a different stepmodels at the bottom need to be fastbut mostly focus on eliminating irrelevant or junk results while models at the top tend to develop more sophisticated notions of relevance and focus on distinguishing between documents that are much closer on the relevance scale So far much of the focus on neural\n"
            },
            {
                "rank": 7,
                "index": 7,
                "score": 0.0007947255,
                "text": "W Bruce Croft Jiafeng Guo Bhaskar Mitra and Maarten de Rijke 2016 Neu IR The SIGIR 2016 Workshop on Neural Information Retrieval 2016 Nick Craswell W Bruce Croft Jiafeng Guo Bhaskar Mitra and Maarten de Rijke 2016 Report on the SIGIR 2016 Workshop on Neural Information Retrieval Neu IR ACM Sigir forum 50 2 2016 96103 41 Nick Craswell David Hawking Ross Wilkinson and Mingfang Wu 2003 Overview of the TREC 2002 Web track In TREC V ol 3 12th Nick Craswell Rosie Jones Georges Dupret and Evelyne Viegas 2009 Proceedings of the 2009 workshop on Web Search Click Data ACM Ferdinand De Saussure Wade Baskin and Perry Meisel 2011 Course in general linguistics Columbia University Press Scott C Deerwester Susan T Dumais Thomas K Landauer George W\n"
            },
            {
                "rank": 8,
                "index": 4,
                "score": 0.00031999825,
                "text": "Nemanja Djuric Vladan Radosavljevic and Narayan Bhamidipati 2015 Search Retargeting using Directed Query Embeddings In Proc WWW International World Wide Web Conferences Steering Committee 3738 Mihajlo Grbovic Nemanja Djuric Vladan Radosavljevic Fabrizio Silvestri and Narayan Bhamidipati 2015 Context and Content aware Embeddings for Query Rewriting in Sponsored Search In Proc SIGIR ACM 383392 Zhiwei Guan and Edward Cutrell 2007 An eye tracking study of the effect of target rank on web search In Proceedings of the SIGCHI conference on Human factors in computing systems ACM 417420 Jiafeng Guo Yixing Fan Qingyao Ai and W Bruce Croft 2016 A Deep Relevance Matching Model for Ad hoc Retrieval In Proc CIKM ACM 5564 Jiafeng Guo Yixing Fan Qingyao Ai and W Bruce Croft 2016 Semantic Matching by Non Linear Word\n"
            },
            {
                "rank": 9,
                "index": 2,
                "score": 0.0001896174,
                "text": "model fine tuning Furthermore our method also benefits state of the art dense retrievers in terms of both in domain and out of domain results 1 Introduction Information retrieval IR aims to locate relevant documents from a large corpus given a user issued query It is a core component in modern search engines and researchers have invested for decades in this field There are two mainstream paradigms for IR lexical based sparse retrieval such as BM25 and embedding based dense retrieval Xiong et al 2021 Qu et al 2021 Although dense retrievers perform better when large amounts of labeled data are available Karpukhin et al 2020 BM25 remains competitive on out ofdomain datasets Thakur et al 2021 Query expansion Rocchio 1971 Lavrenko and Croft 2001 is a long\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": 0.5413350462913513,
                "text": "50 95 99 232 can be investigated in the retrieval context Similarly new methods for training deep models for NLPe g using reinforcement learning 163 224 and generative adversarial networks GANs may carry over to the IR setup However given the pace at which the area of deep learning is growing in terms of the number of new architectures and training regimes we should be wary of the combinatorial explosion of trying every model on every IR task We should not disproportionately focus on maximizing quantitative improvements and in the process neglect theoretical understanding and qualitative insights It would 38 be a bad outcome for the field if these explorations do not grow our understanding of the fundamental principles of machine learning and information retrieval Neural models should\n"
            },
            {
                "rank": 1,
                "score": -0.005044810473918915,
                "text": "traditional non neural IR approaches and more recent neural methods tend to perform well on different segments of queries depending on whether they focus on lexical or semantic matching Figure 24 plots a few of these models based on their per query NDCG values on a test set 8 Conclusion We present a tutorial on neural methods for information retrieval For machine learning researchers who may be less familiar with IR tasks we introduced the fundamentals of traditional IR models and metrics For IR researchers we summarized key concepts related to representation learning with shallow or deep neural networks Finally we presented some of the recent neural methods for document ranking and question answer matching 16It is important to emphasize that while Mitra et al and others have\n"
            },
            {
                "rank": 2,
                "score": -0.2939867377281189,
                "text": "M J Silva and F Martins editors Advances inInformation Retrieval pages 359366 Cham 2020 Springer International Publishing ISBN 978 3 030 45442 5 J Steinhardt Updates and lessons from AI forecasting 2021 URL https bounded regret g host io ai forecasting N Stiennon L Ouyang J Wu D Ziegler R Lowe C Voss A Radford D Amodei andP F Christiano Learning to summarize with human feedback In H Larochelle M Ranzato R Hadsell M F Balcan and H Lin editors Advances inNeuralInformation Processing Systems volume 33 pages 30083021 Curran Associates Inc 2020 URL https proceedings neurips cc paper 2020 file 1f89885d556929e98d3ef9b86448f951 Paper pdf S Sun K Krishna A Mattarella Micke and M Iyyer Do long range language models actually use long range context arXivpreprintarXiv 2109 09115 2021 J\n"
            },
            {
                "rank": 3,
                "score": -0.6017676591873169,
                "text": "et al 2019 fact checking Thorne et al 2018 dialogue Dinan et al 2019 or citation recommendation Petroni et al 2022 Historically this information retrieval step was implemented using term matching methods such as TF IDF or BM25 Jones 1972 Robertson et al 1995 For open domain question answering Voorhees 1999 documents are often retrieved from Wikipedia Chen et al 2017 Recently dense retrievers based on neural networks have become popular These usually follow a dual encoder architecture Yih et al 2011 Huang et al 2013 Shen et al 2014 where queries and passages are encoded independently as vectors and relevance is computed using the inner product or Euclidean distance Popular supervised retrievers include DPR Karpukhin et al 2020 which is trained to discriminate the relevant passage among\n"
            },
            {
                "rank": 4,
                "score": -0.635674238204956,
                "text": "model fine tuning Furthermore our method also benefits state of the art dense retrievers in terms of both in domain and out of domain results 1 Introduction Information retrieval IR aims to locate relevant documents from a large corpus given a user issued query It is a core component in modern search engines and researchers have invested for decades in this field There are two mainstream paradigms for IR lexical based sparse retrieval such as BM25 and embedding based dense retrieval Xiong et al 2021 Qu et al 2021 Although dense retrievers perform better when large amounts of labeled data are available Karpukhin et al 2020 BM25 remains competitive on out ofdomain datasets Thakur et al 2021 Query expansion Rocchio 1971 Lavrenko and Croft 2001 is a long\n"
            },
            {
                "rank": 5,
                "score": -1.7593473196029663,
                "text": "Analysis and Application to Information Retrieval arXiv preprint arXiv 1502 06922 2015 Liang Pang Yanyan Lan Jiafeng Guo Jun Xu and Xueqi Cheng 2016 A study of matchpyramid models on ad hoc retrieval arXiv preprint arXiv 1606 04648 2016 Liang Pang Yanyan Lan Jiafeng Guo Jun Xu Shengxian Wan and Xueqi Cheng 2016 Text Matching as Image Recognition In Proc AAAI Greg Pass Abdur Chowdhury and Cayley Torgeson 2006 A picture of search In Proc InfoScale ACM Jeffrey Pennington Richard Socher and Christopher D Manning 2014 Glove Global vectors for word representation Proc EMNLP 12 2014 15321543 47 Jay M Ponte and W Bruce Croft 1998 A language modeling approach to information retrieval InProc SIGIR ACM 275281 Pranav Rajpurkar Jian Zhang Konstantin Lopyrev and Percy Liang 2016 Squad\n"
            },
            {
                "rank": 6,
                "score": -1.8953341245651245,
                "text": "ideal model therefore may also need to learn to be like a librarian with incomplete domain knowledge but capable of reading documents related to the current query and reasoning about the meaning of the query as part of the retrieval process Many of the breakthroughs in deep learning have been motivated by the needs of specific application areas Convolutional neural networks for example are particularly popular with the vision community whereas recurrent architectures find more applications in speech recognition and NLP It is likely that the specific needs and challenges of IR tasks may motivate novel neural architectures and methods Future IR explorations may also be motivated by developments in related areas such as NLP For example neural architectures that have been evaluated on non IR tasks 39\n"
            },
            {
                "rank": 7,
                "score": -2.6441075801849365,
                "text": "Search engines typically involve large multi tier architectures and the retrieval process generally consists of multiple stages of pruning the candidate set of documents The IR model at the bottom of this telescoping setup may need to sift through billions of documentswhile the model at the top may only need to re rank between tens of promising documents The retrieval approaches that are suitable at one level of the stack may be highly impractical at a different stepmodels at the bottom need to be fastbut mostly focus on eliminating irrelevant or junk results while models at the top tend to develop more sophisticated notions of relevance and focus on distinguishing between documents that are much closer on the relevance scale So far much of the focus on neural\n"
            },
            {
                "rank": 8,
                "score": -3.468482494354248,
                "text": "W Bruce Croft Jiafeng Guo Bhaskar Mitra and Maarten de Rijke 2016 Neu IR The SIGIR 2016 Workshop on Neural Information Retrieval 2016 Nick Craswell W Bruce Croft Jiafeng Guo Bhaskar Mitra and Maarten de Rijke 2016 Report on the SIGIR 2016 Workshop on Neural Information Retrieval Neu IR ACM Sigir forum 50 2 2016 96103 41 Nick Craswell David Hawking Ross Wilkinson and Mingfang Wu 2003 Overview of the TREC 2002 Web track In TREC V ol 3 12th Nick Craswell Rosie Jones Georges Dupret and Evelyne Viegas 2009 Proceedings of the 2009 workshop on Web Search Click Data ACM Ferdinand De Saussure Wade Baskin and Perry Meisel 2011 Course in general linguistics Columbia University Press Scott C Deerwester Susan T Dumais Thomas K Landauer George W\n"
            },
            {
                "rank": 9,
                "score": -4.636465072631836,
                "text": "Nemanja Djuric Vladan Radosavljevic and Narayan Bhamidipati 2015 Search Retargeting using Directed Query Embeddings In Proc WWW International World Wide Web Conferences Steering Committee 3738 Mihajlo Grbovic Nemanja Djuric Vladan Radosavljevic Fabrizio Silvestri and Narayan Bhamidipati 2015 Context and Content aware Embeddings for Query Rewriting in Sponsored Search In Proc SIGIR ACM 383392 Zhiwei Guan and Edward Cutrell 2007 An eye tracking study of the effect of target rank on web search In Proceedings of the SIGCHI conference on Human factors in computing systems ACM 417420 Jiafeng Guo Yixing Fan Qingyao Ai and W Bruce Croft 2016 A Deep Relevance Matching Model for Ad hoc Retrieval In Proc CIKM ACM 5564 Jiafeng Guo Yixing Fan Qingyao Ai and W Bruce Croft 2016 Semantic Matching by Non Linear Word\n"
            }
        ]
    },
    {
        "query": "What are the limitations of using embeddings for document similarity assessment in large-scale search engines?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 1,
                "score": 0.045015533,
                "text": "Mitra et al demonstrate good performances on re ranking tasks may not be indicative how the model would perform if the retrieval involves larger document collections 5 2 Query expansion Instead of comparing the query and the document directly in the embedding space an alternative approach is to use term embeddings to find good expansion candidates from a global vocabulary and then retrieving documents using the expanded query Different functions 51 170 227 have been proposed for estimating the relevance of candidate terms to the queryall of them involves comparing the candidate term individually to every query term using their vector representations and then aggregating the scores For example 51 170 estimate the relevance of candidate term tcas score tc q 1 q tqqcos vtc vtq 48 25\n"
            },
            {
                "rank": 1,
                "index": 3,
                "score": 0.037116908,
                "text": "parameterized and implemented as additional layers of the neural network as in Most of these models have been evaluated on the short text matching task but Mitra et al recently reported meaningful performances on the long document ranking task from models like DSSM and CDSSM Mitra et al also show that sampling the negative documents uniformly from the collection is less effective to using documents that are closer to the query intent but judged as non relelvant by human annotators Notions of similarity It is important to emphasize that our earlier discussion in Section 4 2 on different notions of similarity between terms that can be learnt by shallow embedding models is also relevant in the context of these deeper architectures In the case of Siamese networks such\n"
            },
            {
                "rank": 2,
                "index": 0,
                "score": 0.016979992,
                "text": "embeddings These approaches can be broadly categorized as those that compare the query with the document directly in the embedding space and those that use embeddings to generate suitable query expansion candidates from a global vocabulary and then perform retrieval based on the expanded query We discuss both these classes of approaches in the remainder of this section 5 1 Query document matching A popular strategy for using term embeddings in IR involves deriving a dense vector representation for the query and the document from the embeddings of the individual terms in the corresponding texts The term embeddings can be aggregated in different ways although using the average word or term embeddings AWE is quite popular 96 101 110 143 151 190 207 Non linear combinations of term\n"
            },
            {
                "rank": 3,
                "index": 4,
                "score": 0.0040385914,
                "text": "the last several years dual encoders Gillick et al 2018 Karpukhin et al 2020 Ni et al 2022b Chen et al 2022 have dominated the landscape for first stage information retrieval They model relevance by mapping queries and documents into the same embedding space optimized via contrastive learning Hadsell et al 2006 Gao et al Equal Contribution Work completed while a Student Researcher at Google 2021 Dense embeddings are pre computed for all documents in a corpus and stored in an external index This allows for fast approximate nearest neighbor search Vanderkam et al 2013 Johnson et al 2021 to retrieve relevant documents Cross encoders based on large Transformer models Nogueira et al 2019b 2020 Pradeep et al 2021b often function on top of these retrieved documents to\n"
            },
            {
                "rank": 4,
                "index": 7,
                "score": 0.0034162789,
                "text": "when the retrieval is performed over the full document collection However as seen in the example of Figure 14 the errors made by embedding based models and exact matching models are typically differentand the combination of the two performs better than exact matching models alone 4 58 143 Another popular technique is to use the embedding based model to re rank only a subset of the documents retrieved by a different generally an exact matching basedIR model The chaining of different IR models where each successive model re ranks a smaller number of candidate documents is called Telescoping Telescoping evaluations are popular in the neural IR literature 71 88 141 143 177 and the results are representative of performances of these models on re ranking tasks However as\n"
            },
            {
                "rank": 5,
                "index": 9,
                "score": 0.0007069056,
                "text": "replaced LSTMs by transformer networks and scaled the memory to billions of tokens leading to strong performance improvements More recently RETRO Borgeaud et al 2021 extended these by scaling the retrieval memory to trillions of tokens and changing the model architecture to take retrieved documents as input 3 1 4 Retrieval Augmentation with Search Engines Recently different works have proposed to train large language models to interact with a search engine by generating text queries and using the retrieved documents as additional context Nakano et al 2021 Thoppilan et al 2022 Shuster et al 2022 In the context of few shot question answering Lazaridou et al 2022 used the question to perform a search query and retrieved documents are added to the prompt of a large language model\n"
            },
            {
                "rank": 6,
                "index": 6,
                "score": 0.0002082501,
                "text": "et al have recently emphasized the importance of modelling lexical matches using deep neural networks Mitra et al argue that Web search is a tale of two queries For the query pekarovic land company it is easier to estimate relevance based on patterns of exact matches of the rare term pekarovic On the other hand a neural model focused on matching in the embedding space is unlikely to have a good representation for this rare term In contrast for the query what channel are the seahawks on today the target document likely contains ESPN or Sky Sportsnot the term channel A representation learning neural model can associate occurrences of ESPN in the document as positive evidence towards the document being relevant to the query Figure 22 highlights the\n"
            },
            {
                "rank": 7,
                "index": 2,
                "score": 1.0783312e-05,
                "text": "b Nie et al 2019 Min et al 2019a Wolfson et al 2020 Augmenting text based retrieval with external structured information such as knowledge graph and Wikipedia hyperlinks has also been explored recently Min et al 2019b Asai et al 2020 The use of dense vector representations for retrieval has a long history since Latent Semantic Analysis Deerwester et al 1990 Using labeled pairs of queries and documents discriminatively trained dense encoders have become popular recently Yih et al 2011 Huang et al 2013 Gillick et al 2019 with applications to cross lingual document retrieval ad relevance prediction Web search and entity retrieval Such approaches complement the sparse vector methods as they can potentially give high similarity scores to semantically relevant text pairs even without exact token matching\n"
            },
            {
                "rank": 8,
                "index": 5,
                "score": 1.0209441e-05,
                "text": "due to better generalization across termsalthough some counter evidence the claim of better performances from embedding models have also been reported in the literature The sparse feature spaces of Section 4 3 are easier to visualize and leads to more intuitive explanationswhile their corresponding embeddings are more practically useful Therefore it makes sense to think sparse but act dense in many scenarios In the rest of this section we will describe some of the popular neural and non neural embedding models Latent Semantic Analysis LSA LSA involves performing singular value decomposition SVD on a term document or term passage matrix Xto obtain its low rank approximation 18 SVD onXinvolves finding a solution to X UVT whereUandVare orthogonal matrices andis a diagonal matrix 10 X U V dj dj\n"
            },
            {
                "rank": 9,
                "index": 8,
                "score": 2.1068543e-06,
                "text": "queries directly to relevant document identifiers Metzler et al 2021 Differentiable Search Indexes DSI Tay et al 2022 first demonstrated the potential of this paradigm where T5 is used to parameterize an end to end search system with the model parameters encoding all information about the corpus See Section 3 for more information DSI was shown to outperform a dual encoder baseline on Natural Questions dataset Kwiatkowski et al 2019 Zhuang et al 2022b explores theeffectiveness of DSI and synthetic queries on a 100k passage subset of the MS MARCO passage ranking corpus and XOR QA Asai et al 2021 Neural Corpus Indexer Wang et al 2022 builds on the success of DSI and introduces a combination of more input variants and architectural additions some of which we\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -0.880471408367157,
                "text": "embeddings These approaches can be broadly categorized as those that compare the query with the document directly in the embedding space and those that use embeddings to generate suitable query expansion candidates from a global vocabulary and then perform retrieval based on the expanded query We discuss both these classes of approaches in the remainder of this section 5 1 Query document matching A popular strategy for using term embeddings in IR involves deriving a dense vector representation for the query and the document from the embeddings of the individual terms in the corresponding texts The term embeddings can be aggregated in different ways although using the average word or term embeddings AWE is quite popular 96 101 110 143 151 190 207 Non linear combinations of term\n"
            },
            {
                "rank": 1,
                "score": -0.96839439868927,
                "text": "parameterized and implemented as additional layers of the neural network as in Most of these models have been evaluated on the short text matching task but Mitra et al recently reported meaningful performances on the long document ranking task from models like DSSM and CDSSM Mitra et al also show that sampling the negative documents uniformly from the collection is less effective to using documents that are closer to the query intent but judged as non relelvant by human annotators Notions of similarity It is important to emphasize that our earlier discussion in Section 4 2 on different notions of similarity between terms that can be learnt by shallow embedding models is also relevant in the context of these deeper architectures In the case of Siamese networks such\n"
            },
            {
                "rank": 2,
                "score": -1.6689796447753906,
                "text": "Mitra et al demonstrate good performances on re ranking tasks may not be indicative how the model would perform if the retrieval involves larger document collections 5 2 Query expansion Instead of comparing the query and the document directly in the embedding space an alternative approach is to use term embeddings to find good expansion candidates from a global vocabulary and then retrieving documents using the expanded query Different functions 51 170 227 have been proposed for estimating the relevance of candidate terms to the queryall of them involves comparing the candidate term individually to every query term using their vector representations and then aggregating the scores For example 51 170 estimate the relevance of candidate term tcas score tc q 1 q tqqcos vtc vtq 48 25\n"
            },
            {
                "rank": 3,
                "score": -2.0610036849975586,
                "text": "the last several years dual encoders Gillick et al 2018 Karpukhin et al 2020 Ni et al 2022b Chen et al 2022 have dominated the landscape for first stage information retrieval They model relevance by mapping queries and documents into the same embedding space optimized via contrastive learning Hadsell et al 2006 Gao et al Equal Contribution Work completed while a Student Researcher at Google 2021 Dense embeddings are pre computed for all documents in a corpus and stored in an external index This allows for fast approximate nearest neighbor search Vanderkam et al 2013 Johnson et al 2021 to retrieve relevant documents Cross encoders based on large Transformer models Nogueira et al 2019b 2020 Pradeep et al 2021b often function on top of these retrieved documents to\n"
            },
            {
                "rank": 4,
                "score": -2.159465789794922,
                "text": "when the retrieval is performed over the full document collection However as seen in the example of Figure 14 the errors made by embedding based models and exact matching models are typically differentand the combination of the two performs better than exact matching models alone 4 58 143 Another popular technique is to use the embedding based model to re rank only a subset of the documents retrieved by a different generally an exact matching basedIR model The chaining of different IR models where each successive model re ranks a smaller number of candidate documents is called Telescoping Telescoping evaluations are popular in the neural IR literature 71 88 141 143 177 and the results are representative of performances of these models on re ranking tasks However as\n"
            },
            {
                "rank": 5,
                "score": -2.6840505599975586,
                "text": "due to better generalization across termsalthough some counter evidence the claim of better performances from embedding models have also been reported in the literature The sparse feature spaces of Section 4 3 are easier to visualize and leads to more intuitive explanationswhile their corresponding embeddings are more practically useful Therefore it makes sense to think sparse but act dense in many scenarios In the rest of this section we will describe some of the popular neural and non neural embedding models Latent Semantic Analysis LSA LSA involves performing singular value decomposition SVD on a term document or term passage matrix Xto obtain its low rank approximation 18 SVD onXinvolves finding a solution to X UVT whereUandVare orthogonal matrices andis a diagonal matrix 10 X U V dj dj\n"
            },
            {
                "rank": 6,
                "score": -3.733276844024658,
                "text": "et al have recently emphasized the importance of modelling lexical matches using deep neural networks Mitra et al argue that Web search is a tale of two queries For the query pekarovic land company it is easier to estimate relevance based on patterns of exact matches of the rare term pekarovic On the other hand a neural model focused on matching in the embedding space is unlikely to have a good representation for this rare term In contrast for the query what channel are the seahawks on today the target document likely contains ESPN or Sky Sportsnot the term channel A representation learning neural model can associate occurrences of ESPN in the document as positive evidence towards the document being relevant to the query Figure 22 highlights the\n"
            },
            {
                "rank": 7,
                "score": -4.076330661773682,
                "text": "b Nie et al 2019 Min et al 2019a Wolfson et al 2020 Augmenting text based retrieval with external structured information such as knowledge graph and Wikipedia hyperlinks has also been explored recently Min et al 2019b Asai et al 2020 The use of dense vector representations for retrieval has a long history since Latent Semantic Analysis Deerwester et al 1990 Using labeled pairs of queries and documents discriminatively trained dense encoders have become popular recently Yih et al 2011 Huang et al 2013 Gillick et al 2019 with applications to cross lingual document retrieval ad relevance prediction Web search and entity retrieval Such approaches complement the sparse vector methods as they can potentially give high similarity scores to semantically relevant text pairs even without exact token matching\n"
            },
            {
                "rank": 8,
                "score": -4.2677764892578125,
                "text": "replaced LSTMs by transformer networks and scaled the memory to billions of tokens leading to strong performance improvements More recently RETRO Borgeaud et al 2021 extended these by scaling the retrieval memory to trillions of tokens and changing the model architecture to take retrieved documents as input 3 1 4 Retrieval Augmentation with Search Engines Recently different works have proposed to train large language models to interact with a search engine by generating text queries and using the retrieved documents as additional context Nakano et al 2021 Thoppilan et al 2022 Shuster et al 2022 In the context of few shot question answering Lazaridou et al 2022 used the question to perform a search query and retrieved documents are added to the prompt of a large language model\n"
            },
            {
                "rank": 9,
                "score": -5.6400275230407715,
                "text": "queries directly to relevant document identifiers Metzler et al 2021 Differentiable Search Indexes DSI Tay et al 2022 first demonstrated the potential of this paradigm where T5 is used to parameterize an end to end search system with the model parameters encoding all information about the corpus See Section 3 for more information DSI was shown to outperform a dual encoder baseline on Natural Questions dataset Kwiatkowski et al 2019 Zhuang et al 2022b explores theeffectiveness of DSI and synthetic queries on a 100k passage subset of the MS MARCO passage ranking corpus and XOR QA Asai et al 2021 Neural Corpus Indexer Wang et al 2022 builds on the success of DSI and introduces a combination of more input variants and architectural additions some of which we\n"
            }
        ]
    },
    {
        "query": "How do factoid questions differ from open-ended and hypothetical questions in terms of information retrieval?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 4,
                "score": 0.2295352,
                "text": "verify that in the context of open domain question answering a higher retrieval precision indeed translates to a higher end to end QA accuracy By applying a modern reader model to the top retrieved passages we achieve comparable or better results on multiple QA datasets in the open retrieval setting compared to several much complicated systems 2 Background The problem of open domain QA studied in this paper can be described as follows Given a factoid question such as Who first voiced Meg on Family Guy or Where was the 8th Dalai Lama born a system is required to answer it using a large corpus of diversified topics More specifically we assumethe extractive QA setting in which the answer is restricted to a span appearing in one or\n"
            },
            {
                "rank": 1,
                "index": 0,
                "score": 0.0010649627,
                "text": "the answer options to a smaller range In a further 26 of cases a passage contained all the necessary information to answer the question stated in a straightforward way If read competently such passages make the question simple to answer and often include information such as 6 Note Depending on the question it may not be important or useful to retrieve the exact text of the answer in MMLU and as such a hits k value of 30 does not imply that retrieval fails to surface useful information in 70 of cases 23 Izacard Lewis Lomeli Hosseini Petroni Schick Dwivedi Yu Joulin Riedel Grave canonical definitions or the exact numerical answer requested in the question 28 of retrieval sets did not contain obvious information which would make the\n"
            },
            {
                "rank": 2,
                "index": 5,
                "score": 0.0005976694,
                "text": "10 true and for the model to find the specifics of that true thing An example of an assertive query could be Tell me why Maitland was not at the party instead of Was Maitland at the party Participants found the model more helpful for factual questions than questions of interpretation The more a question was open to interpretation the more it seemed that the model would hallucinate things that didnt exist or latch on to very weak concepts in the text This category includes questions like What was the storys message why wouldnt you want to live on this storys planet and so on These quantitative and qualitative results make it clear that the model under study here has significant room to improve in both its abstract\n"
            },
            {
                "rank": 3,
                "index": 2,
                "score": 0.00039666123,
                "text": "than depth Do not assume a user has given a complete answer to any question so make sure to keep probing different types of interests learn under what conditions the user would believe it is or is not ethical to steal a loaf of bread learn what rules auserbelievesavalid emailaddressformat mustadhereto e g for developingaregexformatchecker Previous questions Elicitation transcript Generate the most informative yes no question open ended question that when answered will reveal the most about the desired behavior beyond what has already been queried for above Make sure your question addresses different aspects of the implementation than the questions that have already been asked At the same time however the question should be bite sized and not ask for too much at once Phrase your question\n"
            },
            {
                "rank": 4,
                "index": 1,
                "score": 0.00013552078,
                "text": "to For example one could emphasize relevance versus irrelevance instead An instance of such an approach is given in the prompt variant in Figure 15 which we will also use in our experiments 3 Experiments We conduct experiments to evaluate S2A in three settings factual question answering and longform generation of arguments following Sharma et al 2023 and math word problem solving from Shi et al 2023 3 1 Tasks Experimental Setup 3 1 1 Factual QA Evaluation We use the modified version of TriviaQA from SycophancyEval1which poses straightforward fact seeking questions with additional opinions in the prompt see e g Figure 4 In particular in addition to asking the question one of three possible comments is added to the prompt given by the following templated sentences Suggest\n"
            },
            {
                "rank": 5,
                "index": 6,
                "score": 6.605215e-05,
                "text": "one and considers both its location and dates when recommending concerts nearby during the correct week These examples motivate that IR models should have some latent representations of intent as expressed by the query and of the different topics in the document textso that inexact matching can be performed that goes beyond lexical term counting Robustness to rare inputs Query frequencies in most IR setups follow a Zipfian distribution see Figure 2 In the publicly available AOL query logs for example more than 70 of the distinct queries are seen only once in the period of three months from which the queries are sampled In the same dataset more than 50 of the distinct documents are clicked only once A good IR method must be able to retrieve\n"
            },
            {
                "rank": 6,
                "index": 8,
                "score": 6.553816e-05,
                "text": "well as the models that focus on representation learning We have focused on retrieval of long and short text In the case of long text the model must deal with variable length documents where the relevant sections of a document may be surrounded by irrelevant text For both long and short text but particularly for short IR models should also deal with the query document vocabulary mismatch problem by learning how patterns of query words and different document words can indicate relevance Models should also consider lexical matches when the query contains rare termssuch as a persons name or a product model numbernot seen during training and to avoid retrieving semantically related but irrelevant results An ideal model for information retrieval would be able to infer the meaning\n"
            },
            {
                "rank": 7,
                "index": 3,
                "score": 6.1088555e-05,
                "text": "Facts and so forth Noting that this information has already been obtained during the revealing phase we focus on the refinement of the solver derived reasoning process Finally we enumerate all newly implied Facts to enable the model to perform an interim review Turn 3 Query Answering Instructions In Turn 3 we present instructions for answering a given Query Following prior works Ceri et al 1989 Tafjord et al 2021 a Query can be considered true within a certain logical context if it is explicitly mentioned or if it can be implied through several Rule applications To handle negation we consider two distinct assumptions 1 the open world assumption OWA that treats any fact that cannot be provable as special truth value unknown 2 the closed world assumption\n"
            },
            {
                "rank": 8,
                "index": 7,
                "score": 3.676463e-05,
                "text": "no statistical reason an LM must hallucinate on systematic facts We will make a few assumptions about factoids 1 One per doc First we assume that there is at most one factoid per document by defining a surjective function f XYwhich extracts a single factoid with each document where f x represents the empty fact to allow for documents with no facts and assume Y This makes the notation simpler as one can define the induced factoid distribution p fDL Y defined by p y P x f x yDL x Similarly we can take g fDLM to be the induced distribution over generated factoids where DLM X is the distribution over documents generated by the LM The surjective requirement simply means that every factoid is describable by\n"
            },
            {
                "rank": 9,
                "index": 9,
                "score": 7.4694067e-06,
                "text": "informative In the case of the query what channel are the seahawks on today the query term channel implies that the IR model should pay attention to occurrences of ESPN or Sky Sports in the document textnone of which appears in the query itself Semantic understanding however goes beyond mapping query terms to document terms A good IR model may consider the terms hot and warm related as well as the terms dog and puppybut must also distinguish that a user who submits the query hot dog is not looking for a warm puppy At the more ambitious end of the spectrum semantic understanding would involve logical reasons by the IR systemso for the query concerts during SIGIR it associates a specific edition of the conference the upcoming\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -1.6025993824005127,
                "text": "verify that in the context of open domain question answering a higher retrieval precision indeed translates to a higher end to end QA accuracy By applying a modern reader model to the top retrieved passages we achieve comparable or better results on multiple QA datasets in the open retrieval setting compared to several much complicated systems 2 Background The problem of open domain QA studied in this paper can be described as follows Given a factoid question such as Who first voiced Meg on Family Guy or Where was the 8th Dalai Lama born a system is required to answer it using a large corpus of diversified topics More specifically we assumethe extractive QA setting in which the answer is restricted to a span appearing in one or\n"
            },
            {
                "rank": 1,
                "score": -2.0379505157470703,
                "text": "10 true and for the model to find the specifics of that true thing An example of an assertive query could be Tell me why Maitland was not at the party instead of Was Maitland at the party Participants found the model more helpful for factual questions than questions of interpretation The more a question was open to interpretation the more it seemed that the model would hallucinate things that didnt exist or latch on to very weak concepts in the text This category includes questions like What was the storys message why wouldnt you want to live on this storys planet and so on These quantitative and qualitative results make it clear that the model under study here has significant room to improve in both its abstract\n"
            },
            {
                "rank": 2,
                "score": -4.298925399780273,
                "text": "than depth Do not assume a user has given a complete answer to any question so make sure to keep probing different types of interests learn under what conditions the user would believe it is or is not ethical to steal a loaf of bread learn what rules auserbelievesavalid emailaddressformat mustadhereto e g for developingaregexformatchecker Previous questions Elicitation transcript Generate the most informative yes no question open ended question that when answered will reveal the most about the desired behavior beyond what has already been queried for above Make sure your question addresses different aspects of the implementation than the questions that have already been asked At the same time however the question should be bite sized and not ask for too much at once Phrase your question\n"
            },
            {
                "rank": 3,
                "score": -5.068582057952881,
                "text": "the answer options to a smaller range In a further 26 of cases a passage contained all the necessary information to answer the question stated in a straightforward way If read competently such passages make the question simple to answer and often include information such as 6 Note Depending on the question it may not be important or useful to retrieve the exact text of the answer in MMLU and as such a hits k value of 30 does not imply that retrieval fails to surface useful information in 70 of cases 23 Izacard Lewis Lomeli Hosseini Petroni Schick Dwivedi Yu Joulin Riedel Grave canonical definitions or the exact numerical answer requested in the question 28 of retrieval sets did not contain obvious information which would make the\n"
            },
            {
                "rank": 4,
                "score": -5.278025150299072,
                "text": "to For example one could emphasize relevance versus irrelevance instead An instance of such an approach is given in the prompt variant in Figure 15 which we will also use in our experiments 3 Experiments We conduct experiments to evaluate S2A in three settings factual question answering and longform generation of arguments following Sharma et al 2023 and math word problem solving from Shi et al 2023 3 1 Tasks Experimental Setup 3 1 1 Factual QA Evaluation We use the modified version of TriviaQA from SycophancyEval1which poses straightforward fact seeking questions with additional opinions in the prompt see e g Figure 4 In particular in addition to asking the question one of three possible comments is added to the prompt given by the following templated sentences Suggest\n"
            },
            {
                "rank": 5,
                "score": -5.412810325622559,
                "text": "no statistical reason an LM must hallucinate on systematic facts We will make a few assumptions about factoids 1 One per doc First we assume that there is at most one factoid per document by defining a surjective function f XYwhich extracts a single factoid with each document where f x represents the empty fact to allow for documents with no facts and assume Y This makes the notation simpler as one can define the induced factoid distribution p fDL Y defined by p y P x f x yDL x Similarly we can take g fDLM to be the induced distribution over generated factoids where DLM X is the distribution over documents generated by the LM The surjective requirement simply means that every factoid is describable by\n"
            },
            {
                "rank": 6,
                "score": -5.530791759490967,
                "text": "well as the models that focus on representation learning We have focused on retrieval of long and short text In the case of long text the model must deal with variable length documents where the relevant sections of a document may be surrounded by irrelevant text For both long and short text but particularly for short IR models should also deal with the query document vocabulary mismatch problem by learning how patterns of query words and different document words can indicate relevance Models should also consider lexical matches when the query contains rare termssuch as a persons name or a product model numbernot seen during training and to avoid retrieving semantically related but irrelevant results An ideal model for information retrieval would be able to infer the meaning\n"
            },
            {
                "rank": 7,
                "score": -5.639193534851074,
                "text": "Facts and so forth Noting that this information has already been obtained during the revealing phase we focus on the refinement of the solver derived reasoning process Finally we enumerate all newly implied Facts to enable the model to perform an interim review Turn 3 Query Answering Instructions In Turn 3 we present instructions for answering a given Query Following prior works Ceri et al 1989 Tafjord et al 2021 a Query can be considered true within a certain logical context if it is explicitly mentioned or if it can be implied through several Rule applications To handle negation we consider two distinct assumptions 1 the open world assumption OWA that treats any fact that cannot be provable as special truth value unknown 2 the closed world assumption\n"
            },
            {
                "rank": 8,
                "score": -7.087994575500488,
                "text": "one and considers both its location and dates when recommending concerts nearby during the correct week These examples motivate that IR models should have some latent representations of intent as expressed by the query and of the different topics in the document textso that inexact matching can be performed that goes beyond lexical term counting Robustness to rare inputs Query frequencies in most IR setups follow a Zipfian distribution see Figure 2 In the publicly available AOL query logs for example more than 70 of the distinct queries are seen only once in the period of three months from which the queries are sampled In the same dataset more than 50 of the distinct documents are clicked only once A good IR method must be able to retrieve\n"
            },
            {
                "rank": 9,
                "score": -7.945366859436035,
                "text": "informative In the case of the query what channel are the seahawks on today the query term channel implies that the IR model should pay attention to occurrences of ESPN or Sky Sports in the document textnone of which appears in the query itself Semantic understanding however goes beyond mapping query terms to document terms A good IR model may consider the terms hot and warm related as well as the terms dog and puppybut must also distinguish that a user who submits the query hot dog is not looking for a warm puppy At the more ambitious end of the spectrum semantic understanding would involve logical reasons by the IR systemso for the query concerts during SIGIR it associates a specific edition of the conference the upcoming\n"
            }
        ]
    },
    {
        "query": "Compare and contrast the performance of different reranking algorithms on text snippet generation tasks.",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 2,
                "score": 0.19961987,
                "text": "were shown to be effective for reranking tasks In this setup models assign scores to the topkresults from a first stage retrieval method One can then use these scores to rerank the documents For example monoT5 Nogueira et al 2020 was the first to leverage T5 as a pointwise reranker by training a model that takes the concatenation of the query and document as input and generates a relevance label Pradeep et al 2021b Zhuang et al 2022a Hui et al 2022 have since improved the performance and efficiency of generation based reranking These approaches continue to demonstrate strong effectiveness Craswell et al 2022 Pradeep et al 2021a 2022 Generative retrieval seeks to replace the entire information retrieval process with a single sequenceto sequence model capable of mapping\n"
            },
            {
                "rank": 1,
                "index": 7,
                "score": 0.04099903,
                "text": "and the second best performance is underlined Our approach ReBRAC achieves state of the art results on Gym MuJoCo AntMaze and Adroit tasks outperforming all baselines on average except SAC RND on Gym MuJoCo tasks which is slightly better Performance profiles and probability of improvement Agarwal et al 2021 in Figure 1b and Figure 1c also demonstrate that ReBRAC is competitive when compared to the algorithms that we contrast against Our method is also comparable to ensemble based approaches see Appendix C for additional comparisons 4 2 Evaluation on offline V D4RL In addition to testing ReBRAC on D4RL we evaluated its performance on V D4RL benchmark Lu et al 2022 Our motivation for doing so was the fact that scores on D4RL Gym MuJoCo tasks have saturated\n"
            },
            {
                "rank": 2,
                "index": 5,
                "score": 0.009340025,
                "text": "The cosine scheduler worked better in our experiments For the summarization tasks we found that using a noise shift Hoogeboom et al 2023 of 4 improves the Rouge L by around 5 comparing to a vanilla setting with noise shift of 1 F Text completion with repetitive prompt We present examples in Table 13 of generation with an ill composed prompt for hotel review generation The results reveal that the FT baselines tend to generate repetitive text Although sampling mitigates this issue to some extent self reinforcement of repetition still occurs during generation In contrast our model exhibits significantly less repetitive generation G Human evaluation We screen the judges using 10 random screening questions the judges pass 80 can participate our evaluation The interrater agreement assessed by Krippendorffs\n"
            },
            {
                "rank": 3,
                "index": 9,
                "score": 0.0037654138,
                "text": "model generates The answer is Most generated outputs have a consistent format of Reasoning paths The answer is X if we prompt the language model in this format 2This also means that the language model is not well calibrated and thus cannot distinguish well between correct solutions and wrong solutions which also explains why additional re rankers were trained to better judge the quality of the solutions in previous work Cobbe et al 2021 Thoppilan et al 2022 3 Published as a conference paper at ICLR 2023 sampling as commonly used for open ended text generation Radford et al 2019 Brown et al 2020 Thoppilan et al 2022 to achieve this goal One should note that self consistency can be applied only to problems where the final answer\n"
            },
            {
                "rank": 4,
                "index": 4,
                "score": 0.0008263576,
                "text": "no good solution if the generation API does not output desire d results which is common These methods will fall back to the initial ranking and due to the h igh failure rate the results are highly sensitive to input ordering These observations are not entirely surprising Existing p opular LLMs are generally not specifically pre trained or fine tuned against ranking tasks However w e next show that LLMs do have a sense of pairwise relative comparisons which is much simpler than r equiring a calibrated pointwise relevance estimation or outputting a permutation for a list of documen ts 3 P AIRWISE RANKING PROMPTING We propose pairwise ranking prompting PRP for ranking wit h LLMs We describe the basic pairwise prompting unit how it supports both generation\n"
            },
            {
                "rank": 5,
                "index": 0,
                "score": 0.00072365726,
                "text": "wide range of reasoning tasks without any additional supervision or fine tuning while still substantially improving the performance of the chain of thought prompting approach proposed in Wei et al 2022 Sampling and re ranking in language models Multiple decoding strategies for language models have been proposed in the literature e g temperature sampling Ackley et al 1985 Ficler Goldberg 2017 topksampling Fan et al 2018 Holtzman et al 2018 Radford et al 2019 nucleus sampling Holtzman et al 2020 minimum Bayes risk decoding Eikema Aziz 2020 Shi et al 2022 and typical decoding Meister et al 2022 Other work has sought to explicitly promote diversity in the decoding process Batra et al 2012 Li et al 2016 Vijayakumar et al 2018 Re ranking is another common approach\n"
            },
            {
                "rank": 6,
                "index": 1,
                "score": 0.0006190417,
                "text": "a wide range of reasoning tasks without any additional supervision or fine tuning while still substantially improving the performance of the chain of thought prompting approach proposed in Wei et al 2022 Sampling and re ranking in language models Multiple decoding strategies for language models have been proposed in the literature e g temperature sampling Ackley et al 1985 Ficler Goldberg 2017 topksampling Fan et al 2018 Holtzman et al 2018 Radford et al 2019 nucleus sampling Holtzman et al 2020 minimum Bayes risk decoding Eikema Aziz 2020 Shi et al 2022 and typical decoding Meister et al 2022 Other work has sought to explicitly promote diversity in the decoding process Batra et al 2012 Li et al 2016 Vijayakumar et al 2018 Re ranking is another common\n"
            },
            {
                "rank": 7,
                "index": 3,
                "score": 0.00010472557,
                "text": "2003 consisting of news articles from number of publishers However the task is more akin to sentence paraphrasing than summarization as only the first sentence of an article is used to predict the headline another sentence RNN based encoder decoder models with attention seq2seq perform very well on this task in both ROUGE Lin 2004 an automatic metric often used in summarization and human evaluation Chopra et al 2016 In Nallapati et al 2016 an abstractive summarization dataset is proposed by modifying a questionanswering dataset of news articles paired with story highlights from Daily Mail and CNN This task is more difficult than headline generation because the information used in the highlights may come from many parts of the article and not only the first sentence One downside\n"
            },
            {
                "rank": 8,
                "index": 8,
                "score": 8.463939e-06,
                "text": "Sadigh et al 2017 Ibarz et al 2018 Lee et al 2021b a Sikchi et al 2022 is a technique for policy optimization based on relative rather than absolute feedback Owing to the relative ease of providing comparative feedback rather than absolute scores for agent behavior for human raters Miller 1956 RLHF has been successfully applied across fields from robotics Cakmak et al 2011 Tucker et al 2020 Swamy et al 2020 Byk et al 2020 to recommendation De Gemmis et al 2009 Ailon Mohri 2010 Viappiani Boutilier 2010 Afsar et al 2022 to retrieval Yue Joachims 2009 As of late RLHF has attracted renewed interest as a leading technique for fine tuning large language models LLMs Ziegler et al 2020 Stiennon et al 2020 Bai et al\n"
            },
            {
                "rank": 9,
                "index": 6,
                "score": 1.2878952e-06,
                "text": "soup After unsupervised pre training and supervised fine tuning we launch Nindependent RL fine tunings on the proxy rewards Ri N i 1 Then we combine the trained networks by interpolation in the weight space The final weights are adapted at test time by selecting the coefficient Figure 1 b shows our results extended in Figure 2 a with LLaMA 7b instruct fine tuned on Alpaca when RL fine tuning for news summarization with N 2reward models assessing diverse preferences of summaries With only two trainings R1andR2rewarded on Figure 1 b the interpolation 01 reveals the green front of Pareto optimal solutions i e that cannot be improved for one reward without sacrificing the other RS matches the costly yellow front of multi objective MORL 45 46 requiring\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": 0.41491878032684326,
                "text": "were shown to be effective for reranking tasks In this setup models assign scores to the topkresults from a first stage retrieval method One can then use these scores to rerank the documents For example monoT5 Nogueira et al 2020 was the first to leverage T5 as a pointwise reranker by training a model that takes the concatenation of the query and document as input and generates a relevance label Pradeep et al 2021b Zhuang et al 2022a Hui et al 2022 have since improved the performance and efficiency of generation based reranking These approaches continue to demonstrate strong effectiveness Craswell et al 2022 Pradeep et al 2021a 2022 Generative retrieval seeks to replace the entire information retrieval process with a single sequenceto sequence model capable of mapping\n"
            },
            {
                "rank": 1,
                "score": -1.1214239597320557,
                "text": "and the second best performance is underlined Our approach ReBRAC achieves state of the art results on Gym MuJoCo AntMaze and Adroit tasks outperforming all baselines on average except SAC RND on Gym MuJoCo tasks which is slightly better Performance profiles and probability of improvement Agarwal et al 2021 in Figure 1b and Figure 1c also demonstrate that ReBRAC is competitive when compared to the algorithms that we contrast against Our method is also comparable to ensemble based approaches see Appendix C for additional comparisons 4 2 Evaluation on offline V D4RL In addition to testing ReBRAC on D4RL we evaluated its performance on V D4RL benchmark Lu et al 2022 Our motivation for doing so was the fact that scores on D4RL Gym MuJoCo tasks have saturated\n"
            },
            {
                "rank": 2,
                "score": -1.82924485206604,
                "text": "wide range of reasoning tasks without any additional supervision or fine tuning while still substantially improving the performance of the chain of thought prompting approach proposed in Wei et al 2022 Sampling and re ranking in language models Multiple decoding strategies for language models have been proposed in the literature e g temperature sampling Ackley et al 1985 Ficler Goldberg 2017 topksampling Fan et al 2018 Holtzman et al 2018 Radford et al 2019 nucleus sampling Holtzman et al 2020 minimum Bayes risk decoding Eikema Aziz 2020 Shi et al 2022 and typical decoding Meister et al 2022 Other work has sought to explicitly promote diversity in the decoding process Batra et al 2012 Li et al 2016 Vijayakumar et al 2018 Re ranking is another common approach\n"
            },
            {
                "rank": 3,
                "score": -2.056088447570801,
                "text": "no good solution if the generation API does not output desire d results which is common These methods will fall back to the initial ranking and due to the h igh failure rate the results are highly sensitive to input ordering These observations are not entirely surprising Existing p opular LLMs are generally not specifically pre trained or fine tuned against ranking tasks However w e next show that LLMs do have a sense of pairwise relative comparisons which is much simpler than r equiring a calibrated pointwise relevance estimation or outputting a permutation for a list of documen ts 3 P AIRWISE RANKING PROMPTING We propose pairwise ranking prompting PRP for ranking wit h LLMs We describe the basic pairwise prompting unit how it supports both generation\n"
            },
            {
                "rank": 4,
                "score": -2.150649309158325,
                "text": "a wide range of reasoning tasks without any additional supervision or fine tuning while still substantially improving the performance of the chain of thought prompting approach proposed in Wei et al 2022 Sampling and re ranking in language models Multiple decoding strategies for language models have been proposed in the literature e g temperature sampling Ackley et al 1985 Ficler Goldberg 2017 topksampling Fan et al 2018 Holtzman et al 2018 Radford et al 2019 nucleus sampling Holtzman et al 2020 minimum Bayes risk decoding Eikema Aziz 2020 Shi et al 2022 and typical decoding Meister et al 2022 Other work has sought to explicitly promote diversity in the decoding process Batra et al 2012 Li et al 2016 Vijayakumar et al 2018 Re ranking is another common\n"
            },
            {
                "rank": 5,
                "score": -2.4345507621765137,
                "text": "model generates The answer is Most generated outputs have a consistent format of Reasoning paths The answer is X if we prompt the language model in this format 2This also means that the language model is not well calibrated and thus cannot distinguish well between correct solutions and wrong solutions which also explains why additional re rankers were trained to better judge the quality of the solutions in previous work Cobbe et al 2021 Thoppilan et al 2022 3 Published as a conference paper at ICLR 2023 sampling as commonly used for open ended text generation Radford et al 2019 Brown et al 2020 Thoppilan et al 2022 to achieve this goal One should note that self consistency can be applied only to problems where the final answer\n"
            },
            {
                "rank": 6,
                "score": -2.983579635620117,
                "text": "The cosine scheduler worked better in our experiments For the summarization tasks we found that using a noise shift Hoogeboom et al 2023 of 4 improves the Rouge L by around 5 comparing to a vanilla setting with noise shift of 1 F Text completion with repetitive prompt We present examples in Table 13 of generation with an ill composed prompt for hotel review generation The results reveal that the FT baselines tend to generate repetitive text Although sampling mitigates this issue to some extent self reinforcement of repetition still occurs during generation In contrast our model exhibits significantly less repetitive generation G Human evaluation We screen the judges using 10 random screening questions the judges pass 80 can participate our evaluation The interrater agreement assessed by Krippendorffs\n"
            },
            {
                "rank": 7,
                "score": -3.497607707977295,
                "text": "2003 consisting of news articles from number of publishers However the task is more akin to sentence paraphrasing than summarization as only the first sentence of an article is used to predict the headline another sentence RNN based encoder decoder models with attention seq2seq perform very well on this task in both ROUGE Lin 2004 an automatic metric often used in summarization and human evaluation Chopra et al 2016 In Nallapati et al 2016 an abstractive summarization dataset is proposed by modifying a questionanswering dataset of news articles paired with story highlights from Daily Mail and CNN This task is more difficult than headline generation because the information used in the highlights may come from many parts of the article and not only the first sentence One downside\n"
            },
            {
                "rank": 8,
                "score": -5.80909538269043,
                "text": "Sadigh et al 2017 Ibarz et al 2018 Lee et al 2021b a Sikchi et al 2022 is a technique for policy optimization based on relative rather than absolute feedback Owing to the relative ease of providing comparative feedback rather than absolute scores for agent behavior for human raters Miller 1956 RLHF has been successfully applied across fields from robotics Cakmak et al 2011 Tucker et al 2020 Swamy et al 2020 Byk et al 2020 to recommendation De Gemmis et al 2009 Ailon Mohri 2010 Viappiani Boutilier 2010 Afsar et al 2022 to retrieval Yue Joachims 2009 As of late RLHF has attracted renewed interest as a leading technique for fine tuning large language models LLMs Ziegler et al 2020 Stiennon et al 2020 Bai et al\n"
            },
            {
                "rank": 9,
                "score": -7.493238925933838,
                "text": "soup After unsupervised pre training and supervised fine tuning we launch Nindependent RL fine tunings on the proxy rewards Ri N i 1 Then we combine the trained networks by interpolation in the weight space The final weights are adapted at test time by selecting the coefficient Figure 1 b shows our results extended in Figure 2 a with LLaMA 7b instruct fine tuned on Alpaca when RL fine tuning for news summarization with N 2reward models assessing diverse preferences of summaries With only two trainings R1andR2rewarded on Figure 1 b the interpolation 01 reveals the green front of Pareto optimal solutions i e that cannot be improved for one reward without sacrificing the other RS matches the costly yellow front of multi objective MORL 45 46 requiring\n"
            }
        ]
    },
    {
        "query": "Discuss the importance of query expansion techniques in improving search engine results for diverse topics.",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 0,
                "score": 0.19269122,
                "text": "standing technique that rewrites the query based on pseudo relevance feedback or external knowledge sources such as WordNet For sparse retrieval it can help bridge the lexical gap between the query and the documents However query expansion methods like RM3 Lavrenko and Croft 2001 Lv and Zhai 2009 have only shown limited success on populardatasets Campos et al 2016 and most state ofthe art dense retrievers do not adopt this technique In the meantime document expansion methods like doc2query Nogueira et al 2019 have proven to be effective for sparse retrieval In this paper we demonstrate the effectiveness of LLMs Brown et al 2020 as query expansion models by generating pseudo documents conditioned on few shot prompts Given that search queries are often short ambiguous or lack necessary\n"
            },
            {
                "rank": 1,
                "index": 5,
                "score": 0.08961382,
                "text": "reveals that most queries from the TREC DL trackare long tailed entity centric queries which benefit more from the exact lexical match The traditional query expansion method RM3 only marginally improves the R 1k metric Although the document expansion method docT5query achieves better numbers on the MS MARCO dev set it requires training a T5 based query generator with all the available labeled data while BM25 query2doc does not require any model fine tuning For dense retrieval the model variants that combine with query2doc also outperform the corresponding baselines on all metrics However the gain brought by query2doc tends to diminish when using intermediate pre training or knowledge distillation from cross encoder re rankers as shown by the SimLM query2doc and E5 query2doc results For zero shot out\n"
            },
            {
                "rank": 2,
                "index": 1,
                "score": 0.049405247,
                "text": "Mitra et al demonstrate good performances on re ranking tasks may not be indicative how the model would perform if the retrieval involves larger document collections 5 2 Query expansion Instead of comparing the query and the document directly in the embedding space an alternative approach is to use term embeddings to find good expansion candidates from a global vocabulary and then retrieving documents using the expanded query Different functions 51 170 227 have been proposed for estimating the relevance of candidate terms to the queryall of them involves comparing the candidate term individually to every query term using their vector representations and then aggregating the scores For example 51 170 estimate the relevance of candidate term tcas score tc q 1 q tqqcos vtc vtq 48 25\n"
            },
            {
                "rank": 3,
                "index": 2,
                "score": 0.014672035,
                "text": "embeddings These approaches can be broadly categorized as those that compare the query with the document directly in the embedding space and those that use embeddings to generate suitable query expansion candidates from a global vocabulary and then perform retrieval based on the expanded query We discuss both these classes of approaches in the remainder of this section 5 1 Query document matching A popular strategy for using term embeddings in IR involves deriving a dense vector representation for the query and the document from the embeddings of the individual terms in the corresponding texts The term embeddings can be aggregated in different ways although using the average word or term embeddings AWE is quite popular 96 101 110 143 151 190 207 Non linear combinations of term\n"
            },
            {
                "rank": 4,
                "index": 3,
                "score": 0.008156248,
                "text": "and a b query specific corpus respectively The grey circles represent individual terms in the vocabulary The white circle represents the query ocean remote sensing by averaging the embeddings of the individual terms in the query and the light grey circles correspond to good expansion terms for this query When the representations are query specific then the meaning of the terms are better disambiguated and more likely to result in the selection of good expansion terms Term embedding based query expansion on its own performs worse than pseudo relevance feedback But like the models in the previous section shows better performances when used in combination with PRF Diaz et al explored the idea of query specific term embeddings and found that they are much more effective in identifying\n"
            },
            {
                "rank": 5,
                "index": 4,
                "score": 0.0013511209,
                "text": "involves rewriting the query based on relevance feedback Lavrenko and Croft 2001 Rocchio 1971 or lexical resources such as WordNet Miller 1992 In cases where labels are not available the top k retrieved documents can serve as pseudo relevance feedback signals Lv and Zhai 2009 Liu et al fine tunes an encoder decoder model to generate contextual clues In contrast document expansion enriches the document representation by appending additional relevant terms Doc2query Nogueira et al 2019 trains a seq2seq model to predict pseudo queries based on documents and then adds generated pseudo queries to the document index Learned sparse retrieval models such as SPLADE Formal et al 2021 and uniCOIL Lin and Ma 2021 also learn document term weighting in an end to end fashion However most state\n"
            },
            {
                "rank": 6,
                "index": 6,
                "score": 0.0011424496,
                "text": "good expansion terms than a global representation see Figure 15 The local model proposed by Diaz et al incorporate relevance feedback in the process of learning the term embeddingsa set of documents is retrieved for the query and a query specific term embedding model is trained This local embedding model is then employed for identifying expansion candidates for the query for a second round of document retrieval Term embeddings have also been explored for re weighting query terms and finding relevant query re writes as well as in the context of other IR tasks such as cross lingual retrieval and entity retrieval In the next section we move on to neural network models with deeper architectures and their applications to retrieval 6 Deep neural networks Deep neural network\n"
            },
            {
                "rank": 7,
                "index": 7,
                "score": 7.1273603e-06,
                "text": "and J R Wen RocketQAv2 A joint training method for dense passage retrieval and passage re ranking In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing pages 28252835 Online and Punta Cana Dominican Republic Nov 2021 Association for Computational Linguistics D Sachan M Lewis M Joshi A Aghajanyan W t Yih J Pineau and L Zettlemoyer Improving passage retrieval with zero shot question generation In Y Goldberg Z Kozareva and Y Zhang editors Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing pages 37813797 Abu Dhabi United Arab Emirates Dec 2022 Association for Computational Linguistics doi 10 18653 v1 2022 emnlp main 249 URL https aclanthology org 2022 emnlp main 249 D S Sachan M Lewis D Yogatama L Zettlemoyer J\n"
            },
            {
                "rank": 8,
                "index": 8,
                "score": 4.936989e-06,
                "text": "loss to warm up the passage embedding space Retrieval experiments on MS MARCO Natural Question and Trivia QA datasets show that coCondenser removes the need for heavy data engineering such as augmentation synthesis or filtering as well as the need for large batch training It shows comparable performance to RocketQA a state of the art heavily engineered system using simple small batch finetuning 1 1 Introduction Building upon the advancements of pre trained language models LM Devlin et al 2019 Liu et al 2019 dense retrieval has become an effective paradigm for text retrieval Lee et al 2019 Chang et al 2020 Karpukhin et al 2020 Qu et al 2021 Recent research has however found that fine tuning dense retrievers to realize their capacity requires carefully designed fine\n"
            },
            {
                "rank": 9,
                "index": 9,
                "score": 1.4708222e-06,
                "text": "now discuss experimental details for each task 3 1 Open domain Question Answering Open domain question answering QA is an important real world application and common testbed for knowledge intensive tasks We treat questions and answers as input output text pairs x y and train RAG by directly minimizing the negative log likelihood of answers We compare RAG to the popular extractive QA paradigm 5 7 31 26 where answers are extracted spans from retrieved documents relying primarily on non parametric knowledge We also compare to Closed Book QA approaches which like RAG generate answers but which do not exploit retrieval instead relying purely on parametric knowledge We consider four popular open domain QA datasets Natural Questions NQ TriviaQA TQA WebQuestions WQ and CuratedTrec CT As CT and\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -0.467653751373291,
                "text": "standing technique that rewrites the query based on pseudo relevance feedback or external knowledge sources such as WordNet For sparse retrieval it can help bridge the lexical gap between the query and the documents However query expansion methods like RM3 Lavrenko and Croft 2001 Lv and Zhai 2009 have only shown limited success on populardatasets Campos et al 2016 and most state ofthe art dense retrievers do not adopt this technique In the meantime document expansion methods like doc2query Nogueira et al 2019 have proven to be effective for sparse retrieval In this paper we demonstrate the effectiveness of LLMs Brown et al 2020 as query expansion models by generating pseudo documents conditioned on few shot prompts Given that search queries are often short ambiguous or lack necessary\n"
            },
            {
                "rank": 1,
                "score": -2.0977871417999268,
                "text": "Mitra et al demonstrate good performances on re ranking tasks may not be indicative how the model would perform if the retrieval involves larger document collections 5 2 Query expansion Instead of comparing the query and the document directly in the embedding space an alternative approach is to use term embeddings to find good expansion candidates from a global vocabulary and then retrieving documents using the expanded query Different functions 51 170 227 have been proposed for estimating the relevance of candidate terms to the queryall of them involves comparing the candidate term individually to every query term using their vector representations and then aggregating the scores For example 51 170 estimate the relevance of candidate term tcas score tc q 1 q tqqcos vtc vtq 48 25\n"
            },
            {
                "rank": 2,
                "score": -2.3483400344848633,
                "text": "and a b query specific corpus respectively The grey circles represent individual terms in the vocabulary The white circle represents the query ocean remote sensing by averaging the embeddings of the individual terms in the query and the light grey circles correspond to good expansion terms for this query When the representations are query specific then the meaning of the terms are better disambiguated and more likely to result in the selection of good expansion terms Term embedding based query expansion on its own performs worse than pseudo relevance feedback But like the models in the previous section shows better performances when used in combination with PRF Diaz et al explored the idea of query specific term embeddings and found that they are much more effective in identifying\n"
            },
            {
                "rank": 3,
                "score": -2.6101481914520264,
                "text": "embeddings These approaches can be broadly categorized as those that compare the query with the document directly in the embedding space and those that use embeddings to generate suitable query expansion candidates from a global vocabulary and then perform retrieval based on the expanded query We discuss both these classes of approaches in the remainder of this section 5 1 Query document matching A popular strategy for using term embeddings in IR involves deriving a dense vector representation for the query and the document from the embeddings of the individual terms in the corresponding texts The term embeddings can be aggregated in different ways although using the average word or term embeddings AWE is quite popular 96 101 110 143 151 190 207 Non linear combinations of term\n"
            },
            {
                "rank": 4,
                "score": -3.569528102874756,
                "text": "reveals that most queries from the TREC DL trackare long tailed entity centric queries which benefit more from the exact lexical match The traditional query expansion method RM3 only marginally improves the R 1k metric Although the document expansion method docT5query achieves better numbers on the MS MARCO dev set it requires training a T5 based query generator with all the available labeled data while BM25 query2doc does not require any model fine tuning For dense retrieval the model variants that combine with query2doc also outperform the corresponding baselines on all metrics However the gain brought by query2doc tends to diminish when using intermediate pre training or knowledge distillation from cross encoder re rankers as shown by the SimLM query2doc and E5 query2doc results For zero shot out\n"
            },
            {
                "rank": 5,
                "score": -4.6987152099609375,
                "text": "good expansion terms than a global representation see Figure 15 The local model proposed by Diaz et al incorporate relevance feedback in the process of learning the term embeddingsa set of documents is retrieved for the query and a query specific term embedding model is trained This local embedding model is then employed for identifying expansion candidates for the query for a second round of document retrieval Term embeddings have also been explored for re weighting query terms and finding relevant query re writes as well as in the context of other IR tasks such as cross lingual retrieval and entity retrieval In the next section we move on to neural network models with deeper architectures and their applications to retrieval 6 Deep neural networks Deep neural network\n"
            },
            {
                "rank": 6,
                "score": -6.150384902954102,
                "text": "now discuss experimental details for each task 3 1 Open domain Question Answering Open domain question answering QA is an important real world application and common testbed for knowledge intensive tasks We treat questions and answers as input output text pairs x y and train RAG by directly minimizing the negative log likelihood of answers We compare RAG to the popular extractive QA paradigm 5 7 31 26 where answers are extracted spans from retrieved documents relying primarily on non parametric knowledge We also compare to Closed Book QA approaches which like RAG generate answers but which do not exploit retrieval instead relying purely on parametric knowledge We consider four popular open domain QA datasets Natural Questions NQ TriviaQA TQA WebQuestions WQ and CuratedTrec CT As CT and\n"
            },
            {
                "rank": 7,
                "score": -6.782994270324707,
                "text": "involves rewriting the query based on relevance feedback Lavrenko and Croft 2001 Rocchio 1971 or lexical resources such as WordNet Miller 1992 In cases where labels are not available the top k retrieved documents can serve as pseudo relevance feedback signals Lv and Zhai 2009 Liu et al fine tunes an encoder decoder model to generate contextual clues In contrast document expansion enriches the document representation by appending additional relevant terms Doc2query Nogueira et al 2019 trains a seq2seq model to predict pseudo queries based on documents and then adds generated pseudo queries to the document index Learned sparse retrieval models such as SPLADE Formal et al 2021 and uniCOIL Lin and Ma 2021 also learn document term weighting in an end to end fashion However most state\n"
            },
            {
                "rank": 8,
                "score": -6.903278350830078,
                "text": "loss to warm up the passage embedding space Retrieval experiments on MS MARCO Natural Question and Trivia QA datasets show that coCondenser removes the need for heavy data engineering such as augmentation synthesis or filtering as well as the need for large batch training It shows comparable performance to RocketQA a state of the art heavily engineered system using simple small batch finetuning 1 1 Introduction Building upon the advancements of pre trained language models LM Devlin et al 2019 Liu et al 2019 dense retrieval has become an effective paradigm for text retrieval Lee et al 2019 Chang et al 2020 Karpukhin et al 2020 Qu et al 2021 Recent research has however found that fine tuning dense retrievers to realize their capacity requires carefully designed fine\n"
            },
            {
                "rank": 9,
                "score": -8.444343566894531,
                "text": "and J R Wen RocketQAv2 A joint training method for dense passage retrieval and passage re ranking In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing pages 28252835 Online and Punta Cana Dominican Republic Nov 2021 Association for Computational Linguistics D Sachan M Lewis M Joshi A Aghajanyan W t Yih J Pineau and L Zettlemoyer Improving passage retrieval with zero shot question generation In Y Goldberg Z Kozareva and Y Zhang editors Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing pages 37813797 Abu Dhabi United Arab Emirates Dec 2022 Association for Computational Linguistics doi 10 18653 v1 2022 emnlp main 249 URL https aclanthology org 2022 emnlp main 249 D S Sachan M Lewis D Yogatama L Zettlemoyer J\n"
            }
        ]
    },
    {
        "query": "How do transformers handle long-term dependencies compared to other recurrent neural network architectures?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 0,
                "score": 0.7937462,
                "text": "dependencies To address this shortcoming Vaswani et al 2017 introduced the transformer architecture a seminal work that transformed RE and NLP in general The transformer is similar in structure to other sequence transduction models in that it consists of two modules an encoder module and a decoder module The encoder takes an input sequence x and produces a dense representation zthat is fed to the decoder The decoder then uses zto produce an output sequence y However the transformer was unique in its use of stacked multi head attention functions in the encoder and decoder modules In the original transformer architecture the encoder and decoder modules consisted of six identical and serially connected layers Each layer contained a multi head attention function as well as a feedforward network\n"
            },
            {
                "rank": 1,
                "index": 3,
                "score": 0.5124715,
                "text": "at the national US average of 0 385 kg CO 2e per KWh starting from feed forward models Bengio et al 2000 recurrent neural networks Elman 1990 Mikolov et al 2010 and LSTMs Hochreiter and Schmidhuber 1997 Graves 2013 More recently transformer networks based on self attention have led to important improvements especially for capturing long range dependencies Vaswani et al 2017 Radford et al 2018 Dai et al 2019 Scaling There is a long history of scaling for language models for both the model and dataset sizes Brants et al 2007 showed the benefits of using language models trained on 2 trillion tokens resulting in 300 billion n grams on the quality of machine translation While this work relied on a simple smoothing technique called Stupid Backoff\n"
            },
            {
                "rank": 2,
                "index": 9,
                "score": 0.21485612,
                "text": "The initial encoder layer is fed an input sequence x and using multi head attention it attends to all positions in that sequencethat is it learns which parts of the input sequence to focus on for a given task Next the result from the multi head attention function is sent through a feed forward network and finally to the next layer in the module Subsequent layers repeat this process using the previous layers output as their input The transformer is arguably the most impactful neural based architecture for RE and NLP as it is the primary architecture used in large pre trained language models 2 4 Large language model based RE 2019 The transformer architecture inspired the development of BERT Devlin et al 2019 a wildly effective pre\n"
            },
            {
                "rank": 3,
                "index": 1,
                "score": 0.03919513,
                "text": "used model architecture the transformer and an extremely well understood class of learning problems linear regression 2 1 T HETRANSFORMER ARCHITECTURE Transformers Vaswani et al 2017 are neural network models that map a sequence of input vectors x x1 xn to a sequence of output vectors y y1 yn Each layer in a transformer maps a matrixH l interpreted as a sequence of vectors to a sequence H l 1 To do so a transformer layer processes each column h l iofH l in parallel Here we are interested in autoregressive or decoder only transformer models in which each layer first computes a self attention ai Attention h l i WF WQ WK WV 1 WF b1 bm 2 where eachbis the response of an attention head defined\n"
            },
            {
                "rank": 4,
                "index": 6,
                "score": 0.018089995,
                "text": "et al Linear Transformer and Performer in a parameter matched setting Table 7 9 following the protocol of Tay et al Details in Appendix G 2 2 7 4 4 Long Range Dependencies As described in Chapter 6 S4 uses a principled approach to address LRDs based on the HIPPO theory of continuous memorization We note that almost all of the tasks in Section 7 3 are also considered LRD tasks in the sequence modeling literature and many were in fact invented primarily to stress tests LRDS e g sequential MNIST for RNNs and CHAPTER 7 S4 AS A GENERAL SEQUENCE MODEL 115 LRA for Transformers These datasetswhich have sequences ranging from length roughly 1000 to 16000are all very challenging for standard RNN CNN and Transformer models and\n"
            },
            {
                "rank": 5,
                "index": 8,
                "score": 0.0121003855,
                "text": "advancements in RNNs transformers and state space models Gu et al 2021 Mahto et al 2021 Smith et al 2023 Ma et al 2023 Such an explicit approach can be traced back to Leaky RNNs Mozer 1991 Jaeger 2002 Kusupati et al 2018 Tallec Ollivier 2018 which use a convex combination of old memory and updates as done in ELM using m Whereas the classic time varying memory decay mediated by implicit timescales Tallec Ollivier 2018 is known from classic gated RNNs like LSTM Hochreiter Schmidhuber 1997 and GRU Cho et al 2014 In contrast to complex gating mechanisms time varying implicit timescales or sophisticated large multi staged architectures the ELM features a much simpler recurrent cell architecture only using fixed but trainable time constants m for gating\n"
            },
            {
                "rank": 6,
                "index": 7,
                "score": 0.00930395,
                "text": "generate comprehensive summaries Shaham et al 2022 otherwise models often miss important information Note that typical transformer models apply full attention to capture token dependencies pair wise It leads to a quadratic time and space complexity w r t input length However such a complexity is prohibitive for long sequences In particular it incurs massive memory consumption during the back propagation For example a transformer model with 250M parameters consumes over 80G GPU memory when sequence length is 8k Zuo et al 2022 To address this scalability issue various approaches have been proposed to reduce the complexity One approach is sparse attention which restricts each token to attend a subset of tokens based on predefined sparsity patterns Beltagy et al 2020 Zaheer et al 2020 Ainslie et al\n"
            },
            {
                "rank": 7,
                "index": 2,
                "score": 0.0019267347,
                "text": "as T Fixup Huang et al 2020 and DeepNet Wang et al 2022 to reduce variance and stabilize training In this study we focus on the first category and propose a new architecture for Transformer models to address the drawbacks of both variants while retaining their benefits Figure 1 c provides an overview of our method Our design goal is to maintain the advantages of both variants and avoid their disadvantages by employing two residual connections In particular our ResiDual model utilizes the Pre Post LN PPLN that consists two residuals one is similar to the Pre LN to prevent the gradient vanishing issue while the other one akin to the Post LN which sustains representation diversity to avoid the representation collapse issue 2 To validate the effectiveness\n"
            },
            {
                "rank": 8,
                "index": 4,
                "score": 0.00015720345,
                "text": "feed forward neural networks such as ResNets 2 1 Considerable effort has been expended to understand 2 layer neural networks with widthtendingto 8 moreprecisely consider 2 1 with L1 wPRd aPRd and 8 The infinite width limit for Transformers is in fact very natural as it is realized by stacking an arbitrary large number of heads H 8 Hence the same questions as for 1 hidden layer neural networks may be asked for instance in the vein of Cyb89 Bar93 Problem 1 Approximation Fixd n2and consider the 1 hidden layer Transformer with multi headed self attention fH pSd1qnpSd1qndefined as fH px1 x nqiPxiH h1n j1exQhxi Khxjy Z i hVhxj where H1andpQh Kh VhqhPrHsare as for 2 7 Can one approximate in some appropriate topology any continuous and permutation\n"
            },
            {
                "rank": 9,
                "index": 5,
                "score": 2.5612821e-06,
                "text": "language models Applied Intelligence 53 7 84218435 A Fan C Gardent C Braud and A Bordes 2021 Augmenting transformers with KNN based composite memory for dialog Transactions of the Association for Computational Linguistics 9 8299 A Fan E Grave and A Joulin 2020 Reducing transformer depth on demand with structured dropout In International Conference on Learning Representations M Fathi J Pilault P L Bacon C Pal O Firat and R Goroshin 2023 Block State Transformer ArXiv 2306 09539 cs W Fedus B Zoph and N Shazeer 2021 Switch transformers Scaling to trillion parameter models with simple and efficient sparsity V Feldman 2020 Does learning require memorization a short tale about a long tail In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing pages 954959\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -0.08956687152385712,
                "text": "at the national US average of 0 385 kg CO 2e per KWh starting from feed forward models Bengio et al 2000 recurrent neural networks Elman 1990 Mikolov et al 2010 and LSTMs Hochreiter and Schmidhuber 1997 Graves 2013 More recently transformer networks based on self attention have led to important improvements especially for capturing long range dependencies Vaswani et al 2017 Radford et al 2018 Dai et al 2019 Scaling There is a long history of scaling for language models for both the model and dataset sizes Brants et al 2007 showed the benefits of using language models trained on 2 trillion tokens resulting in 300 billion n grams on the quality of machine translation While this work relied on a simple smoothing technique called Stupid Backoff\n"
            },
            {
                "rank": 1,
                "score": -1.0762653350830078,
                "text": "dependencies To address this shortcoming Vaswani et al 2017 introduced the transformer architecture a seminal work that transformed RE and NLP in general The transformer is similar in structure to other sequence transduction models in that it consists of two modules an encoder module and a decoder module The encoder takes an input sequence x and produces a dense representation zthat is fed to the decoder The decoder then uses zto produce an output sequence y However the transformer was unique in its use of stacked multi head attention functions in the encoder and decoder modules In the original transformer architecture the encoder and decoder modules consisted of six identical and serially connected layers Each layer contained a multi head attention function as well as a feedforward network\n"
            },
            {
                "rank": 2,
                "score": -1.4916486740112305,
                "text": "et al Linear Transformer and Performer in a parameter matched setting Table 7 9 following the protocol of Tay et al Details in Appendix G 2 2 7 4 4 Long Range Dependencies As described in Chapter 6 S4 uses a principled approach to address LRDs based on the HIPPO theory of continuous memorization We note that almost all of the tasks in Section 7 3 are also considered LRD tasks in the sequence modeling literature and many were in fact invented primarily to stress tests LRDS e g sequential MNIST for RNNs and CHAPTER 7 S4 AS A GENERAL SEQUENCE MODEL 115 LRA for Transformers These datasetswhich have sequences ranging from length roughly 1000 to 16000are all very challenging for standard RNN CNN and Transformer models and\n"
            },
            {
                "rank": 3,
                "score": -1.8625354766845703,
                "text": "generate comprehensive summaries Shaham et al 2022 otherwise models often miss important information Note that typical transformer models apply full attention to capture token dependencies pair wise It leads to a quadratic time and space complexity w r t input length However such a complexity is prohibitive for long sequences In particular it incurs massive memory consumption during the back propagation For example a transformer model with 250M parameters consumes over 80G GPU memory when sequence length is 8k Zuo et al 2022 To address this scalability issue various approaches have been proposed to reduce the complexity One approach is sparse attention which restricts each token to attend a subset of tokens based on predefined sparsity patterns Beltagy et al 2020 Zaheer et al 2020 Ainslie et al\n"
            },
            {
                "rank": 4,
                "score": -1.887507438659668,
                "text": "advancements in RNNs transformers and state space models Gu et al 2021 Mahto et al 2021 Smith et al 2023 Ma et al 2023 Such an explicit approach can be traced back to Leaky RNNs Mozer 1991 Jaeger 2002 Kusupati et al 2018 Tallec Ollivier 2018 which use a convex combination of old memory and updates as done in ELM using m Whereas the classic time varying memory decay mediated by implicit timescales Tallec Ollivier 2018 is known from classic gated RNNs like LSTM Hochreiter Schmidhuber 1997 and GRU Cho et al 2014 In contrast to complex gating mechanisms time varying implicit timescales or sophisticated large multi staged architectures the ELM features a much simpler recurrent cell architecture only using fixed but trainable time constants m for gating\n"
            },
            {
                "rank": 5,
                "score": -2.06148099899292,
                "text": "The initial encoder layer is fed an input sequence x and using multi head attention it attends to all positions in that sequencethat is it learns which parts of the input sequence to focus on for a given task Next the result from the multi head attention function is sent through a feed forward network and finally to the next layer in the module Subsequent layers repeat this process using the previous layers output as their input The transformer is arguably the most impactful neural based architecture for RE and NLP as it is the primary architecture used in large pre trained language models 2 4 Large language model based RE 2019 The transformer architecture inspired the development of BERT Devlin et al 2019 a wildly effective pre\n"
            },
            {
                "rank": 6,
                "score": -3.0788767337799072,
                "text": "as T Fixup Huang et al 2020 and DeepNet Wang et al 2022 to reduce variance and stabilize training In this study we focus on the first category and propose a new architecture for Transformer models to address the drawbacks of both variants while retaining their benefits Figure 1 c provides an overview of our method Our design goal is to maintain the advantages of both variants and avoid their disadvantages by employing two residual connections In particular our ResiDual model utilizes the Pre Post LN PPLN that consists two residuals one is similar to the Pre LN to prevent the gradient vanishing issue while the other one akin to the Post LN which sustains representation diversity to avoid the representation collapse issue 2 To validate the effectiveness\n"
            },
            {
                "rank": 7,
                "score": -3.276777982711792,
                "text": "used model architecture the transformer and an extremely well understood class of learning problems linear regression 2 1 T HETRANSFORMER ARCHITECTURE Transformers Vaswani et al 2017 are neural network models that map a sequence of input vectors x x1 xn to a sequence of output vectors y y1 yn Each layer in a transformer maps a matrixH l interpreted as a sequence of vectors to a sequence H l 1 To do so a transformer layer processes each column h l iofH l in parallel Here we are interested in autoregressive or decoder only transformer models in which each layer first computes a self attention ai Attention h l i WF WQ WK WV 1 WF b1 bm 2 where eachbis the response of an attention head defined\n"
            },
            {
                "rank": 8,
                "score": -4.275308609008789,
                "text": "feed forward neural networks such as ResNets 2 1 Considerable effort has been expended to understand 2 layer neural networks with widthtendingto 8 moreprecisely consider 2 1 with L1 wPRd aPRd and 8 The infinite width limit for Transformers is in fact very natural as it is realized by stacking an arbitrary large number of heads H 8 Hence the same questions as for 1 hidden layer neural networks may be asked for instance in the vein of Cyb89 Bar93 Problem 1 Approximation Fixd n2and consider the 1 hidden layer Transformer with multi headed self attention fH pSd1qnpSd1qndefined as fH px1 x nqiPxiH h1n j1exQhxi Khxjy Z i hVhxj where H1andpQh Kh VhqhPrHsare as for 2 7 Can one approximate in some appropriate topology any continuous and permutation\n"
            },
            {
                "rank": 9,
                "score": -4.311870098114014,
                "text": "language models Applied Intelligence 53 7 84218435 A Fan C Gardent C Braud and A Bordes 2021 Augmenting transformers with KNN based composite memory for dialog Transactions of the Association for Computational Linguistics 9 8299 A Fan E Grave and A Joulin 2020 Reducing transformer depth on demand with structured dropout In International Conference on Learning Representations M Fathi J Pilault P L Bacon C Pal O Firat and R Goroshin 2023 Block State Transformer ArXiv 2306 09539 cs W Fedus B Zoph and N Shazeer 2021 Switch transformers Scaling to trillion parameter models with simple and efficient sparsity V Feldman 2020 Does learning require memorization a short tale about a long tail In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing pages 954959\n"
            }
        ]
    },
    {
        "query": "Analyze the effectiveness of different retrieval methods, such as keyword-based and semantic-based approaches, in academic literature search scenarios.",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 0,
                "score": 0.018797074,
                "text": "b Nie et al 2019 Min et al 2019a Wolfson et al 2020 Augmenting text based retrieval with external structured information such as knowledge graph and Wikipedia hyperlinks has also been explored recently Min et al 2019b Asai et al 2020 The use of dense vector representations for retrieval has a long history since Latent Semantic Analysis Deerwester et al 1990 Using labeled pairs of queries and documents discriminatively trained dense encoders have become popular recently Yih et al 2011 Huang et al 2013 Gillick et al 2019 with applications to cross lingual document retrieval ad relevance prediction Web search and entity retrieval Such approaches complement the sparse vector methods as they can potentially give high similarity scores to semantically relevant text pairs even without exact token matching\n"
            },
            {
                "rank": 1,
                "index": 4,
                "score": 0.009559399,
                "text": "them on this dataset 4 2 Approaches compared Retrieval based Open QA Most existing Open QA systems answer the input question by first retrieving potentially relevant documents from a knowledge corpus and then using a reading comprehension system to extract an answer from the documents In this paradigm the knowledge is stored explicitly in the corpus We wish to compare different methods for implementing retrieval Many approaches use non learned heuristic retrieval such as sparse bag of words matching Robertson et al 2009 or entity linking on the question to select a small set of rel evant documents e g 20 These documents are typically then re ranked using a learned model but coverage may be limited by the initial heuristic retrieval step Approache s such as DrQA\n"
            },
            {
                "rank": 2,
                "index": 6,
                "score": 0.0020911063,
                "text": "Analysis and Application to Information Retrieval arXiv preprint arXiv 1502 06922 2015 Liang Pang Yanyan Lan Jiafeng Guo Jun Xu and Xueqi Cheng 2016 A study of matchpyramid models on ad hoc retrieval arXiv preprint arXiv 1606 04648 2016 Liang Pang Yanyan Lan Jiafeng Guo Jun Xu Shengxian Wan and Xueqi Cheng 2016 Text Matching as Image Recognition In Proc AAAI Greg Pass Abdur Chowdhury and Cayley Torgeson 2006 A picture of search In Proc InfoScale ACM Jeffrey Pennington Richard Socher and Christopher D Manning 2014 Glove Global vectors for word representation Proc EMNLP 12 2014 15321543 47 Jay M Ponte and W Bruce Croft 1998 A language modeling approach to information retrieval InProc SIGIR ACM 275281 Pranav Rajpurkar Jian Zhang Konstantin Lopyrev and Percy Liang 2016 Squad\n"
            },
            {
                "rank": 3,
                "index": 2,
                "score": 0.0020033333,
                "text": "or composing new responses incorporating retrieved information Both the information need and the retrieved results may use the same modality e g retrieving text documents in response to keyword queries or different ones e g image search using text queries Retrieval systems may consider user history physical location temporal changes in information or other context when ranking results They may also help users formulate their intent e g via query auto completion or query suggestion and or extract succinct summaries of results for easier inspection Neural IR refers to the application of shallow or deep neural networks to these retrieval tasks This tutorial serves as an introduction to neural methods for ranking documents in response to a query an The author is a part time PhD student at\n"
            },
            {
                "rank": 4,
                "index": 3,
                "score": 0.0011203773,
                "text": "sophisticated programs that coordinate the LM andRM for the S EARCH step These results may be pegged against the evaluation on HotPotQA in a number of concurrent papers We first compare with non retrieval approaches though our comparisons must be tentative due to variation in evaluation methodologies Si et al 2022 achieve 25 2 EM with CoT prompting With a recite and answer technique for PaLM 62B Chowdhery et al 2022 Sun et al 2022 achieve 26 5 EM Wang et al 2022b achieve 33 8 EM and 44 6 F1 when applying a self consistency prompt for PaLM 540B Next we compare with a contemporaneous retrieval based approach Yao et al 2022 achieve 35 1 EM using a system capable of searching using a Wikipedia API All\n"
            },
            {
                "rank": 5,
                "index": 5,
                "score": 0.0007041516,
                "text": "D Lopes R G Wu Y Michalewski H Saurous R A Sohl Dickstein J et al Language model cascades arXiv preprint arXiv 2207 10342 2022 Fox E A and Shaw J A Combination of multiple searches NIST special publication SP 243 1994 Gao L Dai Z Pasupat P Chen A Chaganty A T Fan Y Zhao V Y Lao N Lee H Juan D C et al Attributed text generation via post hoc research and revision arXiv preprint arXiv 2210 08726 2022 Geva M Khashabi D Segal E Khot T Roth D and Berant J Did aristotle use a laptop a question answering DEMONSTRATE SEARCH PREDICT Composing retrieval and language models benchmark with implicit reasoning strategies Transactions of the Association for Computational Linguistics 9 346361 2021 Hofsttter S\n"
            },
            {
                "rank": 6,
                "index": 8,
                "score": 0.0003175088,
                "text": "D EMONSTRATE the S EARCH stage allows us to simulate many such strategies and many others in terms of passing queries passages and demonstrations between theRM andLM More importantly SEARCH facilitates our vision of advanced strategies in which the LMandRM cooperate to incrementally plan a research path for which the RM gathers information and the LMidentifies next steps Case Study Let us build on our running multi hop example as a case study We can define multihop_search_v2 Line 4 in our core program a slightly more advanced version of the SEARCH transformation from Figure 2 This transformation simulates the iterative retrieval component of fine tuned retrieval augmented systems like IRRR Qi et al 2020 which reads a retrieved passage in every hop and generates a search query or\n"
            },
            {
                "rank": 7,
                "index": 7,
                "score": 0.00012632042,
                "text": "et al 2019 fact checking Thorne et al 2018 dialogue Dinan et al 2019 or citation recommendation Petroni et al 2022 Historically this information retrieval step was implemented using term matching methods such as TF IDF or BM25 Jones 1972 Robertson et al 1995 For open domain question answering Voorhees 1999 documents are often retrieved from Wikipedia Chen et al 2017 Recently dense retrievers based on neural networks have become popular These usually follow a dual encoder architecture Yih et al 2011 Huang et al 2013 Shen et al 2014 where queries and passages are encoded independently as vectors and relevance is computed using the inner product or Euclidean distance Popular supervised retrievers include DPR Karpukhin et al 2020 which is trained to discriminate the relevant passage among\n"
            },
            {
                "rank": 8,
                "index": 1,
                "score": 5.1845658e-05,
                "text": "States b Semantic model Figure 22 Analysis of term importance for estimating the relevance of a passage to the query United States President by a lexical and a semantic deep neural network model The lexical model only considers the matches of the query terms in the document but gives more emphasis to earlier occurrences The semantic model is able to extract evidence of relevance from related terms such as Obama and federal vthings to do in london vlondon vnew york vnew york tourist attractions 54 vuniversity of washington vseattle vdenver vuniversity of colorado 55 vnew york vnewspaper vnew york times 56 By modelling different notions of similarity these deep neural models tend to be more suitable for other IR tasks such as query auto completion or session based\n"
            },
            {
                "rank": 9,
                "index": 9,
                "score": 3.120191e-05,
                "text": "useful for evaluating the quality of content or meaning Reiter and Belz 2009 Reiter 2023 because human assessment often fails to predict performance on downstream applications Kunz et al 2022 In machine translation concrete downstream objectives have formed the basis for a number of metrics Snover et al 2006 examined the extent to which participants had to edit machine generated text to match a reference reflecting the desiderata of human AI collaboration settings Other metrics rely on the users ability to accomplish specific tasks using model output such as answering reading comprehension questions based on translations Scarton and Specia 2016 or summaries Wang et al 2020 In general the best evaluations are likely to rely on realistic assessment of what LLMs enable humans to do In modern LLMs\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -0.3601456582546234,
                "text": "them on this dataset 4 2 Approaches compared Retrieval based Open QA Most existing Open QA systems answer the input question by first retrieving potentially relevant documents from a knowledge corpus and then using a reading comprehension system to extract an answer from the documents In this paradigm the knowledge is stored explicitly in the corpus We wish to compare different methods for implementing retrieval Many approaches use non learned heuristic retrieval such as sparse bag of words matching Robertson et al 2009 or entity linking on the question to select a small set of rel evant documents e g 20 These documents are typically then re ranked using a learned model but coverage may be limited by the initial heuristic retrieval step Approache s such as DrQA\n"
            },
            {
                "rank": 1,
                "score": -1.2539622783660889,
                "text": "sophisticated programs that coordinate the LM andRM for the S EARCH step These results may be pegged against the evaluation on HotPotQA in a number of concurrent papers We first compare with non retrieval approaches though our comparisons must be tentative due to variation in evaluation methodologies Si et al 2022 achieve 25 2 EM with CoT prompting With a recite and answer technique for PaLM 62B Chowdhery et al 2022 Sun et al 2022 achieve 26 5 EM Wang et al 2022b achieve 33 8 EM and 44 6 F1 when applying a self consistency prompt for PaLM 540B Next we compare with a contemporaneous retrieval based approach Yao et al 2022 achieve 35 1 EM using a system capable of searching using a Wikipedia API All\n"
            },
            {
                "rank": 2,
                "score": -1.7896496057510376,
                "text": "et al 2019 fact checking Thorne et al 2018 dialogue Dinan et al 2019 or citation recommendation Petroni et al 2022 Historically this information retrieval step was implemented using term matching methods such as TF IDF or BM25 Jones 1972 Robertson et al 1995 For open domain question answering Voorhees 1999 documents are often retrieved from Wikipedia Chen et al 2017 Recently dense retrievers based on neural networks have become popular These usually follow a dual encoder architecture Yih et al 2011 Huang et al 2013 Shen et al 2014 where queries and passages are encoded independently as vectors and relevance is computed using the inner product or Euclidean distance Popular supervised retrievers include DPR Karpukhin et al 2020 which is trained to discriminate the relevant passage among\n"
            },
            {
                "rank": 3,
                "score": -1.8667054176330566,
                "text": "b Nie et al 2019 Min et al 2019a Wolfson et al 2020 Augmenting text based retrieval with external structured information such as knowledge graph and Wikipedia hyperlinks has also been explored recently Min et al 2019b Asai et al 2020 The use of dense vector representations for retrieval has a long history since Latent Semantic Analysis Deerwester et al 1990 Using labeled pairs of queries and documents discriminatively trained dense encoders have become popular recently Yih et al 2011 Huang et al 2013 Gillick et al 2019 with applications to cross lingual document retrieval ad relevance prediction Web search and entity retrieval Such approaches complement the sparse vector methods as they can potentially give high similarity scores to semantically relevant text pairs even without exact token matching\n"
            },
            {
                "rank": 4,
                "score": -2.0012941360473633,
                "text": "Analysis and Application to Information Retrieval arXiv preprint arXiv 1502 06922 2015 Liang Pang Yanyan Lan Jiafeng Guo Jun Xu and Xueqi Cheng 2016 A study of matchpyramid models on ad hoc retrieval arXiv preprint arXiv 1606 04648 2016 Liang Pang Yanyan Lan Jiafeng Guo Jun Xu Shengxian Wan and Xueqi Cheng 2016 Text Matching as Image Recognition In Proc AAAI Greg Pass Abdur Chowdhury and Cayley Torgeson 2006 A picture of search In Proc InfoScale ACM Jeffrey Pennington Richard Socher and Christopher D Manning 2014 Glove Global vectors for word representation Proc EMNLP 12 2014 15321543 47 Jay M Ponte and W Bruce Croft 1998 A language modeling approach to information retrieval InProc SIGIR ACM 275281 Pranav Rajpurkar Jian Zhang Konstantin Lopyrev and Percy Liang 2016 Squad\n"
            },
            {
                "rank": 5,
                "score": -2.4024910926818848,
                "text": "D Lopes R G Wu Y Michalewski H Saurous R A Sohl Dickstein J et al Language model cascades arXiv preprint arXiv 2207 10342 2022 Fox E A and Shaw J A Combination of multiple searches NIST special publication SP 243 1994 Gao L Dai Z Pasupat P Chen A Chaganty A T Fan Y Zhao V Y Lao N Lee H Juan D C et al Attributed text generation via post hoc research and revision arXiv preprint arXiv 2210 08726 2022 Geva M Khashabi D Segal E Khot T Roth D and Berant J Did aristotle use a laptop a question answering DEMONSTRATE SEARCH PREDICT Composing retrieval and language models benchmark with implicit reasoning strategies Transactions of the Association for Computational Linguistics 9 346361 2021 Hofsttter S\n"
            },
            {
                "rank": 6,
                "score": -2.624460220336914,
                "text": "or composing new responses incorporating retrieved information Both the information need and the retrieved results may use the same modality e g retrieving text documents in response to keyword queries or different ones e g image search using text queries Retrieval systems may consider user history physical location temporal changes in information or other context when ranking results They may also help users formulate their intent e g via query auto completion or query suggestion and or extract succinct summaries of results for easier inspection Neural IR refers to the application of shallow or deep neural networks to these retrieval tasks This tutorial serves as an introduction to neural methods for ranking documents in response to a query an The author is a part time PhD student at\n"
            },
            {
                "rank": 7,
                "score": -2.799515724182129,
                "text": "D EMONSTRATE the S EARCH stage allows us to simulate many such strategies and many others in terms of passing queries passages and demonstrations between theRM andLM More importantly SEARCH facilitates our vision of advanced strategies in which the LMandRM cooperate to incrementally plan a research path for which the RM gathers information and the LMidentifies next steps Case Study Let us build on our running multi hop example as a case study We can define multihop_search_v2 Line 4 in our core program a slightly more advanced version of the SEARCH transformation from Figure 2 This transformation simulates the iterative retrieval component of fine tuned retrieval augmented systems like IRRR Qi et al 2020 which reads a retrieved passage in every hop and generates a search query or\n"
            },
            {
                "rank": 8,
                "score": -4.0319952964782715,
                "text": "States b Semantic model Figure 22 Analysis of term importance for estimating the relevance of a passage to the query United States President by a lexical and a semantic deep neural network model The lexical model only considers the matches of the query terms in the document but gives more emphasis to earlier occurrences The semantic model is able to extract evidence of relevance from related terms such as Obama and federal vthings to do in london vlondon vnew york vnew york tourist attractions 54 vuniversity of washington vseattle vdenver vuniversity of colorado 55 vnew york vnewspaper vnew york times 56 By modelling different notions of similarity these deep neural models tend to be more suitable for other IR tasks such as query auto completion or session based\n"
            },
            {
                "rank": 9,
                "score": -8.126188278198242,
                "text": "useful for evaluating the quality of content or meaning Reiter and Belz 2009 Reiter 2023 because human assessment often fails to predict performance on downstream applications Kunz et al 2022 In machine translation concrete downstream objectives have formed the basis for a number of metrics Snover et al 2006 examined the extent to which participants had to edit machine generated text to match a reference reflecting the desiderata of human AI collaboration settings Other metrics rely on the users ability to accomplish specific tasks using model output such as answering reading comprehension questions based on translations Scarton and Specia 2016 or summaries Wang et al 2020 In general the best evaluations are likely to rely on realistic assessment of what LLMs enable humans to do In modern LLMs\n"
            }
        ]
    },
    {
        "query": "Discuss the implications of using deep learning models for biological language modeling.",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 0,
                "score": 0.9968885,
                "text": "using a language model and deep learning Nature Biotechnology 40 16171623 2022 11 Wu R et al High resolution de novo structure prediction from primary sequence BioRxiv 202207 2022 12 Brandes N Goldman G Wang C H Ye C J Ntranos V Genome wide prediction of disease variant effects with a deep protein language model Nature Genetics 55 15121522 2023 13 Meier J et al Language models enable zero shot prediction of the effects of mutations on protein function Advances in Neural Information Processing Systems 34 2928729303 2021 10 18 CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The\n"
            },
            {
                "rank": 1,
                "index": 2,
                "score": 0.14608724,
                "text": "Rudra and C R 2023 Hungry Hungry Hippos Towards Language Modeling with State Space Models ArXiv 2212 14052 cs S Dathathri A Madotto J Lan J Hung E Frank P Molino J Yosinski and R Liu 2020 Plug and play language models A simple approach to controlled text generation J Dauparas I Anishchenko N Bennett H Bai R J Ragotte L F Milles B I M Wicky A Courbet et al 2022 Robust deep learning x2013 based protein sequence design using proteinmpnn Science 378 6615 4956 N De Cao W Aziz and I Titov 2021 Editing factual knowledge in language models In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing pages 64916506 Online and Punta Cana Dominican Republic Association for Computational Linguistics M Dehghani\n"
            },
            {
                "rank": 2,
                "index": 9,
                "score": 0.09105828,
                "text": "These techniques have been very successful in analyzing deep learning from properties of neural network training Gotmare et al 2018 Neyshabur et al 2020 objectives Resnick et al 2019 Thompson et al 2019 Hermann Lampinen 2020 and dynamics Maheswaranathan et al 2019 to revealing hidden linguistic structure in large language models Bau et al 2019 Kudugunta et al 2019 Wu et al 2019 2020 and applications in neuroscience Shi et al 2019 Li et al 2019 Merel et al 2019 Zhang Bellec 2020 and medicine Raghu et al 2019 3 E XPERIMENTAL SETUP AND BACKGROUND Our goal is to understand the effects of depth and width on the function learned by the underlying neural network in a setting representative of the high performance models used in practice Reflecting\n"
            },
            {
                "rank": 3,
                "index": 6,
                "score": 0.059319142,
                "text": "pipeline with a single forward pass of a pre trained end to end protein language model In the last year protein language modeling with an unsupervised training objective has been investigated by multiple groups Rives et al 2019 Alley et al 2019 Heinzinger et al 2019 Rao et al 2019 Madani et al 2020 The longstanding practice in bioinformatics has been to fit linear models on focused sets of evolutionarily related and aligned sequences by contrast protein language modeling trains nonlinear deep neural networks on large databases of evolutionarily diverse and unaligned sequences High capacity protein language models have been shown to learn underlying intrinsic properties of proteins such as structure and function from sequence data Rives et al 2019 A line of work in this emerging field\n"
            },
            {
                "rank": 4,
                "index": 7,
                "score": 0.039564554,
                "text": "transformers and beyond Large language models for the genome arXiv 2311 07621 2023 20 Benegas G Batra S S Song Y S Dna language models are powerful predictors of genome wide variant effects Proceedings of the National Academy of Sciences 120 e2311219120 2023 21 Nguyen E et al Hyenadna Long range genomic sequence modeling at single nucleotide resolution arXiv preprint arXiv 2306 15794 2023 22 Lal A Biancalani T Eraslan G reglm Designing realistic regulatory dna with autoregressive language models In NeurIPS 2023 Generative AI and Biology GenBio Workshop 2023 23 Dalla Torre H et al The nucleotide transformer Building and evaluating robust foundation models for human genomics bioRxiv 2023 24 Ji Y et al Dnabert pre trained bidirectional encoder representations from transformers model for dna language in\n"
            },
            {
                "rank": 5,
                "index": 5,
                "score": 0.007815889,
                "text": "copyright holder for this preprint this version posted March 4 2024 https doi org 10 1101 2024 02 29 582810doi bioRxiv preprint 14 Madani A et al Large language models generate functional protein sequences across diverse families Nature Biotechnology18 2023 15 Ferruz N Hcker B Controllable protein design with language models Nature Machine Intelligence 4 521532 2022 16 Hie B L et al Efficient evolution of human antibodies from general protein language models Nature Biotechnology 2023 17 Hie B L Yang K K Kim P S Evolutionary velocity with protein language models predicts evolutionary dynamics of diverse proteins Cell Systems 13 274285 2022 18 Zhang Z et al Protein language models learn evolutionary statistics of interacting sequence motifs bioRxiv 202401 2024 19 Consens M E et al To\n"
            },
            {
                "rank": 6,
                "index": 4,
                "score": 0.0020427648,
                "text": "sequence modeling arXiv preprint arXiv 1904 01038 N Ousidhoum X Zhao T Fang Y Song and D Y Yeung 2021 Probing toxic content in large pre trained language models In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing Volume 1 Long Papers pages 42624274 C Outeiral and C Deane 2022 Codon language embeddings provide strong signals for protein engineering bioRxiv pages 202212 L Ouyang J Wu X Jiang D Almeida C Wainwright P Mishkin C Zhang S Agarwal et al 2022 Training language models to follow instructions with human feedback InAdvances in Neural Information Processing Systems M Pagliardini D Paliotta M Jaggi and F Fleuret 2023 Faster causal attention over large sequences through sparse\n"
            },
            {
                "rank": 7,
                "index": 3,
                "score": 0.0010817152,
                "text": "social impacts of language models arXiv preprint arXiv 1908 09203 Solaiman I and Dennison C 2021 Process for adapting language models to society palms with values targeted datasets arXiv preprint arXiv 2106 10328 Stiennon N Ouyang L Wu J Ziegler D M Lowe R V oss C Radford A Amodei D and Christiano P 2020 Learning to summarize from human feedback arXiv preprint arXiv 2009 01325 Tamkin A Brundage M Clark J and Ganguli D 2021 Understanding the capabilities limitations and societal impact of large language models arXiv preprint arXiv 2102 02503 Thoppilan R De Freitas D Hall J Shazeer N Kulshreshtha A Cheng H T Jin A Bos T Baker L Du Y et al 2022 Lamda Language models for dialog applications arXiv preprint arXiv 2201 08239\n"
            },
            {
                "rank": 8,
                "index": 8,
                "score": 0.00094361923,
                "text": "language models are zero shot reasoners Preprint at https doi org 10 48550 arXiv 2205 11916 2022 82 Joshi M Choi E Weld D S Zettlemoyer L TriviaQA a large scale distantly supervised challenge dataset for reading comprehension Preprint at https doi org 10 48550 arXiv 1705 03551 2017 83 Beltagy I Lo K Cohan A SciBERT a pretrained language model for scientific text Preprint at https doi org 10 48550 arXiv 1903 10676 2019 84 Lewis P Ott M Du J Stoyanov V Pretrained language models for biomedical and clinical tasks Understanding and extending the state of the art In Proc 3rd Clinical Natural Language Processing Workshop eds Roberts K Bethard S Naumann T 146157 Association for Computational Linguistics 2020 85 Shin H C etal BioMegatron larger\n"
            },
            {
                "rank": 9,
                "index": 1,
                "score": 0.0008199321,
                "text": "Code as policies Language model programs for embodied control P P Liang C Wu L P Morency and R Salakhutdinov 2021 Towards understanding and mitigating social biases in language models In International Conference on Machine Learning pages 65656576 PMLR P Liang R Bommasani T Lee D Tsipras D Soylu M Yasunaga Y Zhang D Narayanan et al 2022 Holistic evaluation of language models arXiv preprint arXiv 2211 09110 O Lieber O Sharir B Lenz and Y Shoham 2021 Jurassic 1 Technical details and evaluation White Paper AI21 Labs 1 V Livin C E Hother and O Winther 2022 Can large language models reason about medical questions arXiv preprint arXiv 2207 08143 C C Lin A Jaech X Li M R Gormley and J Eisner 2020 Limitations of autoregressive\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -0.8447495698928833,
                "text": "using a language model and deep learning Nature Biotechnology 40 16171623 2022 11 Wu R et al High resolution de novo structure prediction from primary sequence BioRxiv 202207 2022 12 Brandes N Goldman G Wang C H Ye C J Ntranos V Genome wide prediction of disease variant effects with a deep protein language model Nature Genetics 55 15121522 2023 13 Meier J et al Language models enable zero shot prediction of the effects of mutations on protein function Advances in Neural Information Processing Systems 34 2928729303 2021 10 18 CC BY NC ND 4 0 International license available under a which was not certified by peer review is the author funder who has granted bioRxiv a license to display the preprint in perpetuity It is made The\n"
            },
            {
                "rank": 1,
                "score": -1.8722810745239258,
                "text": "Rudra and C R 2023 Hungry Hungry Hippos Towards Language Modeling with State Space Models ArXiv 2212 14052 cs S Dathathri A Madotto J Lan J Hung E Frank P Molino J Yosinski and R Liu 2020 Plug and play language models A simple approach to controlled text generation J Dauparas I Anishchenko N Bennett H Bai R J Ragotte L F Milles B I M Wicky A Courbet et al 2022 Robust deep learning x2013 based protein sequence design using proteinmpnn Science 378 6615 4956 N De Cao W Aziz and I Titov 2021 Editing factual knowledge in language models In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing pages 64916506 Online and Punta Cana Dominican Republic Association for Computational Linguistics M Dehghani\n"
            },
            {
                "rank": 2,
                "score": -2.3640449047088623,
                "text": "pipeline with a single forward pass of a pre trained end to end protein language model In the last year protein language modeling with an unsupervised training objective has been investigated by multiple groups Rives et al 2019 Alley et al 2019 Heinzinger et al 2019 Rao et al 2019 Madani et al 2020 The longstanding practice in bioinformatics has been to fit linear models on focused sets of evolutionarily related and aligned sequences by contrast protein language modeling trains nonlinear deep neural networks on large databases of evolutionarily diverse and unaligned sequences High capacity protein language models have been shown to learn underlying intrinsic properties of proteins such as structure and function from sequence data Rives et al 2019 A line of work in this emerging field\n"
            },
            {
                "rank": 3,
                "score": -2.6157233715057373,
                "text": "These techniques have been very successful in analyzing deep learning from properties of neural network training Gotmare et al 2018 Neyshabur et al 2020 objectives Resnick et al 2019 Thompson et al 2019 Hermann Lampinen 2020 and dynamics Maheswaranathan et al 2019 to revealing hidden linguistic structure in large language models Bau et al 2019 Kudugunta et al 2019 Wu et al 2019 2020 and applications in neuroscience Shi et al 2019 Li et al 2019 Merel et al 2019 Zhang Bellec 2020 and medicine Raghu et al 2019 3 E XPERIMENTAL SETUP AND BACKGROUND Our goal is to understand the effects of depth and width on the function learned by the underlying neural network in a setting representative of the high performance models used in practice Reflecting\n"
            },
            {
                "rank": 4,
                "score": -2.8587803840637207,
                "text": "language models are zero shot reasoners Preprint at https doi org 10 48550 arXiv 2205 11916 2022 82 Joshi M Choi E Weld D S Zettlemoyer L TriviaQA a large scale distantly supervised challenge dataset for reading comprehension Preprint at https doi org 10 48550 arXiv 1705 03551 2017 83 Beltagy I Lo K Cohan A SciBERT a pretrained language model for scientific text Preprint at https doi org 10 48550 arXiv 1903 10676 2019 84 Lewis P Ott M Du J Stoyanov V Pretrained language models for biomedical and clinical tasks Understanding and extending the state of the art In Proc 3rd Clinical Natural Language Processing Workshop eds Roberts K Bethard S Naumann T 146157 Association for Computational Linguistics 2020 85 Shin H C etal BioMegatron larger\n"
            },
            {
                "rank": 5,
                "score": -3.477508068084717,
                "text": "social impacts of language models arXiv preprint arXiv 1908 09203 Solaiman I and Dennison C 2021 Process for adapting language models to society palms with values targeted datasets arXiv preprint arXiv 2106 10328 Stiennon N Ouyang L Wu J Ziegler D M Lowe R V oss C Radford A Amodei D and Christiano P 2020 Learning to summarize from human feedback arXiv preprint arXiv 2009 01325 Tamkin A Brundage M Clark J and Ganguli D 2021 Understanding the capabilities limitations and societal impact of large language models arXiv preprint arXiv 2102 02503 Thoppilan R De Freitas D Hall J Shazeer N Kulshreshtha A Cheng H T Jin A Bos T Baker L Du Y et al 2022 Lamda Language models for dialog applications arXiv preprint arXiv 2201 08239\n"
            },
            {
                "rank": 6,
                "score": -3.596832513809204,
                "text": "transformers and beyond Large language models for the genome arXiv 2311 07621 2023 20 Benegas G Batra S S Song Y S Dna language models are powerful predictors of genome wide variant effects Proceedings of the National Academy of Sciences 120 e2311219120 2023 21 Nguyen E et al Hyenadna Long range genomic sequence modeling at single nucleotide resolution arXiv preprint arXiv 2306 15794 2023 22 Lal A Biancalani T Eraslan G reglm Designing realistic regulatory dna with autoregressive language models In NeurIPS 2023 Generative AI and Biology GenBio Workshop 2023 23 Dalla Torre H et al The nucleotide transformer Building and evaluating robust foundation models for human genomics bioRxiv 2023 24 Ji Y et al Dnabert pre trained bidirectional encoder representations from transformers model for dna language in\n"
            },
            {
                "rank": 7,
                "score": -3.7025647163391113,
                "text": "Code as policies Language model programs for embodied control P P Liang C Wu L P Morency and R Salakhutdinov 2021 Towards understanding and mitigating social biases in language models In International Conference on Machine Learning pages 65656576 PMLR P Liang R Bommasani T Lee D Tsipras D Soylu M Yasunaga Y Zhang D Narayanan et al 2022 Holistic evaluation of language models arXiv preprint arXiv 2211 09110 O Lieber O Sharir B Lenz and Y Shoham 2021 Jurassic 1 Technical details and evaluation White Paper AI21 Labs 1 V Livin C E Hother and O Winther 2022 Can large language models reason about medical questions arXiv preprint arXiv 2207 08143 C C Lin A Jaech X Li M R Gormley and J Eisner 2020 Limitations of autoregressive\n"
            },
            {
                "rank": 8,
                "score": -3.746547222137451,
                "text": "copyright holder for this preprint this version posted March 4 2024 https doi org 10 1101 2024 02 29 582810doi bioRxiv preprint 14 Madani A et al Large language models generate functional protein sequences across diverse families Nature Biotechnology18 2023 15 Ferruz N Hcker B Controllable protein design with language models Nature Machine Intelligence 4 521532 2022 16 Hie B L et al Efficient evolution of human antibodies from general protein language models Nature Biotechnology 2023 17 Hie B L Yang K K Kim P S Evolutionary velocity with protein language models predicts evolutionary dynamics of diverse proteins Cell Systems 13 274285 2022 18 Zhang Z et al Protein language models learn evolutionary statistics of interacting sequence motifs bioRxiv 202401 2024 19 Consens M E et al To\n"
            },
            {
                "rank": 9,
                "score": -3.973950147628784,
                "text": "sequence modeling arXiv preprint arXiv 1904 01038 N Ousidhoum X Zhao T Fang Y Song and D Y Yeung 2021 Probing toxic content in large pre trained language models In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing Volume 1 Long Papers pages 42624274 C Outeiral and C Deane 2022 Codon language embeddings provide strong signals for protein engineering bioRxiv pages 202212 L Ouyang J Wu X Jiang D Almeida C Wainwright P Mishkin C Zhang S Agarwal et al 2022 Training language models to follow instructions with human feedback InAdvances in Neural Information Processing Systems M Pagliardini D Paliotta M Jaggi and F Fleuret 2023 Faster causal attention over large sequences through sparse\n"
            }
        ]
    },
    {
        "query": "How do computer science principles apply to the development of more efficient large language models?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 2,
                "score": 0.98875546,
                "text": "the cost of such models letting us adopt a paradigm of training single large scale models then creating more efficient versions of them for use in appropriate contexts Algorithmic progress may also naturally further increase the efficiency of such models over time similar to trends observed in image recognition and neural machine translation HB20 7 Related Work Several lines of work have focused on increasing parameter count and or computation in language models as a means to improve generative or task performance An early work scaled LSTM based language models to over a billion parameters JVS 16 One line of work straightforwardly increases the size of transformer models scaling up parameters and FLOPS per token roughly in proportion Work in this vein has successively increased model size 213\n"
            },
            {
                "rank": 1,
                "index": 0,
                "score": 0.38531685,
                "text": "retrieval language models 1 Introduction Large language models LLMs are impressive few shot learners Brown et al 2020 Rae et al 2021 Hoffmann et al 2022 Chowdhery et al 2022 They are able to learn new tasks with very few examples or even from instructions alone For this generalisation ability to emerge the key ingredients are scaling both the parameter count of the model and the size of the training data Large language models owe this improvement to both a larger computational budget enabling more complex reasoning and the ability to memorize more Equal contribution Work done while at Meta AI c2023 Gautier Izacard Patrick Lewis Maria Lomeli Lucas Hosseini Fabio Petroni Timo Schick Jane Dwivedi Yu Armand Joulin Sebastian Riedel Edouard Grave License CC BY 4 0\n"
            },
            {
                "rank": 2,
                "index": 5,
                "score": 0.10913479,
                "text": "Smith et al 2022 Thoppilan et al 2022 with the largest dense language models now having over 500 billion parameters These large autoregressive transformers Vaswani et al 2017 have demonstrated impressive performance on many tasks using a variety of evaluation protocols such as zero shot few shot and fine tuning The compute and energy cost for training large language models is substantial Rae et al 2021 Thoppilan et al 2022 and rises with increasing model size In practice the allocated training compute budget is often known in advance how many accelerators are available and for how long we want to use them Since it is typically only feasible to train these large models once accurately estimating the best model hyperparameters for a given compute budget is critical Tay\n"
            },
            {
                "rank": 3,
                "index": 1,
                "score": 0.08601588,
                "text": "C R et al 2023 High throughput generative inference of large language models with a single gpu T Shevlane S Farquhar B Garfinkel M Phuong J Whittlestone J Leung D Kokotajlo N Marchal et al 2023 Model evaluation for extreme risks arXiv preprint arXiv 2305 15324 A Shirafuji Y Watanobe T Ito M Morishita Y Nakamura Y Oda and J Suzuki 2023 Exploring the robustness of large language models for solving programming problems O Shliazhko A Fenogenova M Tikhonova V Mikhailov A Kozlova and T Shavrina 2022 mgpt Few shot learners go multilingual arXiv preprint arXiv 2204 07580 M Shoeybi M Patwary R Puri P LeGresley J Casper and B Catanzaro 2019 Megatron lm Training multibillion parameter language models using model parallelism arXiv preprint arXiv 1909 08053 K\n"
            },
            {
                "rank": 4,
                "index": 3,
                "score": 0.055515114,
                "text": "8 Nijkamp E etal CodeGen an open large language model for code with multi turn program synthesis In Proc International Conference on Learning Representations 2022 9 Chen X Lin M Schrli N Zhou D Teaching large language models to self debug Preprint at https arxiv org abs 2304 05128 2023 10 Liventsev V Grishina A Hrm A Moonen L Fully autonomous programming with large language models Preprint at https arxiv org abs 2304 10423 2023 11 Li Y etal Competition level code generation with alphacode Science 378 10921097 2022 12 Zelikman E Huang Q Poesia G Goodman N D Haber N Parsel a de compositional framework for algorithmic reasoning with language models Preprint at https arxiv org abs 2212 10561 2023 13 Madaan A etal Learning performance improving\n"
            },
            {
                "rank": 5,
                "index": 6,
                "score": 0.022802636,
                "text": "models arXiv preprint arXiv 2205 10625 2022a Yongchao Zhou Andrei Ioan Muresanu Ziwen Han Keiran Paster Silviu Pitis Harris Chan and Jimmy Ba Large language models are human level prompt engineers arXiv preprint arXiv 2211 01910 2022b 25 Large Language Models as Optimizers A S OME FAILURE CASES Although LLMs show the power of optimizing basic math problems Section 3 and prompts Section 4 we see some limitations across all optimizer LLMs that may impede their power of solving more challenging problems These limitations include Hallucinating the values that need to come from math calculation The optimizer LLMs often output contents like the function value at 5 3 is 15 despite that the true value is not 15 The model will get it right if external tools that\n"
            },
            {
                "rank": 6,
                "index": 4,
                "score": 0.003510649,
                "text": "more is required and is left to future work 8 Conclusion The landscape of language technologies with general capabilities is progressing rapidly Language models are a key driver of this progress and we have shown that an emphasis on data quality and scale still yields interesting performance advances over existing work However the benefits of scale are nonuniform some tasks which require more complex mathematical or logical reasoning observe little benefit up to the scale of Gopher This may be an inherent property of the language modelling objective it is hard to compress mathematics and easier to learn many associative facts about the world However it is possible that a sufficiently complex model may become bottlenecked by its poor understanding and thus compression of reasoning and new reasoning\n"
            },
            {
                "rank": 7,
                "index": 7,
                "score": 0.002631674,
                "text": "language models arXiv preprint arXiv 2001 08361 Mikhail Khrushchev Ruslan Vasilev Alexey Petrov and Nikolay Zinov 2022 YaLM 100B Diederik P Kingma and Jimmy Ba 2014 Adam A method for stochastic optimization arXiv preprint arXiv 1412 6980 Denis Kocetkov Raymond Li Loubna Ben Allal Jia Li Chenghao Mou Carlos Muoz Ferrandis Yacine Jernite Margaret Mitchell Sean Hughes Thomas Wolf et al 2022 The Stack 3 TB of permissively licensed source code arXiv preprint arXiv 2211 15533 Takeshi Kojima Shixiang Shane Gu Machel Reid Yutaka Matsuo and Yusuke Iwasawa 2022 Large language models are zero shot reasoners arXiv preprint arXiv 2205 11916 Aran Komatsuzaki 2019 One epoch is all you need arXiv preprint arXiv 1906 06669 Taku Kudo and John Richardson 2018 SentencePiece A simple and language independent subword\n"
            },
            {
                "rank": 8,
                "index": 9,
                "score": 0.0022694687,
                "text": "decreases sycophancy 1 Introduction Large Language Models LLMs are highly capable yet they are still susceptible to making simple mistakes which seem to display weak reasoning abilities For example they can be swayed to make erroneous judgments by irrelevant context Jia Liang 2017 Cho et al 2023 Shi et al 2023 or by preference or opinion inherent in the input prompt in the latter case exhibiting an issue termed sycophancy whereby the model agrees with the input Sharma et al 2023 While several approaches try to mitigate these issues through adding more supervised training data Wei et al 2023 or reinforcement learning strategies Sharma et al 2023 we posit that the underlying problem is inherent in the way the transformer itself is built and in particular its attention\n"
            },
            {
                "rank": 9,
                "index": 8,
                "score": 0.0010083435,
                "text": "B 2529 IOS Press 2019 12 Hendrycks D etal Measuring massive multitask language understanding Preprint at https doi org 10 48550 arXiv 2009 03300 2020 13 Chowdhery A etal PaLM scaling language modeling with pathways Preprint at https doi org 10 48550 arXiv 2204 02311 2022 14 Chung H W etal Scaling instruction finetuned language models Preprint at https doi org 10 48550 arXiv 2210 11416 2022 15 Brown T etal Language models are few shot learners Adv Neural Inf Process Syst 33 18771901 2020 16 Wei J etal Chain of thought prompting elicits reasoning in large language models Preprint at https doi org 10 48550 arXiv 2201 11903 2022 17 Wang X etal Self consistency improves chain of thought reasoning in language models Preprint at https doi\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -0.7603007555007935,
                "text": "the cost of such models letting us adopt a paradigm of training single large scale models then creating more efficient versions of them for use in appropriate contexts Algorithmic progress may also naturally further increase the efficiency of such models over time similar to trends observed in image recognition and neural machine translation HB20 7 Related Work Several lines of work have focused on increasing parameter count and or computation in language models as a means to improve generative or task performance An early work scaled LSTM based language models to over a billion parameters JVS 16 One line of work straightforwardly increases the size of transformer models scaling up parameters and FLOPS per token roughly in proportion Work in this vein has successively increased model size 213\n"
            },
            {
                "rank": 1,
                "score": -1.3241729736328125,
                "text": "retrieval language models 1 Introduction Large language models LLMs are impressive few shot learners Brown et al 2020 Rae et al 2021 Hoffmann et al 2022 Chowdhery et al 2022 They are able to learn new tasks with very few examples or even from instructions alone For this generalisation ability to emerge the key ingredients are scaling both the parameter count of the model and the size of the training data Large language models owe this improvement to both a larger computational budget enabling more complex reasoning and the ability to memorize more Equal contribution Work done while at Meta AI c2023 Gautier Izacard Patrick Lewis Maria Lomeli Lucas Hosseini Fabio Petroni Timo Schick Jane Dwivedi Yu Armand Joulin Sebastian Riedel Edouard Grave License CC BY 4 0\n"
            },
            {
                "rank": 2,
                "score": -1.8297054767608643,
                "text": "C R et al 2023 High throughput generative inference of large language models with a single gpu T Shevlane S Farquhar B Garfinkel M Phuong J Whittlestone J Leung D Kokotajlo N Marchal et al 2023 Model evaluation for extreme risks arXiv preprint arXiv 2305 15324 A Shirafuji Y Watanobe T Ito M Morishita Y Nakamura Y Oda and J Suzuki 2023 Exploring the robustness of large language models for solving programming problems O Shliazhko A Fenogenova M Tikhonova V Mikhailov A Kozlova and T Shavrina 2022 mgpt Few shot learners go multilingual arXiv preprint arXiv 2204 07580 M Shoeybi M Patwary R Puri P LeGresley J Casper and B Catanzaro 2019 Megatron lm Training multibillion parameter language models using model parallelism arXiv preprint arXiv 1909 08053 K\n"
            },
            {
                "rank": 3,
                "score": -1.920363187789917,
                "text": "8 Nijkamp E etal CodeGen an open large language model for code with multi turn program synthesis In Proc International Conference on Learning Representations 2022 9 Chen X Lin M Schrli N Zhou D Teaching large language models to self debug Preprint at https arxiv org abs 2304 05128 2023 10 Liventsev V Grishina A Hrm A Moonen L Fully autonomous programming with large language models Preprint at https arxiv org abs 2304 10423 2023 11 Li Y etal Competition level code generation with alphacode Science 378 10921097 2022 12 Zelikman E Huang Q Poesia G Goodman N D Haber N Parsel a de compositional framework for algorithmic reasoning with language models Preprint at https arxiv org abs 2212 10561 2023 13 Madaan A etal Learning performance improving\n"
            },
            {
                "rank": 4,
                "score": -2.023761749267578,
                "text": "Smith et al 2022 Thoppilan et al 2022 with the largest dense language models now having over 500 billion parameters These large autoregressive transformers Vaswani et al 2017 have demonstrated impressive performance on many tasks using a variety of evaluation protocols such as zero shot few shot and fine tuning The compute and energy cost for training large language models is substantial Rae et al 2021 Thoppilan et al 2022 and rises with increasing model size In practice the allocated training compute budget is often known in advance how many accelerators are available and for how long we want to use them Since it is typically only feasible to train these large models once accurately estimating the best model hyperparameters for a given compute budget is critical Tay\n"
            },
            {
                "rank": 5,
                "score": -2.2069168090820312,
                "text": "decreases sycophancy 1 Introduction Large Language Models LLMs are highly capable yet they are still susceptible to making simple mistakes which seem to display weak reasoning abilities For example they can be swayed to make erroneous judgments by irrelevant context Jia Liang 2017 Cho et al 2023 Shi et al 2023 or by preference or opinion inherent in the input prompt in the latter case exhibiting an issue termed sycophancy whereby the model agrees with the input Sharma et al 2023 While several approaches try to mitigate these issues through adding more supervised training data Wei et al 2023 or reinforcement learning strategies Sharma et al 2023 we posit that the underlying problem is inherent in the way the transformer itself is built and in particular its attention\n"
            },
            {
                "rank": 6,
                "score": -2.3697776794433594,
                "text": "models arXiv preprint arXiv 2205 10625 2022a Yongchao Zhou Andrei Ioan Muresanu Ziwen Han Keiran Paster Silviu Pitis Harris Chan and Jimmy Ba Large language models are human level prompt engineers arXiv preprint arXiv 2211 01910 2022b 25 Large Language Models as Optimizers A S OME FAILURE CASES Although LLMs show the power of optimizing basic math problems Section 3 and prompts Section 4 we see some limitations across all optimizer LLMs that may impede their power of solving more challenging problems These limitations include Hallucinating the values that need to come from math calculation The optimizer LLMs often output contents like the function value at 5 3 is 15 despite that the true value is not 15 The model will get it right if external tools that\n"
            },
            {
                "rank": 7,
                "score": -2.4396982192993164,
                "text": "language models arXiv preprint arXiv 2001 08361 Mikhail Khrushchev Ruslan Vasilev Alexey Petrov and Nikolay Zinov 2022 YaLM 100B Diederik P Kingma and Jimmy Ba 2014 Adam A method for stochastic optimization arXiv preprint arXiv 1412 6980 Denis Kocetkov Raymond Li Loubna Ben Allal Jia Li Chenghao Mou Carlos Muoz Ferrandis Yacine Jernite Margaret Mitchell Sean Hughes Thomas Wolf et al 2022 The Stack 3 TB of permissively licensed source code arXiv preprint arXiv 2211 15533 Takeshi Kojima Shixiang Shane Gu Machel Reid Yutaka Matsuo and Yusuke Iwasawa 2022 Large language models are zero shot reasoners arXiv preprint arXiv 2205 11916 Aran Komatsuzaki 2019 One epoch is all you need arXiv preprint arXiv 1906 06669 Taku Kudo and John Richardson 2018 SentencePiece A simple and language independent subword\n"
            },
            {
                "rank": 8,
                "score": -2.7383506298065186,
                "text": "more is required and is left to future work 8 Conclusion The landscape of language technologies with general capabilities is progressing rapidly Language models are a key driver of this progress and we have shown that an emphasis on data quality and scale still yields interesting performance advances over existing work However the benefits of scale are nonuniform some tasks which require more complex mathematical or logical reasoning observe little benefit up to the scale of Gopher This may be an inherent property of the language modelling objective it is hard to compress mathematics and easier to learn many associative facts about the world However it is possible that a sufficiently complex model may become bottlenecked by its poor understanding and thus compression of reasoning and new reasoning\n"
            },
            {
                "rank": 9,
                "score": -2.87223482131958,
                "text": "B 2529 IOS Press 2019 12 Hendrycks D etal Measuring massive multitask language understanding Preprint at https doi org 10 48550 arXiv 2009 03300 2020 13 Chowdhery A etal PaLM scaling language modeling with pathways Preprint at https doi org 10 48550 arXiv 2204 02311 2022 14 Chung H W etal Scaling instruction finetuned language models Preprint at https doi org 10 48550 arXiv 2210 11416 2022 15 Brown T etal Language models are few shot learners Adv Neural Inf Process Syst 33 18771901 2020 16 Wei J etal Chain of thought prompting elicits reasoning in large language models Preprint at https doi org 10 48550 arXiv 2201 11903 2022 17 Wang X etal Self consistency improves chain of thought reasoning in language models Preprint at https doi\n"
            }
        ]
    },
    {
        "query": "Compare and contrast the effectiveness of different attention mechanisms in transformer-based models for text classification tasks.",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 5,
                "score": 0.91906327,
                "text": "al 2019 and GPT 3 Brown et al 2020 have been very successful in natural language processing NLP achieving state of the art performance in machine translation Vaswani et al 2017 natural language inference Williams Nangia and Bowman 2018 paraphrasing Dolan and Brockett 2005 text classification Howard and Ruder 2018 question answering Rajpurkar et al 2016 and many other NLP tasks Peters et al 2018 Radford et al 2018 A key feature of transformers is what is known as the selfattention mechanism Vaswani et al 2017 where each tokens representation is computed from all other tokens Selfattention enables interactions of token pairs across the full sequence and has been shown quite effective Despite the foregoing advantages self attention also turns out to be a major efficiency bottleneck since\n"
            },
            {
                "rank": 1,
                "index": 1,
                "score": 0.9144437,
                "text": "language models to verify the effectiveness of the momentum based attention on downstream tasks We consider six datasets for sentiment analysis SST5 Socher et al 2013 IMDB Maas et al 2011 MR Pang and Lee 2005 natural language inference CB de Marneffe et al 2019 and multi choice selection ARC E Clark et al 2018 PIQA Bisk et al 2020 For all these datasets we use 32 examples as demonstrations As shown in Table 5 compared with the vanilla Transformer using momentum based attention achieves consistently higher accuracy in all the tasks The performance improvements on both language modeling and in context learning prove that Model Train 1024 Valid 256 Valid 512 Valid 1024 Transformer 17 61 19 50 16 87 15 14 Transformer MoAttn 17 55 19\n"
            },
            {
                "rank": 2,
                "index": 8,
                "score": 0.6837363,
                "text": "37 16 73 15 02 Table 4 Perplexity on the training set and validation sets with different input lengths for language modeling Applying momentum to attention introduces a consistent perplexity improvement compared with the vanilla Transformer Model SST5 IMDB MR CB ARC E PIQA Average Transformer 25 3 64 0 61 2 43 9 48 2 68 7 51 9 Transformer MoAttn 27 4 70 3 64 8 46 8 50 0 69 0 54 7 Table 5 Accuracy on six in context learning tasks Introducing momentum into attention improves the accuracy of the vanilla Transformer by 2 8 on average introducing momentum into attention is an effective strategy which supports our understanding of meta optimization from another aspect 6 Conclusion In this paper we aim to explain\n"
            },
            {
                "rank": 3,
                "index": 2,
                "score": 0.41122907,
                "text": "et al 2018 and IMDB reviews Maas et al 2011 and compare our results to BERT in both accuracy and efficiency Across all tasks our model compares favorably to the vanilla pretrained BERT with significant speedups Finally we evaluate our model on tasks with longer se arXiv 2102 03902v3 cs CL 31 Mar 2021 quence lengths from the Long Range Arena LRA benchmark Tay et al 2020 Nystr omFormer performs well compared to several recent efficient self attention methods including Reformer Kitaev Kaiser and Levskaya 2019 Linformer Wang et al 2020 and Performer Choromanski et al 2020 by margin of 3 4 in average accuracy We believe that the idea is a step towards resource efficient Transformers Related Work We briefly review relevant works on efficient Transformers linearized\n"
            },
            {
                "rank": 4,
                "index": 3,
                "score": 0.12368281,
                "text": "generate comprehensive summaries Shaham et al 2022 otherwise models often miss important information Note that typical transformer models apply full attention to capture token dependencies pair wise It leads to a quadratic time and space complexity w r t input length However such a complexity is prohibitive for long sequences In particular it incurs massive memory consumption during the back propagation For example a transformer model with 250M parameters consumes over 80G GPU memory when sequence length is 8k Zuo et al 2022 To address this scalability issue various approaches have been proposed to reduce the complexity One approach is sparse attention which restricts each token to attend a subset of tokens based on predefined sparsity patterns Beltagy et al 2020 Zaheer et al 2020 Ainslie et al\n"
            },
            {
                "rank": 5,
                "index": 7,
                "score": 0.09773432,
                "text": "more advanced approaches such as Flash408 Attention This implementation effectively enhances the model capacity and enables effective 409 processing of large scale input dimensions Other efficient transformers can also be utilized such 410 as Transformers with linear complexity Linformer and Kernelized Self Attention KSA 411 4 2 2 Cell representation 412 Each cell is considered a sentence composed of genes and its representation h i cRDis obtained 413 by aggregating the learned gene level representations h i n Various pooling operations such as 414 element wise mean pooling or weighted pooling can be readily employed in this context In this 415 study we opt to employ a special token cls for the cell representation enabling the model 416 to learn the pooling operation within transformer blocks The cls\n"
            },
            {
                "rank": 6,
                "index": 0,
                "score": 0.016090874,
                "text": "attention with a kNN lookup to increase speed and reduce memory usage 3 M ETHOD The architecture of our kNN augmented transformer is shown in Figure 2 The bulk of the model is a vanilla decoder only transformer Vaswani et al 2017 The input text is tokenized and the tokens are embedded into vector space The embedding vectors are passed through a series of transformer layers each of which does dense self attention followed by a feed forward network FFN Since this is a decoder only language model we use a causal attention mask and the token embeddings of the last layer are used to predict the next token Long documents are split into subsequences of 512 tokens and each subsequence is used as the input for one\n"
            },
            {
                "rank": 7,
                "index": 9,
                "score": 0.010818449,
                "text": "One write head is all you need ArXiv abs 1911 02150 2019 So D R Manke W Liu H Dai Z Shazeer N M and Le Q V Primer Searching for efficient transformers for language modeling ArXiv abs 2109 08668 2021 Stern M Shazeer N and Uszkoreit J Blockwise parallel decoding for deep autoregressive models Advances in Neural Information Processing Systems 31 2018 Sukhbaatar S Grave E Bojanowski P and Joulin A Adaptive attention span in transformers In Annual Meeting of the Association for Computational Linguistics 2019 Sun X Ge T Wei F and Wang H Instantaneous grammatical error correction with shallow aggressive decoding ArXiv abs 2106 04970 2021 9 Fast Inference from Transformers via Speculative Decoding Thoppilan R Freitas D D Hall J Shazeer N M Kulshreshtha\n"
            },
            {
                "rank": 8,
                "index": 6,
                "score": 0.0012943681,
                "text": "Range Arena A Benchmark for Efficient Transformers arXiv preprint arXiv 2011 04006 Vaswani A Shazeer N Parmar N Uszkoreit J Jones L Gomez A N Kaiser and Polosukhin I 2017 Attention is all you need In Advances in Neural Information Processing Systems NeurIPS 59986008 Vyas A Katharopoulos A and Fleuret F 2020 Fast transformers with clustered attention Advances in Neural Information Processing Systems 33 Wang A Singh A Michael J Hill F Levy O and Bowman S R 2018 GLUE A Multi Task Benchmark and Analysis Platform for Natural Language Understanding In International Conference on Learning Representations ICLR Wang A Singh A Michael J Hill F Levy O and Bowman S R 2019 GLUE A Multi Task Benchmark and Analysis Platform for Natural Language Understanding In Proceedings of\n"
            },
            {
                "rank": 9,
                "index": 4,
                "score": 6.302759e-05,
                "text": "have been proposed to address this issue Beltagy et al 2020 Gulati et al 2020 Chan et al 2020 Child et al 2019 Bello et al 2019 Most approaches restrict the attention mechanism to attend to local neighborhoods Parmar et al 2018 or incorporate structural priors on attention such as sparsity Child et al 2019 pooling based compression Rae et al 2020 clustering binning convolution techniques e g Roy et al 2020 which applies k means clustering to learn dynamic sparse attention regions or Kitaev et al 2020 where locality sensitive hashing is used to group together tokens of similar embeddings sliding windows Beltagy et al 2020 or truncated targeting Chelba et al 2020 There is also a long line of research on using dense attention matrices but\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": 1.814443588256836,
                "text": "37 16 73 15 02 Table 4 Perplexity on the training set and validation sets with different input lengths for language modeling Applying momentum to attention introduces a consistent perplexity improvement compared with the vanilla Transformer Model SST5 IMDB MR CB ARC E PIQA Average Transformer 25 3 64 0 61 2 43 9 48 2 68 7 51 9 Transformer MoAttn 27 4 70 3 64 8 46 8 50 0 69 0 54 7 Table 5 Accuracy on six in context learning tasks Introducing momentum into attention improves the accuracy of the vanilla Transformer by 2 8 on average introducing momentum into attention is an effective strategy which supports our understanding of meta optimization from another aspect 6 Conclusion In this paper we aim to explain\n"
            },
            {
                "rank": 1,
                "score": 1.5595190525054932,
                "text": "language models to verify the effectiveness of the momentum based attention on downstream tasks We consider six datasets for sentiment analysis SST5 Socher et al 2013 IMDB Maas et al 2011 MR Pang and Lee 2005 natural language inference CB de Marneffe et al 2019 and multi choice selection ARC E Clark et al 2018 PIQA Bisk et al 2020 For all these datasets we use 32 examples as demonstrations As shown in Table 5 compared with the vanilla Transformer using momentum based attention achieves consistently higher accuracy in all the tasks The performance improvements on both language modeling and in context learning prove that Model Train 1024 Valid 256 Valid 512 Valid 1024 Transformer 17 61 19 50 16 87 15 14 Transformer MoAttn 17 55 19\n"
            },
            {
                "rank": 2,
                "score": 0.47256186604499817,
                "text": "al 2019 and GPT 3 Brown et al 2020 have been very successful in natural language processing NLP achieving state of the art performance in machine translation Vaswani et al 2017 natural language inference Williams Nangia and Bowman 2018 paraphrasing Dolan and Brockett 2005 text classification Howard and Ruder 2018 question answering Rajpurkar et al 2016 and many other NLP tasks Peters et al 2018 Radford et al 2018 A key feature of transformers is what is known as the selfattention mechanism Vaswani et al 2017 where each tokens representation is computed from all other tokens Selfattention enables interactions of token pairs across the full sequence and has been shown quite effective Despite the foregoing advantages self attention also turns out to be a major efficiency bottleneck since\n"
            },
            {
                "rank": 3,
                "score": 0.3881763517856598,
                "text": "generate comprehensive summaries Shaham et al 2022 otherwise models often miss important information Note that typical transformer models apply full attention to capture token dependencies pair wise It leads to a quadratic time and space complexity w r t input length However such a complexity is prohibitive for long sequences In particular it incurs massive memory consumption during the back propagation For example a transformer model with 250M parameters consumes over 80G GPU memory when sequence length is 8k Zuo et al 2022 To address this scalability issue various approaches have been proposed to reduce the complexity One approach is sparse attention which restricts each token to attend a subset of tokens based on predefined sparsity patterns Beltagy et al 2020 Zaheer et al 2020 Ainslie et al\n"
            },
            {
                "rank": 4,
                "score": 0.060897454619407654,
                "text": "et al 2018 and IMDB reviews Maas et al 2011 and compare our results to BERT in both accuracy and efficiency Across all tasks our model compares favorably to the vanilla pretrained BERT with significant speedups Finally we evaluate our model on tasks with longer se arXiv 2102 03902v3 cs CL 31 Mar 2021 quence lengths from the Long Range Arena LRA benchmark Tay et al 2020 Nystr omFormer performs well compared to several recent efficient self attention methods including Reformer Kitaev Kaiser and Levskaya 2019 Linformer Wang et al 2020 and Performer Choromanski et al 2020 by margin of 3 4 in average accuracy We believe that the idea is a step towards resource efficient Transformers Related Work We briefly review relevant works on efficient Transformers linearized\n"
            },
            {
                "rank": 5,
                "score": -0.5874242782592773,
                "text": "attention with a kNN lookup to increase speed and reduce memory usage 3 M ETHOD The architecture of our kNN augmented transformer is shown in Figure 2 The bulk of the model is a vanilla decoder only transformer Vaswani et al 2017 The input text is tokenized and the tokens are embedded into vector space The embedding vectors are passed through a series of transformer layers each of which does dense self attention followed by a feed forward network FFN Since this is a decoder only language model we use a causal attention mask and the token embeddings of the last layer are used to predict the next token Long documents are split into subsequences of 512 tokens and each subsequence is used as the input for one\n"
            },
            {
                "rank": 6,
                "score": -0.8650515079498291,
                "text": "more advanced approaches such as Flash408 Attention This implementation effectively enhances the model capacity and enables effective 409 processing of large scale input dimensions Other efficient transformers can also be utilized such 410 as Transformers with linear complexity Linformer and Kernelized Self Attention KSA 411 4 2 2 Cell representation 412 Each cell is considered a sentence composed of genes and its representation h i cRDis obtained 413 by aggregating the learned gene level representations h i n Various pooling operations such as 414 element wise mean pooling or weighted pooling can be readily employed in this context In this 415 study we opt to employ a special token cls for the cell representation enabling the model 416 to learn the pooling operation within transformer blocks The cls\n"
            },
            {
                "rank": 7,
                "score": -1.2766430377960205,
                "text": "Range Arena A Benchmark for Efficient Transformers arXiv preprint arXiv 2011 04006 Vaswani A Shazeer N Parmar N Uszkoreit J Jones L Gomez A N Kaiser and Polosukhin I 2017 Attention is all you need In Advances in Neural Information Processing Systems NeurIPS 59986008 Vyas A Katharopoulos A and Fleuret F 2020 Fast transformers with clustered attention Advances in Neural Information Processing Systems 33 Wang A Singh A Michael J Hill F Levy O and Bowman S R 2018 GLUE A Multi Task Benchmark and Analysis Platform for Natural Language Understanding In International Conference on Learning Representations ICLR Wang A Singh A Michael J Hill F Levy O and Bowman S R 2019 GLUE A Multi Task Benchmark and Analysis Platform for Natural Language Understanding In Proceedings of\n"
            },
            {
                "rank": 8,
                "score": -1.3101561069488525,
                "text": "One write head is all you need ArXiv abs 1911 02150 2019 So D R Manke W Liu H Dai Z Shazeer N M and Le Q V Primer Searching for efficient transformers for language modeling ArXiv abs 2109 08668 2021 Stern M Shazeer N and Uszkoreit J Blockwise parallel decoding for deep autoregressive models Advances in Neural Information Processing Systems 31 2018 Sukhbaatar S Grave E Bojanowski P and Joulin A Adaptive attention span in transformers In Annual Meeting of the Association for Computational Linguistics 2019 Sun X Ge T Wei F and Wang H Instantaneous grammatical error correction with shallow aggressive decoding ArXiv abs 2106 04970 2021 9 Fast Inference from Transformers via Speculative Decoding Thoppilan R Freitas D D Hall J Shazeer N M Kulshreshtha\n"
            },
            {
                "rank": 9,
                "score": -2.439648151397705,
                "text": "have been proposed to address this issue Beltagy et al 2020 Gulati et al 2020 Chan et al 2020 Child et al 2019 Bello et al 2019 Most approaches restrict the attention mechanism to attend to local neighborhoods Parmar et al 2018 or incorporate structural priors on attention such as sparsity Child et al 2019 pooling based compression Rae et al 2020 clustering binning convolution techniques e g Roy et al 2020 which applies k means clustering to learn dynamic sparse attention regions or Kitaev et al 2020 where locality sensitive hashing is used to group together tokens of similar embeddings sliding windows Beltagy et al 2020 or truncated targeting Chelba et al 2020 There is also a long line of research on using dense attention matrices but\n"
            }
        ]
    },
    {
        "query": "What role do hyperparameters play in optimizing reranking algorithms, and how are they tuned in practice?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 0,
                "score": 0.9408985,
                "text": "of the error on the training data via recursive application of the chain rule At this point the user must become involved again by 3 selecting one of numerous optimisation algorithms and 4 manually tuning its hyperparameters in particular the initial learning rate and the learning rate decay schedule Goodfellow et al 2016 But manually tuning hyperparameters is irksome An abundance of hyperparameters makes it difficult to rank the performance of different deep learning algorithms Lucic et al 2017 Schmidt et al 2021 and difficult to reproduce results in the literature Henderson et al 2018 Hyperparameters confound our efforts to build a scientific understanding of generalisation in deep learning Jiang et al 2020 Farhang et al 2022 And when training neural networks at the largest scale in pursuit\n"
            },
            {
                "rank": 1,
                "index": 8,
                "score": 0.01627746,
                "text": "evaluate the performance on document ranking The queries were created by the TREC 2009 2012 Web Tracks based on 50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval method Since document ranking or ad hoc retrieval mainly concerns the low level representations instead of high level semantics this dataset serves as a testbed for evaluating the quality of word embeddings We use a pretrained XLNet to extract word embeddings for the documents and queries without finetuning and employ a kernel pooling network to rank the documents A 4 Hyperparameters A 4 1 Pretraining Hyperparameters Hparam Value Number of layers 24 Hidden size 1024 Number of attention heads 16 Attention head size 64 FFN inner hidden size 4096 Hidden Dropout 0\n"
            },
            {
                "rank": 2,
                "index": 4,
                "score": 0.003027835,
                "text": "Kolesnikov et al 2020 Typically the fine tuning process involves two steps 1 fine tune models with a variety of hyperparameter configurations and 2 select the model which achieves the highest accuracy on the held out validation set The remaining models are then discarded Selecting a single model and discarding the rest has several downsides For one ensembling outputs of many models can outperform the best single model albeit at a high computational cost during inference For another fine tuning a model on downstream tasks can sometimes reduce out ofdistribution performance Radford et al 2021 Andreassen et al 2021 Wortsman et al 2021 Pham et al 2021 and the best single model on the target distribution may not be the best model on out of distribution data In\n"
            },
            {
                "rank": 3,
                "index": 3,
                "score": 0.00013982208,
                "text": "on fine tuning In particular we want to investigate the following research question RQ 3 How to efficiently fine tune Atlason tasks with limited training data To answer this question we compare the different strategies to fine tune the retriever module described in Section 2 4 We report results in Table 4 First as for pre training we observe that keeping the retriever fixed during fine tuning leads to a significant performance drops for both 64and 1024 shot settings Second the re ranking strategy row 2 leads to very similar results to fully updating the index row 1 while being significantly more efficient Lastly fine tuning only the query encoder also leads to strong results in particular in the 64 shot setup this is slightly stronger than performing\n"
            },
            {
                "rank": 4,
                "index": 5,
                "score": 6.761842e-05,
                "text": "of S BASE and RL Rto changes in hyperparameters in Fig 10 We focus on E 512 due to anecdotal experience that the largest performance variance occurred at this scale RL Ris found to be highly sensitive to the hyperparameters in Table 5 especially the choice of balancing weight In addition changes to the policy entropy weight can lead to unbalanced routers when the balancing weight is not tuned jointly Unlike Switch which has been shown to be sensitive to the choice of balancing loss Roller et al 2021 S BASE is robust to changes in balancing weight for values of 1e3to 1 S BASE also has competitive performance without a balancing loss but training is less stable Additionally Switch has higher expert oversubscription rates even when tuning\n"
            },
            {
                "rank": 5,
                "index": 6,
                "score": 4.264747e-05,
                "text": "models Note that the entropy loss is negative to encourage a more concentrated policy and the weight must be tuned jointly with the load balancing loss to keep routing balanced This is in line with Bengio et al who also use two loss terms to both encourage early specialization and expert diversity Additionally since the policy entropy loss has a similar effect to nucleus sampling we did not see an improvement from including both regularization methods RLR B consistently performed the best especially with regards to scalability in EandN For that reason we selected it as our prime example and refer to it as RL Relsewhere Table 5 Selected hyperparameters for RL Rvariants Hyperparameter RLR G RLR S RLR B Policy entropy weight 0 0 5e 4 Load\n"
            },
            {
                "rank": 6,
                "index": 7,
                "score": 2.775165e-05,
                "text": "Ribi 1 2 otherwise 27 We numerate three RL Rvariants below Greedy REINFORCE RLR G REINFORCE selecting the topkexperts and no additional auxiliary losses Nucleus sampled REINFORCE RLR S REINFORCE using nucleus sampling to eliminate less reliable expert selections and reduce noise in the policy gradient update In this method we sample from the topp truncated distribution Nucleus sampling at a fixed toppscales well with increasing the number of experts REINFORCE with baseline RLR B Our RL method which stabilizes training with a learned baseline and a policy entropy regularization loss We learn a baseline with a value function that has a single hidden layer of sizedmodel 8 Table 5 details the hyperparameters chosen for each RL Rvariant and Fig 8 contains validation losses across a number of\n"
            },
            {
                "rank": 7,
                "index": 2,
                "score": 1.4971084e-05,
                "text": "Sadigh et al 2017 Ibarz et al 2018 Lee et al 2021b a Sikchi et al 2022 is a technique for policy optimization based on relative rather than absolute feedback Owing to the relative ease of providing comparative feedback rather than absolute scores for agent behavior for human raters Miller 1956 RLHF has been successfully applied across fields from robotics Cakmak et al 2011 Tucker et al 2020 Swamy et al 2020 Byk et al 2020 to recommendation De Gemmis et al 2009 Ailon Mohri 2010 Viappiani Boutilier 2010 Afsar et al 2022 to retrieval Yue Joachims 2009 As of late RLHF has attracted renewed interest as a leading technique for fine tuning large language models LLMs Ziegler et al 2020 Stiennon et al 2020 Bai et al\n"
            },
            {
                "rank": 8,
                "index": 1,
                "score": 5.77192e-06,
                "text": "amount of overoptimization during RL implies that larger policies do not increase the amount of optimization power applied to the RM or learn faster even though they start out with higher performance on the gold score While it is expected that larger policies have less to gain from optimizing against the same RM we might also expect the gold score to peak at a substantially earlier KL distance analogous to what we see when we scale the RM size section 3 2 or for larger policies to more efficiently utilize the same number of RL feedback steps section 3 3 12 One possible hypothesis is that because RLHF can be viewed as Bayesian inference from the prior of the initial policy Korbak et al 2022 13 increases in\n"
            },
            {
                "rank": 9,
                "index": 9,
                "score": 1.918313e-06,
                "text": "soup After unsupervised pre training and supervised fine tuning we launch Nindependent RL fine tunings on the proxy rewards Ri N i 1 Then we combine the trained networks by interpolation in the weight space The final weights are adapted at test time by selecting the coefficient Figure 1 b shows our results extended in Figure 2 a with LLaMA 7b instruct fine tuned on Alpaca when RL fine tuning for news summarization with N 2reward models assessing diverse preferences of summaries With only two trainings R1andR2rewarded on Figure 1 b the interpolation 01 reveals the green front of Pareto optimal solutions i e that cannot be improved for one reward without sacrificing the other RS matches the costly yellow front of multi objective MORL 45 46 requiring\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -0.12256672978401184,
                "text": "of the error on the training data via recursive application of the chain rule At this point the user must become involved again by 3 selecting one of numerous optimisation algorithms and 4 manually tuning its hyperparameters in particular the initial learning rate and the learning rate decay schedule Goodfellow et al 2016 But manually tuning hyperparameters is irksome An abundance of hyperparameters makes it difficult to rank the performance of different deep learning algorithms Lucic et al 2017 Schmidt et al 2021 and difficult to reproduce results in the literature Henderson et al 2018 Hyperparameters confound our efforts to build a scientific understanding of generalisation in deep learning Jiang et al 2020 Farhang et al 2022 And when training neural networks at the largest scale in pursuit\n"
            },
            {
                "rank": 1,
                "score": -2.0226287841796875,
                "text": "evaluate the performance on document ranking The queries were created by the TREC 2009 2012 Web Tracks based on 50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval method Since document ranking or ad hoc retrieval mainly concerns the low level representations instead of high level semantics this dataset serves as a testbed for evaluating the quality of word embeddings We use a pretrained XLNet to extract word embeddings for the documents and queries without finetuning and employ a kernel pooling network to rank the documents A 4 Hyperparameters A 4 1 Pretraining Hyperparameters Hparam Value Number of layers 24 Hidden size 1024 Number of attention heads 16 Attention head size 64 FFN inner hidden size 4096 Hidden Dropout 0\n"
            },
            {
                "rank": 2,
                "score": -3.351708173751831,
                "text": "of S BASE and RL Rto changes in hyperparameters in Fig 10 We focus on E 512 due to anecdotal experience that the largest performance variance occurred at this scale RL Ris found to be highly sensitive to the hyperparameters in Table 5 especially the choice of balancing weight In addition changes to the policy entropy weight can lead to unbalanced routers when the balancing weight is not tuned jointly Unlike Switch which has been shown to be sensitive to the choice of balancing loss Roller et al 2021 S BASE is robust to changes in balancing weight for values of 1e3to 1 S BASE also has competitive performance without a balancing loss but training is less stable Additionally Switch has higher expert oversubscription rates even when tuning\n"
            },
            {
                "rank": 3,
                "score": -3.3786940574645996,
                "text": "Kolesnikov et al 2020 Typically the fine tuning process involves two steps 1 fine tune models with a variety of hyperparameter configurations and 2 select the model which achieves the highest accuracy on the held out validation set The remaining models are then discarded Selecting a single model and discarding the rest has several downsides For one ensembling outputs of many models can outperform the best single model albeit at a high computational cost during inference For another fine tuning a model on downstream tasks can sometimes reduce out ofdistribution performance Radford et al 2021 Andreassen et al 2021 Wortsman et al 2021 Pham et al 2021 and the best single model on the target distribution may not be the best model on out of distribution data In\n"
            },
            {
                "rank": 4,
                "score": -3.5396249294281006,
                "text": "Ribi 1 2 otherwise 27 We numerate three RL Rvariants below Greedy REINFORCE RLR G REINFORCE selecting the topkexperts and no additional auxiliary losses Nucleus sampled REINFORCE RLR S REINFORCE using nucleus sampling to eliminate less reliable expert selections and reduce noise in the policy gradient update In this method we sample from the topp truncated distribution Nucleus sampling at a fixed toppscales well with increasing the number of experts REINFORCE with baseline RLR B Our RL method which stabilizes training with a learned baseline and a policy entropy regularization loss We learn a baseline with a value function that has a single hidden layer of sizedmodel 8 Table 5 details the hyperparameters chosen for each RL Rvariant and Fig 8 contains validation losses across a number of\n"
            },
            {
                "rank": 5,
                "score": -3.6586976051330566,
                "text": "models Note that the entropy loss is negative to encourage a more concentrated policy and the weight must be tuned jointly with the load balancing loss to keep routing balanced This is in line with Bengio et al who also use two loss terms to both encourage early specialization and expert diversity Additionally since the policy entropy loss has a similar effect to nucleus sampling we did not see an improvement from including both regularization methods RLR B consistently performed the best especially with regards to scalability in EandN For that reason we selected it as our prime example and refer to it as RL Relsewhere Table 5 Selected hyperparameters for RL Rvariants Hyperparameter RLR G RLR S RLR B Policy entropy weight 0 0 5e 4 Load\n"
            },
            {
                "rank": 6,
                "score": -4.770284652709961,
                "text": "on fine tuning In particular we want to investigate the following research question RQ 3 How to efficiently fine tune Atlason tasks with limited training data To answer this question we compare the different strategies to fine tune the retriever module described in Section 2 4 We report results in Table 4 First as for pre training we observe that keeping the retriever fixed during fine tuning leads to a significant performance drops for both 64and 1024 shot settings Second the re ranking strategy row 2 leads to very similar results to fully updating the index row 1 while being significantly more efficient Lastly fine tuning only the query encoder also leads to strong results in particular in the 64 shot setup this is slightly stronger than performing\n"
            },
            {
                "rank": 7,
                "score": -6.300586700439453,
                "text": "amount of overoptimization during RL implies that larger policies do not increase the amount of optimization power applied to the RM or learn faster even though they start out with higher performance on the gold score While it is expected that larger policies have less to gain from optimizing against the same RM we might also expect the gold score to peak at a substantially earlier KL distance analogous to what we see when we scale the RM size section 3 2 or for larger policies to more efficiently utilize the same number of RL feedback steps section 3 3 12 One possible hypothesis is that because RLHF can be viewed as Bayesian inference from the prior of the initial policy Korbak et al 2022 13 increases in\n"
            },
            {
                "rank": 8,
                "score": -6.741934299468994,
                "text": "Sadigh et al 2017 Ibarz et al 2018 Lee et al 2021b a Sikchi et al 2022 is a technique for policy optimization based on relative rather than absolute feedback Owing to the relative ease of providing comparative feedback rather than absolute scores for agent behavior for human raters Miller 1956 RLHF has been successfully applied across fields from robotics Cakmak et al 2011 Tucker et al 2020 Swamy et al 2020 Byk et al 2020 to recommendation De Gemmis et al 2009 Ailon Mohri 2010 Viappiani Boutilier 2010 Afsar et al 2022 to retrieval Yue Joachims 2009 As of late RLHF has attracted renewed interest as a leading technique for fine tuning large language models LLMs Ziegler et al 2020 Stiennon et al 2020 Bai et al\n"
            },
            {
                "rank": 9,
                "score": -7.1801300048828125,
                "text": "soup After unsupervised pre training and supervised fine tuning we launch Nindependent RL fine tunings on the proxy rewards Ri N i 1 Then we combine the trained networks by interpolation in the weight space The final weights are adapted at test time by selecting the coefficient Figure 1 b shows our results extended in Figure 2 a with LLaMA 7b instruct fine tuned on Alpaca when RL fine tuning for news summarization with N 2reward models assessing diverse preferences of summaries With only two trainings R1andR2rewarded on Figure 1 b the interpolation 01 reveals the green front of Pareto optimal solutions i e that cannot be improved for one reward without sacrificing the other RS matches the costly yellow front of multi objective MORL 45 46 requiring\n"
            }
        ]
    },
    {
        "query": "Discuss the impact of pre-training on the performance of transformer models in various NLP tasks.",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 0,
                "score": 0.99998224,
                "text": "et al 2020 Ouyang et al 2022 or encoder decoder training Liu et al 2020 Chen et al 2021 Xue et al 2021 pre trained Transformer based large language models have shown impressive performance on multiple NLP tasks across languages Previous work Zhao Schtze 2021 Winata et al 2021 Lin et al 2021b investigated prompting in the multilingual setting and found that using English prompts with non English examples led to strong few shot performance Evaluation of multilingual models has mostly focused on general information extraction tasks such as question answering Clark et al 2020 Hu et al 2020 Kassner et al 2021 Ruder Sil 2021 as well as specific types of reasoning such as commonsense reasoning Ponti et al 2020 Lin et al 2021a and temporal reasoning\n"
            },
            {
                "rank": 1,
                "index": 7,
                "score": 0.9986801,
                "text": "complicated techniques with a single round of negative mining 2 Related Work Transformer Bi encoder LM pre training followed by task fine tuning has become one important paradigm in NLP Howard and Ruder 2018 SOTA models adopt the Transformer architecture Devlin et al 2019 Liu et al 2019 Yang et al 2019 Lan et al 2020 One challenge for applying deep Transformer is their computation cost when used to retrieve text from large collections Motivated by this Reimers and Gurevych 2019 propose SBERT which trains biencoder from BERT and uses vector product for efficient sentence similarity comparison Transformer bi encoders were soon also adopted as dense retriever Lee et al 2019 Chang et al 2020 Karpukhin et al 2020 Gao et al 2021b Dense Retrieval Dense retrieval compares\n"
            },
            {
                "rank": 2,
                "index": 6,
                "score": 0.9978843,
                "text": "several Natural Language Processing NLP tasks 2 5 22 23 25 33 Scaling up the size of these models has been shown to confer various benefits such as improved model prediction performance and sample efficiency 9 14 34 The conventional paradigm is to pre train large scale models on generic web scale data and fine tune the models to downstream tasks However fine tuning these models has become prohibitively expensive Since 2018 the model size has increased by almost two orders of magnitude faster than GPU memory resulting in prohibitively high cost to advance AI technologies Only a few well funded institutions have the resources to fine tune these models Parameter efficient transfer learning PETL 1 13 15 16 18 19 38 has emerged as a promising solution\n"
            },
            {
                "rank": 3,
                "index": 1,
                "score": 0.99087435,
                "text": "many areas and we present several case studies including novel results in memorization term frequency effects on few shot performance and reducing gender bias We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics Trained models analysis code training code and training data can be found at https github com EleutherAI pythia 1 Introduction Over the past several years large transformer models have established themselves as the premier methodology for generative tasks in natural language processing Brown et al 2020 Sanh et al 2021 Chowdhery et al 2022 Beyond NLP transformers have also made big splashes as generative models in areas as diverse as text to image synthesis Ramesh et al 2022 Crowson et al 2022 Rombach et\n"
            },
            {
                "rank": 4,
                "index": 9,
                "score": 0.9810583,
                "text": "pretraining also creates complications for downstream tasks which are subsequently tied to the same tokenizer and vocabulary used for pretraining even if it is not well suited for the target domain and or end task Boukkouri et al 2020 showed that B ERTs Wikipedia BooksCorpus WordPiece vocabulary results in excessive segmentation when fine tuning on medical data diminishing the benefit of pre training as a strategy 2 2 Enabling better generalization Much as Tenney et al 2019 showed that large encoders learn elements of the classic NLP pipeline it seems natural to let the model discover tokenization as well With this in mind we seek an approach that can better generalize beyond the orthographic forms encountered during pre training In terms of scientific inquiry we would like to\n"
            },
            {
                "rank": 5,
                "index": 2,
                "score": 0.70161086,
                "text": "work on adapting pre trained language models to better follow instructions 117 18 68 59 69 130 85 27 113 101 71 However following prior work 43 70 and given their overall prevalence we limit ourselves to GPT style decoder only transformers that have solely been pre trained Scaling laws Kaplan et al investigate scaling trends in GPT language models Bahri et al investigate different scaling regimes theoretically and Sharma Kaplan relate scaling coefficients to data manifold dimensions Tay et al 106 107 elucidate the connection between model architecture and scaling trends while Hernandez et al Tay et al develop scaling laws for transfer learning Ivgi et al also consider transfer learning scaling laws and highlight the importance of hyperparameter selection in the low compute regime Ghorbani et\n"
            },
            {
                "rank": 6,
                "index": 5,
                "score": 0.4327225,
                "text": "We show that the performance steadily improves when we increase the size of memory up to 262K tokens On benchmarks including code and mathematics we find that the model is capable of making use of newly defined functions and theorems during test time 1 I NTRODUCTION Transformers Vaswani et al 2017 have led to remarkable progress in natural language processing Devlin et al 2019 Brown et al 2020 mathematical reasoning Polu Sutskever 2020 Wang et al 2020a Rabe et al 2021 Li et al 2021 Hahn et al 2021 Cobbe et al 2021 and program synthesis Austin et al 2021 Chen et al 2021 Li et al 2022 However transformer performance on many of these tasks is limited by the context length of attention which is typically short\n"
            },
            {
                "rank": 7,
                "index": 4,
                "score": 0.26588047,
                "text": "11446 2021 76 Raffel C etal Exploring the limits of transfer learning with a unified text to text transformer J Mach Learn Res 21 167 2020 77 Zhang S etal OPT open pre trained transformer language models Preprint at https doi org 10 48550 arXiv 2205 01068 2022 78 Vaswani A etal Attention is all you need In 31st Conference on Neural Information Processing Systems Association of Computational Machinery 2017 79 Kaplan J etal Scaling laws for neural language models Preprint at https doi org 10 48550 arXiv 2001 08361 2020 80 Lampinen A K etal Can language models learn from explanations in context Preprint at https doi org 10 48550 arXiv 2204 02329 2022 81 Kojima T Gu S S Reid M Matsuo Y Iwasawa Y Large\n"
            },
            {
                "rank": 8,
                "index": 3,
                "score": 0.18374003,
                "text": "parameters the lower bound of the studied datasets Some methods require pre training either on the target dataset or on other external datasets We also list preprocessing required by the models in Table 2 including tokenization tok building vocabulary dictionaries and mapping tokens dict using pretrained word embeddings emb lowercasing words lower and padding sequences to a certain length pad Other modelspecific preprocessing includes an extra bag of ngrams ngram for fastText and positional embedding pe for BERT Note that for models that only require training we do not use pretrained word embeddings otherwise the boundary between pretraining and training will become ambiguous 6813 Model Pre training Training AGNews DBpedia YahooAnswers 20News Ohsumed R8 R52 TFIDF LR 0 898 0 982 0 715 0 827 0 549 0\n"
            },
            {
                "rank": 9,
                "index": 8,
                "score": 5.2662068e-05,
                "text": "different mixing rates and find that mixing in code is able to provide a 2 increase in effective tokens even when evaluating only natural language tasks For filtering we revisit perplexity and deduplication filtering strategies on both noisy and clean datasets and find that data filtering is primarily effective for noisy datasets 2 Background Predicting the scaling behavior of large models is critical when deciding on training resources Specifically two questions are of interest Allocation What is the optimal balance of resources Return What is the expected value of additional resources For scaling LLMs the resource is compute measured in FLOPs and it can be allocated to training a larger model or training for more 2 steps 1The metric used to quantify progress is the models loss on\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": 2.0815768241882324,
                "text": "et al 2020 Ouyang et al 2022 or encoder decoder training Liu et al 2020 Chen et al 2021 Xue et al 2021 pre trained Transformer based large language models have shown impressive performance on multiple NLP tasks across languages Previous work Zhao Schtze 2021 Winata et al 2021 Lin et al 2021b investigated prompting in the multilingual setting and found that using English prompts with non English examples led to strong few shot performance Evaluation of multilingual models has mostly focused on general information extraction tasks such as question answering Clark et al 2020 Hu et al 2020 Kassner et al 2021 Ruder Sil 2021 as well as specific types of reasoning such as commonsense reasoning Ponti et al 2020 Lin et al 2021a and temporal reasoning\n"
            },
            {
                "rank": 1,
                "score": 1.1548855304718018,
                "text": "several Natural Language Processing NLP tasks 2 5 22 23 25 33 Scaling up the size of these models has been shown to confer various benefits such as improved model prediction performance and sample efficiency 9 14 34 The conventional paradigm is to pre train large scale models on generic web scale data and fine tune the models to downstream tasks However fine tuning these models has become prohibitively expensive Since 2018 the model size has increased by almost two orders of magnitude faster than GPU memory resulting in prohibitively high cost to advance AI technologies Only a few well funded institutions have the resources to fine tune these models Parameter efficient transfer learning PETL 1 13 15 16 18 19 38 has emerged as a promising solution\n"
            },
            {
                "rank": 2,
                "score": 0.8809164762496948,
                "text": "complicated techniques with a single round of negative mining 2 Related Work Transformer Bi encoder LM pre training followed by task fine tuning has become one important paradigm in NLP Howard and Ruder 2018 SOTA models adopt the Transformer architecture Devlin et al 2019 Liu et al 2019 Yang et al 2019 Lan et al 2020 One challenge for applying deep Transformer is their computation cost when used to retrieve text from large collections Motivated by this Reimers and Gurevych 2019 propose SBERT which trains biencoder from BERT and uses vector product for efficient sentence similarity comparison Transformer bi encoders were soon also adopted as dense retriever Lee et al 2019 Chang et al 2020 Karpukhin et al 2020 Gao et al 2021b Dense Retrieval Dense retrieval compares\n"
            },
            {
                "rank": 3,
                "score": 0.13432736694812775,
                "text": "many areas and we present several case studies including novel results in memorization term frequency effects on few shot performance and reducing gender bias We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics Trained models analysis code training code and training data can be found at https github com EleutherAI pythia 1 Introduction Over the past several years large transformer models have established themselves as the premier methodology for generative tasks in natural language processing Brown et al 2020 Sanh et al 2021 Chowdhery et al 2022 Beyond NLP transformers have also made big splashes as generative models in areas as diverse as text to image synthesis Ramesh et al 2022 Crowson et al 2022 Rombach et\n"
            },
            {
                "rank": 4,
                "score": 0.01987350732088089,
                "text": "We show that the performance steadily improves when we increase the size of memory up to 262K tokens On benchmarks including code and mathematics we find that the model is capable of making use of newly defined functions and theorems during test time 1 I NTRODUCTION Transformers Vaswani et al 2017 have led to remarkable progress in natural language processing Devlin et al 2019 Brown et al 2020 mathematical reasoning Polu Sutskever 2020 Wang et al 2020a Rabe et al 2021 Li et al 2021 Hahn et al 2021 Cobbe et al 2021 and program synthesis Austin et al 2021 Chen et al 2021 Li et al 2022 However transformer performance on many of these tasks is limited by the context length of attention which is typically short\n"
            },
            {
                "rank": 5,
                "score": -0.3041629493236542,
                "text": "pretraining also creates complications for downstream tasks which are subsequently tied to the same tokenizer and vocabulary used for pretraining even if it is not well suited for the target domain and or end task Boukkouri et al 2020 showed that B ERTs Wikipedia BooksCorpus WordPiece vocabulary results in excessive segmentation when fine tuning on medical data diminishing the benefit of pre training as a strategy 2 2 Enabling better generalization Much as Tenney et al 2019 showed that large encoders learn elements of the classic NLP pipeline it seems natural to let the model discover tokenization as well With this in mind we seek an approach that can better generalize beyond the orthographic forms encountered during pre training In terms of scientific inquiry we would like to\n"
            },
            {
                "rank": 6,
                "score": -1.1740458011627197,
                "text": "parameters the lower bound of the studied datasets Some methods require pre training either on the target dataset or on other external datasets We also list preprocessing required by the models in Table 2 including tokenization tok building vocabulary dictionaries and mapping tokens dict using pretrained word embeddings emb lowercasing words lower and padding sequences to a certain length pad Other modelspecific preprocessing includes an extra bag of ngrams ngram for fastText and positional embedding pe for BERT Note that for models that only require training we do not use pretrained word embeddings otherwise the boundary between pretraining and training will become ambiguous 6813 Model Pre training Training AGNews DBpedia YahooAnswers 20News Ohsumed R8 R52 TFIDF LR 0 898 0 982 0 715 0 827 0 549 0\n"
            },
            {
                "rank": 7,
                "score": -1.4618926048278809,
                "text": "11446 2021 76 Raffel C etal Exploring the limits of transfer learning with a unified text to text transformer J Mach Learn Res 21 167 2020 77 Zhang S etal OPT open pre trained transformer language models Preprint at https doi org 10 48550 arXiv 2205 01068 2022 78 Vaswani A etal Attention is all you need In 31st Conference on Neural Information Processing Systems Association of Computational Machinery 2017 79 Kaplan J etal Scaling laws for neural language models Preprint at https doi org 10 48550 arXiv 2001 08361 2020 80 Lampinen A K etal Can language models learn from explanations in context Preprint at https doi org 10 48550 arXiv 2204 02329 2022 81 Kojima T Gu S S Reid M Matsuo Y Iwasawa Y Large\n"
            },
            {
                "rank": 8,
                "score": -1.6045124530792236,
                "text": "work on adapting pre trained language models to better follow instructions 117 18 68 59 69 130 85 27 113 101 71 However following prior work 43 70 and given their overall prevalence we limit ourselves to GPT style decoder only transformers that have solely been pre trained Scaling laws Kaplan et al investigate scaling trends in GPT language models Bahri et al investigate different scaling regimes theoretically and Sharma Kaplan relate scaling coefficients to data manifold dimensions Tay et al 106 107 elucidate the connection between model architecture and scaling trends while Hernandez et al Tay et al develop scaling laws for transfer learning Ivgi et al also consider transfer learning scaling laws and highlight the importance of hyperparameter selection in the low compute regime Ghorbani et\n"
            },
            {
                "rank": 9,
                "score": -4.074317455291748,
                "text": "different mixing rates and find that mixing in code is able to provide a 2 increase in effective tokens even when evaluating only natural language tasks For filtering we revisit perplexity and deduplication filtering strategies on both noisy and clean datasets and find that data filtering is primarily effective for noisy datasets 2 Background Predicting the scaling behavior of large models is critical when deciding on training resources Specifically two questions are of interest Allocation What is the optimal balance of resources Return What is the expected value of additional resources For scaling LLMs the resource is compute measured in FLOPs and it can be allocated to training a larger model or training for more 2 steps 1The metric used to quantify progress is the models loss on\n"
            }
        ]
    },
    {
        "query": "Compare and contrast different reranking approaches for ad hoc document retrieval tasks in academic literature search scenarios.",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 0,
                "score": 0.8899121,
                "text": "search results Search results may be passages of text or full text documents The systems goal is to rank the users preferred search results at the top This problem is a central one in the IR literature with well understood challenges and solutions This section provides an overview of those such that we can refer to them in subsequent sections 2 1 IR tasks Text retrieval methods for full text documents and for short text passages have application in ad hoc retrieval systems and question answering systems respectively Ad hoc retrieval Ranked document retrieval is a classic problem in information retrieval as in the main task of the Text Retrieval Conference and performed by popular search engines such as Google Bing Baidu or Yandex TREC tasks may offer\n"
            },
            {
                "rank": 1,
                "index": 9,
                "score": 0.8889517,
                "text": "to be more robust towards the vocabulary mismatch problem compared to lexical term based matching models On the other hand documents with long body texts may contain mixture of many topics and the query matches may be spread over the whole document A neural document ranking model NDRM must effectively aggregate the relevant matches from different parts of a long document In the rest of this section we discuss different types of NDRM architectures and approaches that have been explored in the literature 7 1 Document auto encoders Salakhutdinov and Hinton proposed one of the earliest deep neural models for ad hoc retrieval The model is a deep auto encoder trained on unlabelled document corpus The model treats each document as a bag of terms and uses a\n"
            },
            {
                "rank": 2,
                "index": 7,
                "score": 0.8227595,
                "text": "Mitra et al demonstrate good performances on re ranking tasks may not be indicative how the model would perform if the retrieval involves larger document collections 5 2 Query expansion Instead of comparing the query and the document directly in the embedding space an alternative approach is to use term embeddings to find good expansion candidates from a global vocabulary and then retrieving documents using the expanded query Different functions 51 170 227 have been proposed for estimating the relevance of candidate terms to the queryall of them involves comparing the candidate term individually to every query term using their vector representations and then aggregating the scores For example 51 170 estimate the relevance of candidate term tcas score tc q 1 q tqqcos vtc vtq 48 25\n"
            },
            {
                "rank": 3,
                "index": 1,
                "score": 0.23704568,
                "text": "IR approaches have been limited to re ranking topndocuments 5As an extreme example in the proactive retrieval scenario the retrieval can be triggered based solely on implicit context without any explicit query submission from the user 6http www internetlivestats com one second google band 6 Table 1 Notation used in this tutorial Meaning Notation Single query q Single document d Set of queries Q Collection of documents D Term in query q t q Term in document d t d Full vocabulary of all terms T Set of ranked results retrieved for query q R q Result tuple document dat ranki i d wherei dRq Ground truth relevance label of document dfor queryq rel q d diis more relevant than djfor queryq rel q di relq dj or\n"
            },
            {
                "rank": 4,
                "index": 6,
                "score": 0.14854057,
                "text": "were shown to be effective for reranking tasks In this setup models assign scores to the topkresults from a first stage retrieval method One can then use these scores to rerank the documents For example monoT5 Nogueira et al 2020 was the first to leverage T5 as a pointwise reranker by training a model that takes the concatenation of the query and document as input and generates a relevance label Pradeep et al 2021b Zhuang et al 2022a Hui et al 2022 have since improved the performance and efficiency of generation based reranking These approaches continue to demonstrate strong effectiveness Craswell et al 2022 Pradeep et al 2021a 2022 Generative retrieval seeks to replace the entire information retrieval process with a single sequenceto sequence model capable of mapping\n"
            },
            {
                "rank": 5,
                "index": 3,
                "score": 0.12689333,
                "text": "metadata is unavailable it is crucial to estimate the documents relevance primarily based on its text content In the text retrieval community retrieving documents for short text queries by considering the long body text of the document is an important challenge The ad hoc andWeb tracks2at the popular Text REtrieval Conference TREC focus specifically on this task The TREC participants are provided a set of say fifty search queries and a document collection containing 500 700K newswire and other documents Top ranked documents retrieved for each query from the collection by different competing retrieval systems are assessed by human annotators based on their relevance to the query Given a query the goal of the IR model is to rank documents with better assessor ratings higher than the rest\n"
            },
            {
                "rank": 6,
                "index": 5,
                "score": 0.044931643,
                "text": "Nemanja Djuric Vladan Radosavljevic and Narayan Bhamidipati 2015 Search Retargeting using Directed Query Embeddings In Proc WWW International World Wide Web Conferences Steering Committee 3738 Mihajlo Grbovic Nemanja Djuric Vladan Radosavljevic Fabrizio Silvestri and Narayan Bhamidipati 2015 Context and Content aware Embeddings for Query Rewriting in Sponsored Search In Proc SIGIR ACM 383392 Zhiwei Guan and Edward Cutrell 2007 An eye tracking study of the effect of target rank on web search In Proceedings of the SIGCHI conference on Human factors in computing systems ACM 417420 Jiafeng Guo Yixing Fan Qingyao Ai and W Bruce Croft 2016 A Deep Relevance Matching Model for Ad hoc Retrieval In Proc CIKM ACM 5564 Jiafeng Guo Yixing Fan Qingyao Ai and W Bruce Croft 2016 Semantic Matching by Non Linear Word\n"
            },
            {
                "rank": 7,
                "index": 8,
                "score": 0.0013406205,
                "text": "retrieval pipeline is the nearest neighbour search for shortlisting For example even naive re ranking of 200 images with 2048 dimensions only costs 400 KFLOPs While we report exact search cost per query for all AR experiments the shortlisting component of the pipeline can be sped up using ANNS HNSW Appendix I has a detailed discussion on compute cost for exact search memory overhead of HNSW indices and wall clock times for both implementations We note that using HNSW with 32 neighbours for shortlisting does not decrease accuracy during retrieval Figure 8 showcases the compute vs accuracy trade off for adaptive retrieval using Matryoshka Representations compared to single shot using fixed features with ResNet50 on ImageNet 1K We observed that all AR settings lied above the Pareto frontier\n"
            },
            {
                "rank": 8,
                "index": 2,
                "score": 4.5753914e-05,
                "text": "b Nie et al 2019 Min et al 2019a Wolfson et al 2020 Augmenting text based retrieval with external structured information such as knowledge graph and Wikipedia hyperlinks has also been explored recently Min et al 2019b Asai et al 2020 The use of dense vector representations for retrieval has a long history since Latent Semantic Analysis Deerwester et al 1990 Using labeled pairs of queries and documents discriminatively trained dense encoders have become popular recently Yih et al 2011 Huang et al 2013 Gillick et al 2019 with applications to cross lingual document retrieval ad relevance prediction Web search and entity retrieval Such approaches complement the sparse vector methods as they can potentially give high similarity scores to semantically relevant text pairs even without exact token matching\n"
            },
            {
                "rank": 9,
                "index": 4,
                "score": 1.2805474e-05,
                "text": "2017 Luandri a Clean Lua Interface to the Indri Search Engine In Proc SIGIR ACM Bhaskar Mitra Eric Nalisnick Nick Craswell and Rich Caruana 2016 A Dual Embedding Space Model for Document Ranking arXiv preprint arXiv 1602 01137 2016 Bhaskar Mitra Milad Shokouhi Filip Radlinski and Katja Hofmann 2014 On User Interactions with Query Auto Completion In Proc SIGIR 10551058 Bhaskar Mitra Grady Simon Jianfeng Gao Nick Craswell and Li Deng 2016 A Proposal for Evaluating Answer Distillation from Web Data In Proceedings of the SIGIR 2016 WebQA Workshop V olodymyr Mnih Nicolas Heess Alex Graves and others 2014 Recurrent models of visual attention In Proc NIPS 22042212 V olodymyr Mnih Koray Kavukcuoglu David Silver Andrei A Rusu Joel Veness Marc G Bellemare Alex Graves Martin Riedmiller Andreas\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": 1.1820414066314697,
                "text": "were shown to be effective for reranking tasks In this setup models assign scores to the topkresults from a first stage retrieval method One can then use these scores to rerank the documents For example monoT5 Nogueira et al 2020 was the first to leverage T5 as a pointwise reranker by training a model that takes the concatenation of the query and document as input and generates a relevance label Pradeep et al 2021b Zhuang et al 2022a Hui et al 2022 have since improved the performance and efficiency of generation based reranking These approaches continue to demonstrate strong effectiveness Craswell et al 2022 Pradeep et al 2021a 2022 Generative retrieval seeks to replace the entire information retrieval process with a single sequenceto sequence model capable of mapping\n"
            },
            {
                "rank": 1,
                "score": 0.46484431624412537,
                "text": "IR approaches have been limited to re ranking topndocuments 5As an extreme example in the proactive retrieval scenario the retrieval can be triggered based solely on implicit context without any explicit query submission from the user 6http www internetlivestats com one second google band 6 Table 1 Notation used in this tutorial Meaning Notation Single query q Single document d Set of queries Q Collection of documents D Term in query q t q Term in document d t d Full vocabulary of all terms T Set of ranked results retrieved for query q R q Result tuple document dat ranki i d wherei dRq Ground truth relevance label of document dfor queryq rel q d diis more relevant than djfor queryq rel q di relq dj or\n"
            },
            {
                "rank": 2,
                "score": 0.37520232796669006,
                "text": "to be more robust towards the vocabulary mismatch problem compared to lexical term based matching models On the other hand documents with long body texts may contain mixture of many topics and the query matches may be spread over the whole document A neural document ranking model NDRM must effectively aggregate the relevant matches from different parts of a long document In the rest of this section we discuss different types of NDRM architectures and approaches that have been explored in the literature 7 1 Document auto encoders Salakhutdinov and Hinton proposed one of the earliest deep neural models for ad hoc retrieval The model is a deep auto encoder trained on unlabelled document corpus The model treats each document as a bag of terms and uses a\n"
            },
            {
                "rank": 3,
                "score": 0.02255377173423767,
                "text": "Mitra et al demonstrate good performances on re ranking tasks may not be indicative how the model would perform if the retrieval involves larger document collections 5 2 Query expansion Instead of comparing the query and the document directly in the embedding space an alternative approach is to use term embeddings to find good expansion candidates from a global vocabulary and then retrieving documents using the expanded query Different functions 51 170 227 have been proposed for estimating the relevance of candidate terms to the queryall of them involves comparing the candidate term individually to every query term using their vector representations and then aggregating the scores For example 51 170 estimate the relevance of candidate term tcas score tc q 1 q tqqcos vtc vtq 48 25\n"
            },
            {
                "rank": 4,
                "score": -0.7076237797737122,
                "text": "search results Search results may be passages of text or full text documents The systems goal is to rank the users preferred search results at the top This problem is a central one in the IR literature with well understood challenges and solutions This section provides an overview of those such that we can refer to them in subsequent sections 2 1 IR tasks Text retrieval methods for full text documents and for short text passages have application in ad hoc retrieval systems and question answering systems respectively Ad hoc retrieval Ranked document retrieval is a classic problem in information retrieval as in the main task of the Text Retrieval Conference and performed by popular search engines such as Google Bing Baidu or Yandex TREC tasks may offer\n"
            },
            {
                "rank": 5,
                "score": -1.4339392185211182,
                "text": "metadata is unavailable it is crucial to estimate the documents relevance primarily based on its text content In the text retrieval community retrieving documents for short text queries by considering the long body text of the document is an important challenge The ad hoc andWeb tracks2at the popular Text REtrieval Conference TREC focus specifically on this task The TREC participants are provided a set of say fifty search queries and a document collection containing 500 700K newswire and other documents Top ranked documents retrieved for each query from the collection by different competing retrieval systems are assessed by human annotators based on their relevance to the query Given a query the goal of the IR model is to rank documents with better assessor ratings higher than the rest\n"
            },
            {
                "rank": 6,
                "score": -2.3427250385284424,
                "text": "Nemanja Djuric Vladan Radosavljevic and Narayan Bhamidipati 2015 Search Retargeting using Directed Query Embeddings In Proc WWW International World Wide Web Conferences Steering Committee 3738 Mihajlo Grbovic Nemanja Djuric Vladan Radosavljevic Fabrizio Silvestri and Narayan Bhamidipati 2015 Context and Content aware Embeddings for Query Rewriting in Sponsored Search In Proc SIGIR ACM 383392 Zhiwei Guan and Edward Cutrell 2007 An eye tracking study of the effect of target rank on web search In Proceedings of the SIGCHI conference on Human factors in computing systems ACM 417420 Jiafeng Guo Yixing Fan Qingyao Ai and W Bruce Croft 2016 A Deep Relevance Matching Model for Ad hoc Retrieval In Proc CIKM ACM 5564 Jiafeng Guo Yixing Fan Qingyao Ai and W Bruce Croft 2016 Semantic Matching by Non Linear Word\n"
            },
            {
                "rank": 7,
                "score": -2.6487419605255127,
                "text": "b Nie et al 2019 Min et al 2019a Wolfson et al 2020 Augmenting text based retrieval with external structured information such as knowledge graph and Wikipedia hyperlinks has also been explored recently Min et al 2019b Asai et al 2020 The use of dense vector representations for retrieval has a long history since Latent Semantic Analysis Deerwester et al 1990 Using labeled pairs of queries and documents discriminatively trained dense encoders have become popular recently Yih et al 2011 Huang et al 2013 Gillick et al 2019 with applications to cross lingual document retrieval ad relevance prediction Web search and entity retrieval Such approaches complement the sparse vector methods as they can potentially give high similarity scores to semantically relevant text pairs even without exact token matching\n"
            },
            {
                "rank": 8,
                "score": -3.292977809906006,
                "text": "retrieval pipeline is the nearest neighbour search for shortlisting For example even naive re ranking of 200 images with 2048 dimensions only costs 400 KFLOPs While we report exact search cost per query for all AR experiments the shortlisting component of the pipeline can be sped up using ANNS HNSW Appendix I has a detailed discussion on compute cost for exact search memory overhead of HNSW indices and wall clock times for both implementations We note that using HNSW with 32 neighbours for shortlisting does not decrease accuracy during retrieval Figure 8 showcases the compute vs accuracy trade off for adaptive retrieval using Matryoshka Representations compared to single shot using fixed features with ResNet50 on ImageNet 1K We observed that all AR settings lied above the Pareto frontier\n"
            },
            {
                "rank": 9,
                "score": -3.827404499053955,
                "text": "2017 Luandri a Clean Lua Interface to the Indri Search Engine In Proc SIGIR ACM Bhaskar Mitra Eric Nalisnick Nick Craswell and Rich Caruana 2016 A Dual Embedding Space Model for Document Ranking arXiv preprint arXiv 1602 01137 2016 Bhaskar Mitra Milad Shokouhi Filip Radlinski and Katja Hofmann 2014 On User Interactions with Query Auto Completion In Proc SIGIR 10551058 Bhaskar Mitra Grady Simon Jianfeng Gao Nick Craswell and Li Deng 2016 A Proposal for Evaluating Answer Distillation from Web Data In Proceedings of the SIGIR 2016 WebQA Workshop V olodymyr Mnih Nicolas Heess Alex Graves and others 2014 Recurrent models of visual attention In Proc NIPS 22042212 V olodymyr Mnih Koray Kavukcuoglu David Silver Andrei A Rusu Joel Veness Marc G Bellemare Alex Graves Martin Riedmiller Andreas\n"
            }
        ]
    },
    {
        "query": "How do textual entailment and semantic similarity influence the performance of reranking algorithms in information retrieval?",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 1,
                "score": 0.08601588,
                "text": "were shown to be effective for reranking tasks In this setup models assign scores to the topkresults from a first stage retrieval method One can then use these scores to rerank the documents For example monoT5 Nogueira et al 2020 was the first to leverage T5 as a pointwise reranker by training a model that takes the concatenation of the query and document as input and generates a relevance label Pradeep et al 2021b Zhuang et al 2022a Hui et al 2022 have since improved the performance and efficiency of generation based reranking These approaches continue to demonstrate strong effectiveness Craswell et al 2022 Pradeep et al 2021a 2022 Generative retrieval seeks to replace the entire information retrieval process with a single sequenceto sequence model capable of mapping\n"
            },
            {
                "rank": 1,
                "index": 4,
                "score": 0.05470151,
                "text": "Mitra et al demonstrate good performances on re ranking tasks may not be indicative how the model would perform if the retrieval involves larger document collections 5 2 Query expansion Instead of comparing the query and the document directly in the embedding space an alternative approach is to use term embeddings to find good expansion candidates from a global vocabulary and then retrieving documents using the expanded query Different functions 51 170 227 have been proposed for estimating the relevance of candidate terms to the queryall of them involves comparing the candidate term individually to every query term using their vector representations and then aggregating the scores For example 51 170 estimate the relevance of candidate term tcas score tc q 1 q tqqcos vtc vtq 48 25\n"
            },
            {
                "rank": 2,
                "index": 8,
                "score": 0.027480694,
                "text": "Nemanja Djuric Vladan Radosavljevic and Narayan Bhamidipati 2015 Search Retargeting using Directed Query Embeddings In Proc WWW International World Wide Web Conferences Steering Committee 3738 Mihajlo Grbovic Nemanja Djuric Vladan Radosavljevic Fabrizio Silvestri and Narayan Bhamidipati 2015 Context and Content aware Embeddings for Query Rewriting in Sponsored Search In Proc SIGIR ACM 383392 Zhiwei Guan and Edward Cutrell 2007 An eye tracking study of the effect of target rank on web search In Proceedings of the SIGCHI conference on Human factors in computing systems ACM 417420 Jiafeng Guo Yixing Fan Qingyao Ai and W Bruce Croft 2016 A Deep Relevance Matching Model for Ad hoc Retrieval In Proc CIKM ACM 5564 Jiafeng Guo Yixing Fan Qingyao Ai and W Bruce Croft 2016 Semantic Matching by Non Linear Word\n"
            },
            {
                "rank": 3,
                "index": 2,
                "score": 0.022933565,
                "text": "when the retrieval is performed over the full document collection However as seen in the example of Figure 14 the errors made by embedding based models and exact matching models are typically differentand the combination of the two performs better than exact matching models alone 4 58 143 Another popular technique is to use the embedding based model to re rank only a subset of the documents retrieved by a different generally an exact matching basedIR model The chaining of different IR models where each successive model re ranks a smaller number of candidate documents is called Telescoping Telescoping evaluations are popular in the neural IR literature 71 88 141 143 177 and the results are representative of performances of these models on re ranking tasks However as\n"
            },
            {
                "rank": 4,
                "index": 0,
                "score": 0.01858215,
                "text": "b Nie et al 2019 Min et al 2019a Wolfson et al 2020 Augmenting text based retrieval with external structured information such as knowledge graph and Wikipedia hyperlinks has also been explored recently Min et al 2019b Asai et al 2020 The use of dense vector representations for retrieval has a long history since Latent Semantic Analysis Deerwester et al 1990 Using labeled pairs of queries and documents discriminatively trained dense encoders have become popular recently Yih et al 2011 Huang et al 2013 Gillick et al 2019 with applications to cross lingual document retrieval ad relevance prediction Web search and entity retrieval Such approaches complement the sparse vector methods as they can potentially give high similarity scores to semantically relevant text pairs even without exact token matching\n"
            },
            {
                "rank": 5,
                "index": 3,
                "score": 0.000872767,
                "text": "well as the models that focus on representation learning We have focused on retrieval of long and short text In the case of long text the model must deal with variable length documents where the relevant sections of a document may be surrounded by irrelevant text For both long and short text but particularly for short IR models should also deal with the query document vocabulary mismatch problem by learning how patterns of query words and different document words can indicate relevance Models should also consider lexical matches when the query contains rare termssuch as a persons name or a product model numbernot seen during training and to avoid retrieving semantically related but irrelevant results An ideal model for information retrieval would be able to infer the meaning\n"
            },
            {
                "rank": 6,
                "index": 7,
                "score": 4.5044595e-05,
                "text": "size and the number of retrieved neighbours Our largest model obtains state of the art results on a range of downstream evaluation datasets including Wikitext103 Merity et al 2017 and the Pile Gao et al 2020 4 We show that R e sc t sc r sc o sccan be fine tuned to achieve competitive performance on downstream tasks such as question answering 4 3 We propose an evaluation aware of proximity of test documents with the training set 2 6 addressing the problem of test set leakage Lee et al 2021 This is relevant for all language models andespeciallyforretrieval enhancedmodelssincetheyhavedirectaccesstothetraining dataset during evaluation Using this methodology we show that the performance of R e sc t sc r sc o sc comes from both explicit neighbour copying\n"
            },
            {
                "rank": 7,
                "index": 6,
                "score": 1.5446309e-05,
                "text": "the last several years dual encoders Gillick et al 2018 Karpukhin et al 2020 Ni et al 2022b Chen et al 2022 have dominated the landscape for first stage information retrieval They model relevance by mapping queries and documents into the same embedding space optimized via contrastive learning Hadsell et al 2006 Gao et al Equal Contribution Work completed while a Student Researcher at Google 2021 Dense embeddings are pre computed for all documents in a corpus and stored in an external index This allows for fast approximate nearest neighbor search Vanderkam et al 2013 Johnson et al 2021 to retrieve relevant documents Cross encoders based on large Transformer models Nogueira et al 2019b 2020 Pradeep et al 2021b often function on top of these retrieved documents to\n"
            },
            {
                "rank": 8,
                "index": 5,
                "score": 1.5326108e-05,
                "text": "have better Recall fewer false negatives across all datasets 4 3 Passage Reranking on MS MARCO We also tested reranking coCondenser results with a deep LM reranker Similar to Qu et al 2021 we train an ensemble of ERNIE and RoBERTa to rerank the top 1000 retrieved passages on the MS MARCO evaluation set and test them on Microsofts hidden test set Table 3 shows the top three systems on August 11 2021 coCondenser is best by a small perhaps insignificant margin Essentially the three systems represent three distinct and equally good approaches for effective web passage retrieval optimized dense retrieval fine tuning in RocketQA Qu et al 2021 contextualized sparse retrieval in COIL Gao et al 2021a and corpus aware unsupervised pre training in coCondenser 5 Analysis\n"
            },
            {
                "rank": 9,
                "index": 9,
                "score": 1.0533519e-05,
                "text": "o scmodels 15 Improving language models by retrieving from trillions of tokens gains do not diminish for models with up to at least 7B parameters and correspond to non retrieval models with 10 more parameters on certain datasets On Wikitext103 and the Pile R e sc t sc r sc o scoutperforms previous models trained on large scale datasets We also show that R e sc t sc r sc o scis competitive on retrieval intensive downstream tasks such as question answering R e sc t sc r sc o scmodels are flexible and can be used without retrieval at evaluation and still achieve comparable performance to baseline models Conversely baseline models can be rapidly fine tuned intoR e sc t sc r sc o scmodelstoobtainnearlythesameperformanceasiftrainedfromscratch Carefulanalysis shows\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": -2.291469097137451,
                "text": "b Nie et al 2019 Min et al 2019a Wolfson et al 2020 Augmenting text based retrieval with external structured information such as knowledge graph and Wikipedia hyperlinks has also been explored recently Min et al 2019b Asai et al 2020 The use of dense vector representations for retrieval has a long history since Latent Semantic Analysis Deerwester et al 1990 Using labeled pairs of queries and documents discriminatively trained dense encoders have become popular recently Yih et al 2011 Huang et al 2013 Gillick et al 2019 with applications to cross lingual document retrieval ad relevance prediction Web search and entity retrieval Such approaches complement the sparse vector methods as they can potentially give high similarity scores to semantically relevant text pairs even without exact token matching\n"
            },
            {
                "rank": 1,
                "score": -2.4209442138671875,
                "text": "when the retrieval is performed over the full document collection However as seen in the example of Figure 14 the errors made by embedding based models and exact matching models are typically differentand the combination of the two performs better than exact matching models alone 4 58 143 Another popular technique is to use the embedding based model to re rank only a subset of the documents retrieved by a different generally an exact matching basedIR model The chaining of different IR models where each successive model re ranks a smaller number of candidate documents is called Telescoping Telescoping evaluations are popular in the neural IR literature 71 88 141 143 177 and the results are representative of performances of these models on re ranking tasks However as\n"
            },
            {
                "rank": 2,
                "score": -2.45367431640625,
                "text": "were shown to be effective for reranking tasks In this setup models assign scores to the topkresults from a first stage retrieval method One can then use these scores to rerank the documents For example monoT5 Nogueira et al 2020 was the first to leverage T5 as a pointwise reranker by training a model that takes the concatenation of the query and document as input and generates a relevance label Pradeep et al 2021b Zhuang et al 2022a Hui et al 2022 have since improved the performance and efficiency of generation based reranking These approaches continue to demonstrate strong effectiveness Craswell et al 2022 Pradeep et al 2021a 2022 Generative retrieval seeks to replace the entire information retrieval process with a single sequenceto sequence model capable of mapping\n"
            },
            {
                "rank": 3,
                "score": -2.953089714050293,
                "text": "have better Recall fewer false negatives across all datasets 4 3 Passage Reranking on MS MARCO We also tested reranking coCondenser results with a deep LM reranker Similar to Qu et al 2021 we train an ensemble of ERNIE and RoBERTa to rerank the top 1000 retrieved passages on the MS MARCO evaluation set and test them on Microsofts hidden test set Table 3 shows the top three systems on August 11 2021 coCondenser is best by a small perhaps insignificant margin Essentially the three systems represent three distinct and equally good approaches for effective web passage retrieval optimized dense retrieval fine tuning in RocketQA Qu et al 2021 contextualized sparse retrieval in COIL Gao et al 2021a and corpus aware unsupervised pre training in coCondenser 5 Analysis\n"
            },
            {
                "rank": 4,
                "score": -2.9856088161468506,
                "text": "Mitra et al demonstrate good performances on re ranking tasks may not be indicative how the model would perform if the retrieval involves larger document collections 5 2 Query expansion Instead of comparing the query and the document directly in the embedding space an alternative approach is to use term embeddings to find good expansion candidates from a global vocabulary and then retrieving documents using the expanded query Different functions 51 170 227 have been proposed for estimating the relevance of candidate terms to the queryall of them involves comparing the candidate term individually to every query term using their vector representations and then aggregating the scores For example 51 170 estimate the relevance of candidate term tcas score tc q 1 q tqqcos vtc vtq 48 25\n"
            },
            {
                "rank": 5,
                "score": -3.1956889629364014,
                "text": "well as the models that focus on representation learning We have focused on retrieval of long and short text In the case of long text the model must deal with variable length documents where the relevant sections of a document may be surrounded by irrelevant text For both long and short text but particularly for short IR models should also deal with the query document vocabulary mismatch problem by learning how patterns of query words and different document words can indicate relevance Models should also consider lexical matches when the query contains rare termssuch as a persons name or a product model numbernot seen during training and to avoid retrieving semantically related but irrelevant results An ideal model for information retrieval would be able to infer the meaning\n"
            },
            {
                "rank": 6,
                "score": -3.580402374267578,
                "text": "Nemanja Djuric Vladan Radosavljevic and Narayan Bhamidipati 2015 Search Retargeting using Directed Query Embeddings In Proc WWW International World Wide Web Conferences Steering Committee 3738 Mihajlo Grbovic Nemanja Djuric Vladan Radosavljevic Fabrizio Silvestri and Narayan Bhamidipati 2015 Context and Content aware Embeddings for Query Rewriting in Sponsored Search In Proc SIGIR ACM 383392 Zhiwei Guan and Edward Cutrell 2007 An eye tracking study of the effect of target rank on web search In Proceedings of the SIGCHI conference on Human factors in computing systems ACM 417420 Jiafeng Guo Yixing Fan Qingyao Ai and W Bruce Croft 2016 A Deep Relevance Matching Model for Ad hoc Retrieval In Proc CIKM ACM 5564 Jiafeng Guo Yixing Fan Qingyao Ai and W Bruce Croft 2016 Semantic Matching by Non Linear Word\n"
            },
            {
                "rank": 7,
                "score": -4.420506954193115,
                "text": "the last several years dual encoders Gillick et al 2018 Karpukhin et al 2020 Ni et al 2022b Chen et al 2022 have dominated the landscape for first stage information retrieval They model relevance by mapping queries and documents into the same embedding space optimized via contrastive learning Hadsell et al 2006 Gao et al Equal Contribution Work completed while a Student Researcher at Google 2021 Dense embeddings are pre computed for all documents in a corpus and stored in an external index This allows for fast approximate nearest neighbor search Vanderkam et al 2013 Johnson et al 2021 to retrieve relevant documents Cross encoders based on large Transformer models Nogueira et al 2019b 2020 Pradeep et al 2021b often function on top of these retrieved documents to\n"
            },
            {
                "rank": 8,
                "score": -4.581259727478027,
                "text": "size and the number of retrieved neighbours Our largest model obtains state of the art results on a range of downstream evaluation datasets including Wikitext103 Merity et al 2017 and the Pile Gao et al 2020 4 We show that R e sc t sc r sc o sccan be fine tuned to achieve competitive performance on downstream tasks such as question answering 4 3 We propose an evaluation aware of proximity of test documents with the training set 2 6 addressing the problem of test set leakage Lee et al 2021 This is relevant for all language models andespeciallyforretrieval enhancedmodelssincetheyhavedirectaccesstothetraining dataset during evaluation Using this methodology we show that the performance of R e sc t sc r sc o sc comes from both explicit neighbour copying\n"
            },
            {
                "rank": 9,
                "score": -5.2794108390808105,
                "text": "o scmodels 15 Improving language models by retrieving from trillions of tokens gains do not diminish for models with up to at least 7B parameters and correspond to non retrieval models with 10 more parameters on certain datasets On Wikitext103 and the Pile R e sc t sc r sc o scoutperforms previous models trained on large scale datasets We also show that R e sc t sc r sc o scis competitive on retrieval intensive downstream tasks such as question answering R e sc t sc r sc o scmodels are flexible and can be used without retrieval at evaluation and still achieve comparable performance to baseline models Conversely baseline models can be rapidly fine tuned intoR e sc t sc r sc o scmodelstoobtainnearlythesameperformanceasiftrainedfromscratch Carefulanalysis shows\n"
            }
        ]
    },
    {
        "query": "Analyze the potential risks and benefits of incorporating unsupervised learning techniques into reranking models for natural language processing tasks.",
        "reranked_results_cohere": [
            {
                "rank": 0,
                "index": 2,
                "score": 0.0054059,
                "text": "B Peng A Kumar G Zhang and S Levine Advantage weighted regression Simple and scalable off policy reinforcement learning arXiv preprint arXiv 1910 00177 2019 J Peters and S Schaal Reinforcement learning by reward weighted regression for operational space control In Proceedings of the 24th international conference on Machine learning pages 745750 2007 R L Plackett The analysis of permutations Journal of the Royal Statistical Society Series C Applied Statistics 24 2 193202 1975 doi https doi org 10 2307 2346567 A Radford J Wu R Child D Luan D Amodei and I Sutskever Language models are unsupervised multitask learners 2019 Ms OpenAI R Ramamurthy P Ammanabrolu K Brantley J Hessel R Sifa C Bauckhage H Hajishirzi and Y Choi Is reinforcement learning not for natural language processing\n"
            },
            {
                "rank": 1,
                "index": 9,
                "score": 0.002946405,
                "text": "were shown to be effective for reranking tasks In this setup models assign scores to the topkresults from a first stage retrieval method One can then use these scores to rerank the documents For example monoT5 Nogueira et al 2020 was the first to leverage T5 as a pointwise reranker by training a model that takes the concatenation of the query and document as input and generates a relevance label Pradeep et al 2021b Zhuang et al 2022a Hui et al 2022 have since improved the performance and efficiency of generation based reranking These approaches continue to demonstrate strong effectiveness Craswell et al 2022 Pradeep et al 2021a 2022 Generative retrieval seeks to replace the entire information retrieval process with a single sequenceto sequence model capable of mapping\n"
            },
            {
                "rank": 2,
                "index": 5,
                "score": 0.00092900335,
                "text": "strategies match the re training baseline for a 10B parameter LLM Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies matching the re training baseline using only a fraction of the compute Finally inspired by previous work we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re warming and that are not bound to a fixed token budget 1 Introduction Over the past few years large pre trained models have enabled massive performance improvements in language modeling Brown et al 2020 Zhao et al 2023 visual understanding Radford et al 2021 Alayrac et al 2022 Kirillov et al 2023 text to image generation Rombach et al 2022 Pernias et al 2024 and text to\n"
            },
            {
                "rank": 3,
                "index": 6,
                "score": 0.0003150387,
                "text": "least two approaches to leverage this existing data using unsupervised learning such as unsupervised pretraining or third person imitation learning Stadie et al 2017 or by manually annotating it 4 5 4 Hierarchical feedback The same arguments that support hierarchical RL Dayan Hinton 1993 Sutton et al 1999 Vezhnevets et al 2017 also encourage having a hierarchical decomposition of the reward model This would allow the user to provide both low level and high level feedback Both hierarchical RL and hierarchical reward models should be quite natural to combine if the temporal hierarchies between agent and reward model align then at each level of the hierarchy the reward model can train the corresponding level of the agent This might help bypass some very difficult long term credit assignment\n"
            },
            {
                "rank": 4,
                "index": 0,
                "score": 0.000118667966,
                "text": "inputs Note that estimating performance on novel distributions has additional practical value in allowing us to then potentially adapt the model to that new situation Finally it might also be valuable to create an environment where an RL agent must learn to interpret speech as part of some larger task and to explore how to respond appropriately to its own estimates of its transcription error 8 Related Efforts As mentioned in the introduction several other communities have thought broadly about the safety of AI systems both within and outside of the machine learning community Work within the machine learning community on accidents in particular was discussed in detail above but here we very briefly highlight a few other communities doing work that is broadly related to the topic\n"
            },
            {
                "rank": 5,
                "index": 3,
                "score": 6.8683185e-05,
                "text": "pretraining with up to date facts Meng et al 2022 yielding outdated answers Lewis et al 2020b To mitigate the problems above one line of research proposes to improve language models with retrieval The retrieval process can be integrated into LMs at i fine tuning stage Karpukhin et al 2020 Lewis et al 2020b Guu et al 2020 orii pretraining stage Borgeaud et al 2022 Izacard et al 2022 Most previous work augments BERT or encoder decoder LMs with retrieval at fine tuning stage demonstrating successes for knowledge intensive NLP tasks Guu et al 2020 Karpukhin et al 2020 Lewis et al 2020b Khandelwal et al 2020 However it re arXiv 2304 06762v1 cs CL 13 Apr 2023 mains relatively underexplored to pretrain autoregressive decoder only LMs with\n"
            },
            {
                "rank": 6,
                "index": 4,
                "score": 4.13354e-05,
                "text": "Overall while we observe certain tendencies for each policy both produce high quality summaries that are relatively similar 7 Related Work LLMs Brown et al 2020 Thoppilan et al 2022 Chowdhery et al 2022 Google et al 2023 OpenAI 2023 have shown impressive performance over a wide range of NLP tasks For several of these tasks RL has emerged as an effective optimization technique While initial applications of RL on tasks such as translation Wu et al 2016 2018 and summarization Gao et al 2019 Wu and Hu 2018 used automatic evaluation metrics as rewards such simplified formulations of rewards did not fully align with human notions of quality Reinforcement learning from human feedback Christiano et al 2017 has been used as a technique to directly align LLMs\n"
            },
            {
                "rank": 7,
                "index": 8,
                "score": 9.972941e-06,
                "text": "Related Work Automatically improving or self correcting large language models is becoming a major focus of research A recent survey from Pan et al attempts to summarize the topic However this is a rapidly moving area and there are already promising new works not covered there Reinforcement Learning from Human Feedback RLHF Preference learning approaches such as in Ziegler et al Stiennon et al Ouyang et al Bai et al 2022a train a fixed reward model from human preference data and then use the reward model to train via reinforcement learning RL e g via Proximal Policy Optimization PPO Schulman et al 2017 Thus the reward signal in a certain sense already comes from a model even in these works but distilled from human data Nevertheless this is\n"
            },
            {
                "rank": 8,
                "index": 7,
                "score": 8.267873e-06,
                "text": "important subtask of semi supervised RL is identifying proxies which predict the reward and learning the conditions under which those proxies are valid For example if a cleaning robots real reward is given by a detailed human evaluation then it could learn that asking the human is the room clean can provide a very useful approximation to the reward function and it could eventually learn that checking for visible dirt is an even cheaper but still useful approximation This could allow it to learn a good cleaning policy using an extremely small number of detailed evaluations More broadly use of semi supervised RL with a reliable but sparse true approval metric may incentivize communication and transparency by the agent since the agent will want to get as much\n"
            },
            {
                "rank": 9,
                "index": 1,
                "score": 5.77192e-06,
                "text": "propose REPLUG LSR REPLUG with LM Supervised Retrieval which adapts the retriever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved Inspired by Sachan et al 2022 our approach can be seen as adjusting the probabilities of the retrieved documents to match the probabilities of the output sequence perplexities of the language model In other words we would like the retriever to find documents that result in lower perplexity scores As shown in Figure 3 our training algorithm consists of the four steps 1 retrieving documents and computing the retrieval likelihood 4 1 2 scoring the retrieved documents by the language model 4 2 3 updating the retrieval model parameters by minimizing the KL divergence between the retrieval likelihood and the LMs\n"
            }
        ],
        "reranked_results_local": [
            {
                "rank": 0,
                "score": 0.810024082660675,
                "text": "B Peng A Kumar G Zhang and S Levine Advantage weighted regression Simple and scalable off policy reinforcement learning arXiv preprint arXiv 1910 00177 2019 J Peters and S Schaal Reinforcement learning by reward weighted regression for operational space control In Proceedings of the 24th international conference on Machine learning pages 745750 2007 R L Plackett The analysis of permutations Journal of the Royal Statistical Society Series C Applied Statistics 24 2 193202 1975 doi https doi org 10 2307 2346567 A Radford J Wu R Child D Luan D Amodei and I Sutskever Language models are unsupervised multitask learners 2019 Ms OpenAI R Ramamurthy P Ammanabrolu K Brantley J Hessel R Sifa C Bauckhage H Hajishirzi and Y Choi Is reinforcement learning not for natural language processing\n"
            },
            {
                "rank": 1,
                "score": 0.09859414398670197,
                "text": "Overall while we observe certain tendencies for each policy both produce high quality summaries that are relatively similar 7 Related Work LLMs Brown et al 2020 Thoppilan et al 2022 Chowdhery et al 2022 Google et al 2023 OpenAI 2023 have shown impressive performance over a wide range of NLP tasks For several of these tasks RL has emerged as an effective optimization technique While initial applications of RL on tasks such as translation Wu et al 2016 2018 and summarization Gao et al 2019 Wu and Hu 2018 used automatic evaluation metrics as rewards such simplified formulations of rewards did not fully align with human notions of quality Reinforcement learning from human feedback Christiano et al 2017 has been used as a technique to directly align LLMs\n"
            },
            {
                "rank": 2,
                "score": -0.7817208766937256,
                "text": "were shown to be effective for reranking tasks In this setup models assign scores to the topkresults from a first stage retrieval method One can then use these scores to rerank the documents For example monoT5 Nogueira et al 2020 was the first to leverage T5 as a pointwise reranker by training a model that takes the concatenation of the query and document as input and generates a relevance label Pradeep et al 2021b Zhuang et al 2022a Hui et al 2022 have since improved the performance and efficiency of generation based reranking These approaches continue to demonstrate strong effectiveness Craswell et al 2022 Pradeep et al 2021a 2022 Generative retrieval seeks to replace the entire information retrieval process with a single sequenceto sequence model capable of mapping\n"
            },
            {
                "rank": 3,
                "score": -1.8610111474990845,
                "text": "least two approaches to leverage this existing data using unsupervised learning such as unsupervised pretraining or third person imitation learning Stadie et al 2017 or by manually annotating it 4 5 4 Hierarchical feedback The same arguments that support hierarchical RL Dayan Hinton 1993 Sutton et al 1999 Vezhnevets et al 2017 also encourage having a hierarchical decomposition of the reward model This would allow the user to provide both low level and high level feedback Both hierarchical RL and hierarchical reward models should be quite natural to combine if the temporal hierarchies between agent and reward model align then at each level of the hierarchy the reward model can train the corresponding level of the agent This might help bypass some very difficult long term credit assignment\n"
            },
            {
                "rank": 4,
                "score": -2.361345052719116,
                "text": "strategies match the re training baseline for a 10B parameter LLM Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies matching the re training baseline using only a fraction of the compute Finally inspired by previous work we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re warming and that are not bound to a fixed token budget 1 Introduction Over the past few years large pre trained models have enabled massive performance improvements in language modeling Brown et al 2020 Zhao et al 2023 visual understanding Radford et al 2021 Alayrac et al 2022 Kirillov et al 2023 text to image generation Rombach et al 2022 Pernias et al 2024 and text to\n"
            },
            {
                "rank": 5,
                "score": -3.330740451812744,
                "text": "Related Work Automatically improving or self correcting large language models is becoming a major focus of research A recent survey from Pan et al attempts to summarize the topic However this is a rapidly moving area and there are already promising new works not covered there Reinforcement Learning from Human Feedback RLHF Preference learning approaches such as in Ziegler et al Stiennon et al Ouyang et al Bai et al 2022a train a fixed reward model from human preference data and then use the reward model to train via reinforcement learning RL e g via Proximal Policy Optimization PPO Schulman et al 2017 Thus the reward signal in a certain sense already comes from a model even in these works but distilled from human data Nevertheless this is\n"
            },
            {
                "rank": 6,
                "score": -3.520906925201416,
                "text": "pretraining with up to date facts Meng et al 2022 yielding outdated answers Lewis et al 2020b To mitigate the problems above one line of research proposes to improve language models with retrieval The retrieval process can be integrated into LMs at i fine tuning stage Karpukhin et al 2020 Lewis et al 2020b Guu et al 2020 orii pretraining stage Borgeaud et al 2022 Izacard et al 2022 Most previous work augments BERT or encoder decoder LMs with retrieval at fine tuning stage demonstrating successes for knowledge intensive NLP tasks Guu et al 2020 Karpukhin et al 2020 Lewis et al 2020b Khandelwal et al 2020 However it re arXiv 2304 06762v1 cs CL 13 Apr 2023 mains relatively underexplored to pretrain autoregressive decoder only LMs with\n"
            },
            {
                "rank": 7,
                "score": -3.785674571990967,
                "text": "important subtask of semi supervised RL is identifying proxies which predict the reward and learning the conditions under which those proxies are valid For example if a cleaning robots real reward is given by a detailed human evaluation then it could learn that asking the human is the room clean can provide a very useful approximation to the reward function and it could eventually learn that checking for visible dirt is an even cheaper but still useful approximation This could allow it to learn a good cleaning policy using an extremely small number of detailed evaluations More broadly use of semi supervised RL with a reliable but sparse true approval metric may incentivize communication and transparency by the agent since the agent will want to get as much\n"
            },
            {
                "rank": 8,
                "score": -4.7837324142456055,
                "text": "propose REPLUG LSR REPLUG with LM Supervised Retrieval which adapts the retriever inREPLUG by using the LM itself to provide supervision about which documents should be retrieved Inspired by Sachan et al 2022 our approach can be seen as adjusting the probabilities of the retrieved documents to match the probabilities of the output sequence perplexities of the language model In other words we would like the retriever to find documents that result in lower perplexity scores As shown in Figure 3 our training algorithm consists of the four steps 1 retrieving documents and computing the retrieval likelihood 4 1 2 scoring the retrieved documents by the language model 4 2 3 updating the retrieval model parameters by minimizing the KL divergence between the retrieval likelihood and the LMs\n"
            },
            {
                "rank": 9,
                "score": -5.422367095947266,
                "text": "inputs Note that estimating performance on novel distributions has additional practical value in allowing us to then potentially adapt the model to that new situation Finally it might also be valuable to create an environment where an RL agent must learn to interpret speech as part of some larger task and to explore how to respond appropriately to its own estimates of its transcription error 8 Related Efforts As mentioned in the introduction several other communities have thought broadly about the safety of AI systems both within and outside of the machine learning community Work within the machine learning community on accidents in particular was discussed in detail above but here we very briefly highlight a few other communities doing work that is broadly related to the topic\n"
            }
        ]
    }
]